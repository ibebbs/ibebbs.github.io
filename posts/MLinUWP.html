
<!DOCTYPE html>
<html lang="en">
        <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=Edge"/>

        <title>Ian Bebbington - State-of-the-art ML in UWP</title>
        <meta name="description" content="IObservable&lt;Opinion&gt;" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0">        

        <link type="application/rss+xml" rel="alternate" title="Ian Bebbington" href="/feed.rss" />
                <link type="application/atom+xml" rel="alternate" title="Ian Bebbington" href="/feed.atom" />
        <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
        <link rel="icon" href="/favicon.ico" type="image/x-icon">

        <link href="/assets/css/bootstrap.min.css" rel="stylesheet" />
        <link href="/assets/css/highlight.css" rel="stylesheet">
        <link href="/assets/css/clean-blog.css" rel="stylesheet" />
        <link href="/assets/css/master.css" rel="stylesheet" />
        <link href="/assets/css/font-awesome.min.css" rel="stylesheet" type="text/css">
        <link href='//fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
        <link href='//fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>
        <link href="/assets/css/override.css" rel="stylesheet" />


        <meta name="application-name" content="Ian Bebbington" />
        <meta name="msapplication-tooltip" content="Ian Bebbington" />
        <meta name="msapplication-starturl" content="/" />

        <meta property="og:title" content="Ian Bebbington - State-of-the-art ML in UWP" />
        <meta property="og:type" content="website" />
        <meta property="og:url" content="http://ian.bebbs.co.uk/posts/MLinUWP" />
        <!-- TODO: More social graph meta tags -->

        <link href="https://cdnjs.cloudflare.com/ajax/libs/ekko-lightbox/5.3.0/ekko-lightbox.css" rel="stylesheet">
<link href="/assets/css/ekko-override.css" rel="stylesheet">
<script src="https://cdnjs.cloudflare.com/ajax/libs/ekko-lightbox/5.3.0/ekko-lightbox.min.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-70151903-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-70151903-1');
</script>


        </head>
        <body>
                
                <!-- Navigation -->
                <nav class="navbar navbar-default navbar-custom navbar-fixed-top">
                        <div class="container-fluid">
                                <!-- Brand and toggle get grouped for better mobile display -->
                                <div class="navbar-header page-scroll">
                                        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#navbar-collapse">
                                        <span class="sr-only">Toggle navigation</span>
                                        <span class="icon-bar"></span>
                                        <span class="icon-bar"></span>
                                        <span class="icon-bar"></span>
                                        </button>
                                        <a class="navbar-brand" href="/">Ian Bebbington</a>
                                </div>
                        
                                <!-- Collect the nav links, forms, and other content for toggling -->
                                <div class="collapse navbar-collapse" id="navbar-collapse">
                                        <ul class="nav navbar-nav navbar-right">
                                                        <li><a href="/posts">Archive</a></li>
        <li><a href="/tags">Tags</a></li>
        <li><a href="/about">About</a></li>
 
                                        </ul>
                                </div>
                                <!-- /.navbar-collapse -->
                        </div>
                        <!-- /.container -->
                </nav>
                
                <!-- Page Header -->
                <header class="intro-header" id="intro-header">
                        <div class="container">
                                <div class="row">
                                        <div class="col-md-12">

    
<div class="post-heading">
    <h1>State-of-the-art ML in UWP</h1>
        <h2 class="subheading">Using U&#xB2;-Net for Salient Object Detection</h2>
    <div class="meta">        
Published on 24 August 2020<br>    </div>
        <div class="tags">
                    <a role="button" href="/tags/ML" class="btn btn-default btn-xs">ML</a>
                    <a role="button" href="/tags/ONNX" class="btn btn-default btn-xs">ONNX</a>
                    <a role="button" href="/tags/UWP" class="btn btn-default btn-xs">UWP</a>
        </div>     
</div>
                                        </div>
                                </div>
                        </div>
                </header>
                
                <!-- Main Content -->
                <div class="container">
                        <div class="row">
                                <div id="content" class="col-md-12">
                                        

<h2 id="tldr">TL;DR</h2>
<p>In this post I show how to use a state-of-the-art machine learning model to implement Salient Object Detection and Image Segmentation. I then show this model can be used to provide local inference capabilities entirely within a UWP app.</p>
<h2 id="intro">Intro</h2>
<p>A while ago I found myself prototyping a UI in which I wanted to show portrait images of people. However, I wanted to remove the background from these portrait images so that they appeared integrated into the UI rather than layered on top of it. For example, something like this Premier League Player of the Month card:</p>
<img src="/Content/MLinUWP/Pukki.png" class="img-responsive" style="margin: auto; max-width:50%; margin-top: 6px; margin-bottom: 6px;" alt="Premier League Player of the Month card"/>
<p>Looking around I came across <a href="https://www.remove.bg/">this website</a> which purported to use &quot;sophisticated AI technology to detect foreground layers and separate them from the background&quot;. Intrigued I gave it a go and was shocked at how good the results were. Here's an image of my little girl (endeavouring to learn how to go cross-eyed) followed by the image produced by <a href="https://www.remove.bg/">Remove.bg</a>:</p>
<table>
<tr>
<td><img src="/Content/MLinUWP/CrossEyed-Original.png" class="img-responsive" style="margin: auto; max-width:50%; margin-top: 6px; margin-bottom: 6px;" alt="Original"/></td>
<td><img src="/Content/MLinUWP/CrossEyed-removebg-preview.png" class="img-responsive" style="margin: auto; max-width:50%; margin-top: 6px; margin-bottom: 6px;" alt="Background Removed"/></td>
</tr>
<tr>
<td style="text-align: center">Original</td>
<td style="text-align: center">Background Removed</td>
</tr>
</table>
<p>Very cool and easily integrated using their API.</p>
<p>Unfortunately my use-case required background removal from user supplied content and the project costing probably wouldn't extend to paying for (potentially) thousands of calls a month.</p>
<p>So, like any good hacker, I hit the books to learn how this &quot;sophisticated AI&quot; worked...</p>
<h2 id="salient-object-detection-image-segmentation">Salient Object Detection &amp; Image Segmentation</h2>
<p>A thoroughly enjoyable couple of hours study commenced whereupon I learned of the wonders of <a href="https://paperswithcode.com/task/salient-object-detection">Salient Object Detection</a> and <a href="https://towardsdatascience.com/image-segmentation-in-2020-756b77fa88fc">Image Segmentation</a>.</p>
<p>During this research I happened upon <a href="https://github.com/NathanUA/U-2-Net">U²-Net</a>, a very recently published (May 2020) &quot;deep network architecture&quot; for salient object detection. In this repository they provided everything needed to start using their model including all weights and even sample code for inference. Moreover this model had already been used to great effect in the &quot;AR Cut &amp; Paste&quot; demo shown below (<a href="https://twitter.com/cyrildiagne/status/1256916982764646402">link here for Firefox users (like me) who don't see the tweet embedded correctly</a>):</p>
<blockquote class="twitter-tweet tw-align-center"><p lang="en" dir="ltr">4/10 - Cut &amp; paste your surroundings to Photoshop<br><br>Code: <a href="https://t.co/cVddH3u3ik">https://t.co/cVddH3u3ik</a><br><br>Book: <a href="https://twitter.com/HOLOmagazine?ref_src=twsrc%5Etfw">&#64;HOLOmagazine</a><br>Garment: SS17 by <a href="https://twitter.com/thekarentopacio?ref_src=twsrc%5Etfw">&#64;thekarentopacio</a> <br>Type: Sainte Colombe by <a href="https://twitter.com/MinetYoann?ref_src=twsrc%5Etfw">&#64;MinetYoann</a> <a href="https://twitter.com/ProductionType?ref_src=twsrc%5Etfw">&#64;ProductionType</a><br>Technical Insights: ↓<a href="https://twitter.com/hashtag/ML?src=hash&amp;ref_src=twsrc%5Etfw">#ML</a> <a href="https://twitter.com/hashtag/AR?src=hash&amp;ref_src=twsrc%5Etfw">#AR</a> <a href="https://twitter.com/hashtag/AI?src=hash&amp;ref_src=twsrc%5Etfw">#AI</a> <a href="https://twitter.com/hashtag/AIUX?src=hash&amp;ref_src=twsrc%5Etfw">#AIUX</a> <a href="https://twitter.com/hashtag/Adobe?src=hash&amp;ref_src=twsrc%5Etfw">#Adobe</a> <a href="https://twitter.com/hashtag/Photoshop?src=hash&amp;ref_src=twsrc%5Etfw">#Photoshop</a> <a href="https://t.co/LkTBe0t0rF">pic.twitter.com/LkTBe0t0rF</a></p>&mdash; Cyril Diagne (&#64;cyrildiagne) <a href="https://twitter.com/cyrildiagne/status/1256916982764646402?ref_src=twsrc%5Etfw">May 3, 2020</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>I was inspired. I wanted that tech in my product. But how?</p>
<h2 id="the-easy-way">The &quot;Easy&quot; Way</h2>
<p>In this day and age we, as developers, are somewhat spoiled. Once you understand enough about what it is you need to be able to ask the right questions, you can almost guarantee that someone out there has already posted the answers. This is very much why I endeavour to document my continued learnings on this blog; a sort of &quot;pay-it-forward&quot; thank-you for all the myriad blogs and SO answers I've benefitted from across the years.</p>
<p>Anyway, once I knew I wanted to use U²-Net, it didn't take me long to find <a href="https://hub.docker.com/r/luukio/u2net-bg-removal">a docker image</a> which provided an Http endpoint for performing U²-Net inference on a supplied image and which returned the supplied image with the background removed. Perfecto!</p>
<p>Unfortunately, trying to run this docker image caused an error. Looking at the <a href="https://github.com/ideo/bg-removal-with-u2net/blob/master/Dockerfile">Dockerfile</a> in the associated <a href="https://github.com/ideo/bg-removal-with-u2net">Github repository</a> explained why: the <a href="https://pytorch.org/">PyTorch</a> image on which this docker image was based expected to have CUDA hardware available to it. As I was running on Windows, with the docker container running within WSL2 (<a href="https://docs.microsoft.com/en-us/windows/win32/direct3d12/gpu-cuda-in-wsl">and didn't want to go back to running a Fast Ring build</a>) this docker image was of little direct use.</p>
<p>However, given the Dockerfile provided a good breakdown of all the software required to get U²-Net running, it wasn't rocket science (but perhaps artificial brain surgery?) to write a new Dockerfile which limited PyTorch to only using cpu inference. Spinning this up provided me a local (and free!) endpoint which could take my original sample image and return one with the background removed as shown below:</p>
<table>
<tr>
<td><img src="/Content/MLinUWP/CrossEyed-Original.png" class="img-responsive" style="margin: auto; max-width:50%; margin-top: 6px; margin-bottom: 6px;" alt="Original"/></td>
<td><img src="/Content/MLinUWP/CrossEyed-u2net-local.png" class="img-responsive" style="margin: auto; max-width:50%; margin-top: 6px; margin-bottom: 6px;" alt="Background Removed"/></td>
</tr>
<tr>
<td style="text-align: center">Original</td>
<td style="text-align: center">U²-Net Result</td>
</tr>
</table>
<p>For some this would be enough and, accordingly, I pushed the <a href="https://hub.docker.com/r/ibebbs/u2net-http">docker image</a> and <a href="https://github.com/ibebbs/U2Net-cpu-HTTP">associated repository</a> for others to use (please star them should you find them useful/helpful).</p>
<p>But... why pay for docker instance hosting when my users could perform the inference on their own machines from within a UWP app?</p>
<h2 id="the-hard-way">The &quot;Hard&quot; Way</h2>
<p>For those that are unaware, Windows actually ships with strong support for machine learning in UWP via the <a href="https://docs.microsoft.com/en-us/uwp/api/windows.ai.machinelearning?view=winrt-19041">Windows.AI.MachineLearning</a> namespace. Using the types provided here, a developer is able to load and perform inference using ONNX (Open Neural Network eXchange) models (up to version 1.4 -opset 9) in a (relatively) straight forward manner.</p>
<p>However, in accordance with their strategy of decoupling core technologies from releases of the OS, Microsoft have recently shifted development toward the <a href="https://github.com/Microsoft/onnxruntime">open-source</a> <a href="https://www.nuget.org/packages/Microsoft.AI.MachineLearning">Microsoft.AI.MachineLearning</a> nuget package. This package can can be installed on any recent build of windows (I believe back to 18362) and provides compatibility for the very latest ONNX models (versiol 1.7 - opset 12).</p>
<p>Given, PyTorch (the ML framework used for U²-Net) has strong support for exporting to ONNX, my challenge was clear:</p>
<ol>
<li>Export a fully weighted U²-Net model from PyTorch to ONNX.</li>
<li>Use the Microsoft.AI.MachineLearning package to load the ONNX model.</li>
<li>Write code to process a source image into U²-Net's input tensor.</li>
<li>Use the ONNX model to perform inference on the input image.</li>
<li>Write code to process a result image using U²-Net's output tensor as an alpha channel.</li>
<li>Test</li>
</ol>
<p>Now, while none of these tasks are super-difficult, you will need to be fairly analytical as they involve interpreting Python code (along with lots of Python packages) and byte bashing pixel data to/from 4 dimensional arrays.</p>
<h3 id="exporting-from-pytorch-to-onnx">Exporting from PyTorch to ONNX</h3>
<p>Given we already have a docker image that has everything needed to perform inference using U²-Net, I am going to use this image to export the ONNX model. This can be achieved by running the docker image and overriding the entry-point such that we get access to a command prompt; like so:</p>
<pre><code>docker run -it --entrypoint /bin/bash ibebbs/u2net-http
</code></pre>
<p>Once we have access to a command prompt within the container, we can use Python interactively to load and export the ONNX model. So, from the container's command prompt, start Python (in the <code>U-2-Net</code> directory) by running:</p>
<pre><code>cd U-2-Net
python3
</code></pre>
<p>This will land you at the Python command prompt <code>&gt;&gt;&gt;</code> from which we can follow the steps in <a href="https://github.com/ibebbs/U2Net-cpu-HTTP/blob/master/u2net.py"><code>u2net.py</code></a> to load the model as shown below (many of these imports are unnessary but it was just easier to include them all):</p>
<pre><code>import sys
sys.path.insert(0, 'U-2-Net')

from skimage import io, transform
import torch
import torchvision
from torch.autograd import Variable
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms

from torch.utils.data import Dataset, DataLoader

import numpy as np
from PIL import Image

from data_loader import RescaleT
from data_loader import ToTensorLab

from model import U2NET

model_dir = './saved_models/u2net/u2net.pth'
net = U2NET(3, 1)
net.load_state_dict(torch.load(model_dir, map_location=torch.device('cpu')))
</code></pre>
<p>At this point we have the <code>net</code> variable loaded with the U²-Net architecture and weights from &quot;u2net.pth&quot;. Now we need to export this variable as an ONNX model.</p>
<p>Fortunately, PyTorch has some excellent documentation for exporting ONNX (for example <a href="https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html?highlight=onnx">here</a> and <a href="https://pytorch.org/docs/master/onnx.html">here</a>) which made exporting the model fairly trivial:</p>
<pre><code>import torch.onnx
dummy_input = torch.randn(1, 3, 320, 320, device='cpu')
input_names = [ &quot;input&quot; ]
output_names = [ &quot;o0&quot;, &quot;o1&quot;, &quot;o2&quot;, &quot;o3&quot;, &quot;o4&quot;, &quot;o5&quot;, &quot;o6&quot; ]
torch.onnx.export(net, dummy_input, &quot;u2net.onnx&quot;, export_params=True, opset_version=12, input_names=input_names, output_names=output_names)
</code></pre>
<p>Here we create a random dummy input, name the input and output tensors and then export the model to ONNX using the latest operator set (<code>opset_version=12</code>).</p>
<p>This will take a few seconds and you might see a few warnings about various functions having been deprecated but, once complete, if you exit interactive Python (using <code>exit()</code>) and return to the container's command prompt, you should be able to see a &quot;u2net.onnx&quot; file in the directory as shown below:</p>
<pre><code>&gt;&gt;&gt; exit()
root&#64;88fa6881c8ea:/app/U-2-Net# dir
LICENSE    __pycache__     figures  saved_models  u2net.onnx     u2net_train.py
README.md  data_loader.py  model    test_data     u2net_test.py
</code></pre>
<p>You now need to extract the &quot;u2net.onnx&quot; from the container. There are many ways to do this, for me the easiest was to use <a href="https://en.wikipedia.org/wiki/Secure_copy_protocol">&quot;secure copy&quot;</a> to transfer the file to my machine, but do whatever is easiest for you.</p>
<h3 id="load-the-onnx-model-from-a-uwp-app">Load the ONNX model from a UWP app</h3>
<p>With the &quot;u2net.onnx&quot; model in hand, we're now going to use the <code>Microsoft.AI.MachineLearning</code> package to load the model in preparation for running inference.</p>
<p>Before we create the UWP app though, we're going to install the <a href="https://marketplace.visualstudio.com/items?itemName=WinML.mlgenv2">&quot;Windows Machine Learning Code Generator&quot;</a> extension which automatically scaffolds code for interacting with an ONNX model and makes getting started with ML super-easy. So, start VS and install the extension before continuing to the next step (if you're not using VS or would prefer not to install the extension, you can simply copy the file generated in the next step from <a href="https://github.com/ibebbs/UwpMl/blob/master/UwpMl/u2net.cs%5D">here</a>.</p>
<p>Now, from Visual Studio and with the extension installed, create a new UWP project - I named mine &quot;UwpMl&quot; - and add the &quot;u2net.onnx&quot; model to the Assets folder. As you do so, you should see that a &quot;u2net.cs&quot; file is also added to the project thanks to the &quot;Windows Machine Learning Code Generator&quot; extension. Opening this file should show class definitions similar to the following:</p>
<pre><code class="language-c#">// This file was automatically generated by VS extension Windows Machine Learning Code Generator v3
// from model file u2net.onnx
// Warning: This file may get overwritten if you add add an onnx file with the same name
using System;
using System.Collections.Generic;
using System.Threading.Tasks;
using Windows.Media;
using Windows.Storage;
using Windows.Storage.Streams;
using Windows.AI.MachineLearning;
namespace UwpMl
{
    
    public sealed class u2netInput
    {
        public TensorFloat input; // shape(1,3,320,320)
    }
    
    public sealed class u2netOutput
    {
        public TensorFloat o0; // shape(1,1,320,320)
        public TensorFloat o1; // shape(1,1,320,320)
        public TensorFloat o2; // shape(1,1,320,320)
        public TensorFloat o3; // shape(1,1,320,320)
        public TensorFloat o4; // shape(1,1,320,320)
        public TensorFloat o5; // shape(1,1,320,320)
        public TensorFloat o6; // shape(1,1,320,320)
    }
    
    public sealed class u2netModel
    {
        private LearningModel model;
        private LearningModelSession session;
        private LearningModelBinding binding;
        public static async Task&lt;u2netModel&gt; CreateFromStreamAsync(IRandomAccessStreamReference stream)
        {
            u2netModel learningModel = new u2netModel();
            learningModel.model = await LearningModel.LoadFromStreamAsync(stream);
            learningModel.session = new LearningModelSession(learningModel.model);
            learningModel.binding = new LearningModelBinding(learningModel.session);
            return learningModel;
        }
        public async Task&lt;u2netOutput&gt; EvaluateAsync(u2netInput input)
        {
            binding.Bind(&quot;input&quot;, input.input);
            var result = await session.EvaluateAsync(binding, &quot;0&quot;);
            var output = new u2netOutput();
            output.o0 = result.Outputs[&quot;o0&quot;] as TensorFloat;
            output.o1 = result.Outputs[&quot;o1&quot;] as TensorFloat;
            output.o2 = result.Outputs[&quot;o2&quot;] as TensorFloat;
            output.o3 = result.Outputs[&quot;o3&quot;] as TensorFloat;
            output.o4 = result.Outputs[&quot;o4&quot;] as TensorFloat;
            output.o5 = result.Outputs[&quot;o5&quot;] as TensorFloat;
            output.o6 = result.Outputs[&quot;o6&quot;] as TensorFloat;
            return output;
        }
    }
}
</code></pre>
<p>Well, there you go. By just adding the &quot;onnx&quot; file to the project, we now have a &quot;u2netModel&quot; which is able to load the model (<code>CreateFromStreamAsync</code>) and use it to perform inference (<code>EvaluateAsync</code>).</p>
<p>However, we should note the <code>using Windows.AI.MachineLearning;</code> line. As discussed earlier, the &quot;Windows.AI.MachineLearning&quot; namespace is included as part of recent builds of Windows and, while it allows us to use ONNX models without any additional packages, it only supports ONNX models up to version 1.4 (opset 9). Given we exported the ONNX model for U²-Net using opset 12 we need to migrate to using the more recent &quot;Microsoft.AI.MachineLearning&quot; package. Fortunately this is very straight forward and simply involves installing the <a href="https://www.nuget.org/packages/Microsoft.AI.MachineLearning/">&quot;Microsoft.AI.MachineLearning&quot; nuget package</a> into the project then changing the above <code>using</code> clause to <code>using Microsoft.AI.MachineLearning</code>. Everything else remains the same.</p>
<p>Next we'll implement a UI which will allow us to load the image on which we want to perform inference and which will display both input and output images. For simplicity, we'll eschew MVVM and use the code-behind file for &quot;MainPage&quot; to implement this functionality.</p>
<p>So, in &quot;MainPage.xaml&quot;, add the following:</p>
<pre><code class="language-xaml">&lt;Page
    x:Class=&quot;UwpMl.MainPage&quot;
    xmlns=&quot;http://schemas.microsoft.com/winfx/2006/xaml/presentation&quot;
    xmlns:x=&quot;http://schemas.microsoft.com/winfx/2006/xaml&quot;
    xmlns:local=&quot;using:UwpMl&quot;
    xmlns:d=&quot;http://schemas.microsoft.com/expression/blend/2008&quot;
    xmlns:mc=&quot;http://schemas.openxmlformats.org/markup-compatibility/2006&quot;
    mc:Ignorable=&quot;d&quot;
    Background=&quot;{ThemeResource ApplicationPageBackgroundThemeBrush}&quot;&gt;

    &lt;Grid&gt;
        &lt;Grid.RowDefinitions&gt;
            &lt;RowDefinition Height=&quot;0.6*&quot;/&gt;
            &lt;RowDefinition Height=&quot;0.3*&quot;/&gt;
            &lt;RowDefinition Height=&quot;Auto&quot;/&gt;
        &lt;/Grid.RowDefinitions&gt;
        &lt;Grid.ColumnDefinitions&gt;
            &lt;ColumnDefinition Width=&quot;0.5*&quot;/&gt;
            &lt;ColumnDefinition Width=&quot;0.5*&quot;/&gt;
        &lt;/Grid.ColumnDefinitions&gt;
        &lt;Image Grid.Column=&quot;0&quot; Source=&quot;/Assets/Checkerboard.png&quot; Width=&quot;320&quot; Height=&quot;320&quot; Stretch=&quot;UniformToFill&quot; HorizontalAlignment=&quot;Center&quot; VerticalAlignment=&quot;Center&quot; /&gt;
        &lt;Image Grid.Column=&quot;0&quot; x:Name=&quot;sourceImage&quot; Stretch=&quot;None&quot; HorizontalAlignment=&quot;Center&quot; VerticalAlignment=&quot;Center&quot; /&gt;
        &lt;Image Grid.Column=&quot;1&quot; Source=&quot;/Assets/Checkerboard.png&quot; Width=&quot;320&quot; Height=&quot;320&quot; Stretch=&quot;UniformToFill&quot; HorizontalAlignment=&quot;Center&quot; VerticalAlignment=&quot;Center&quot; /&gt;
        &lt;Image Grid.Column=&quot;1&quot;  x:Name=&quot;targetImage&quot; Stretch=&quot;None&quot; HorizontalAlignment=&quot;Center&quot; VerticalAlignment=&quot;Center&quot; /&gt;
        &lt;ScrollViewer Grid.Row=&quot;1&quot; Grid.ColumnSpan=&quot;2&quot; HorizontalScrollMode=&quot;Auto&quot; HorizontalScrollBarVisibility=&quot;Auto&quot; VerticalScrollMode=&quot;Disabled&quot; VerticalScrollBarVisibility=&quot;Hidden&quot;&gt;
            &lt;StackPanel Orientation=&quot;Horizontal&quot;&gt;
                &lt;Image x:Name=&quot;o6&quot; Grid.Row=&quot;6&quot; Stretch=&quot;Uniform&quot; HorizontalAlignment=&quot;Center&quot; VerticalAlignment=&quot;Center&quot; /&gt;
                &lt;Image x:Name=&quot;o5&quot; Grid.Row=&quot;5&quot; Stretch=&quot;Uniform&quot; HorizontalAlignment=&quot;Center&quot; VerticalAlignment=&quot;Center&quot; /&gt;
                &lt;Image x:Name=&quot;o4&quot; Grid.Row=&quot;4&quot; Stretch=&quot;Uniform&quot; HorizontalAlignment=&quot;Center&quot; VerticalAlignment=&quot;Center&quot; /&gt;
                &lt;Image x:Name=&quot;o3&quot; Grid.Row=&quot;3&quot; Stretch=&quot;Uniform&quot; HorizontalAlignment=&quot;Center&quot; VerticalAlignment=&quot;Center&quot; /&gt;
                &lt;Image x:Name=&quot;o2&quot; Grid.Row=&quot;2&quot; Stretch=&quot;Uniform&quot; HorizontalAlignment=&quot;Center&quot; VerticalAlignment=&quot;Center&quot; /&gt;
                &lt;Image x:Name=&quot;o1&quot; Grid.Row=&quot;1&quot; Stretch=&quot;Uniform&quot; HorizontalAlignment=&quot;Center&quot; VerticalAlignment=&quot;Center&quot; /&gt;
            &lt;/StackPanel&gt;
        &lt;/ScrollViewer&gt;
        &lt;StackPanel Grid.Row=&quot;2&quot; Orientation=&quot;Horizontal&quot; HorizontalAlignment=&quot;Center&quot; Margin=&quot;4&quot; Grid.ColumnSpan=&quot;2&quot;&gt;
            &lt;Button Content=&quot;Go!&quot; Padding=&quot;32,16&quot; Margin=&quot;4&quot; Click=&quot;Button_Click&quot;/&gt;
        &lt;/StackPanel&gt;
    &lt;/Grid&gt;
&lt;/Page&gt;
</code></pre>
<p>Here you'll see that we add an <code>Image</code> named &quot;sourceImage&quot; which is used to display the input image and another <code>Image</code> named &quot;targetImage&quot; which is used to display the output. Behind these images I add additional <code>Image</code> elements which display a checkerboard pattern; this is to demonstrate opacity in the target image and is completely optional but should you wish to display these you can find the &quot;Checkerboard.png&quot; file <a href="https://github.com/ibebbs/UwpMl/blob/master/UwpMl/Assets/Checkerboard.png">here</a>.</p>
<p>Underneath these images I add a horizontally oriented <code>StackPanel</code> containing further <code>Image</code> elements. These are used to display the intermediate results of the U²-Net architecture which I found very useful for debugging but again is completely optional as it has no bearing on the final output.</p>
<p>Finally, in the bottom row of the UI we have a <code>StackPanel</code> containing a singular <code>Button</code> displaying the content &quot;Go!&quot;. This button will be used to load and display an image, perform inference and, finally, display the output image. We'll use the &quot;Click&quot; event to invoke this functionality in the &quot;MainPage.xaml.cs&quot; file as shown below:</p>
<pre><code class="language-c#">using Microsoft.AI.MachineLearning;
using System;
using System.Threading.Tasks;
using Windows.Graphics.Imaging;
using Windows.Storage;
using Windows.Storage.Streams;
using Windows.UI.Xaml;
using Windows.UI.Xaml.Controls;
using Windows.UI.Xaml.Media.Imaging;

namespace UwpMl
{
    /// &lt;summary&gt;
    /// An empty page that can be used on its own or navigated to within a Frame.
    /// &lt;/summary&gt;
    public sealed partial class MainPage : Page
    {
        public MainPage()
        {
            this.InitializeComponent();
        }

        private async void Button_Click(object sender, RoutedEventArgs e)
        {
            // Use Picket to get file
            var file = await GetImageFile();

            SoftwareBitmap softwareBitmap;
            byte[] bytes;


            // Load image &amp; scale to tensor input dimensions
            using (IRandomAccessStream stream = await file.OpenAsync(FileAccessMode.Read))
            {
                bytes = await GetImageAsByteArrayAsync(stream, 320, 320, BitmapPixelFormat.Rgba8);
                softwareBitmap = await GetImageAsSoftwareBitmapAsync(stream, 320, 320, BitmapPixelFormat.Bgra8);
            }

            // Display source image
            var source = new SoftwareBitmapSource();
            await source.SetBitmapAsync(softwareBitmap);

            sourceImage.Source = source;

            // Convert rgba-rgba-...-rgba to bb...b-rr...r-gg...g as colour weighted tensor (0..1)
            var input = TensorFloat.CreateFromIterable(new long[] { 1, 3, 320, 320 }, TensorBrg(bytes));

            // Load model &amp; perform inference
            StorageFile modelFile = await StorageFile.GetFileFromApplicationUriAsync(new Uri($&quot;ms-appx:///Assets/u2net.onnx&quot;));
            u2netModel model = await u2netModel.CreateFromStreamAsync(modelFile);
            u2netOutput output = await model.EvaluateAsync(new u2netInput { input = input });

            // Display intermediate results
            await ToImage(output.o6, o6);
            await ToImage(output.o5, o5);
            await ToImage(output.o4, o4);
            await ToImage(output.o3, o3);
            await ToImage(output.o2, o2);
            await ToImage(output.o1, o1);

            // Display final result using the tensor as alpha mask on source image
            await ToImage(bytes, output.o0, targetImage);
        }
    }
}
</code></pre>
<p>As you can see, quality has been traded for clarity here to ensure the flow of how an image is retrieved and passed to a <code>u2netmodel</code> instance is clear. Pasting this code into &quot;MainPage.xaml.cs&quot; will give you a bunch of red squigglies indicating undefined methods which we'll implement next, starting with the easy bits:</p>
<h4 id="getimagefile">GetImageFile</h4>
<pre><code class="language-c#">private async Task&lt;StorageFile&gt; GetImageFile()
{
    var picker = new Windows.Storage.Pickers.FileOpenPicker();
    picker.ViewMode = Windows.Storage.Pickers.PickerViewMode.Thumbnail;
    picker.SuggestedStartLocation = Windows.Storage.Pickers.PickerLocationId.PicturesLibrary;
    picker.FileTypeFilter.Add(&quot;.jpg&quot;);
    picker.FileTypeFilter.Add(&quot;.jpeg&quot;);
    picker.FileTypeFilter.Add(&quot;.png&quot;);

    var file = await picker.PickSingleFileAsync();

    return file;
}
</code></pre>
<p>This code uses a file picker to allow the user to select the source image.</p>
<h4 id="getimageassoftwarebitmapasync">GetImageAsSoftwareBitmapAsync</h4>
<pre><code class="language-c#">private async Task&lt;SoftwareBitmap&gt; GetImageAsSoftwareBitmapAsync(IRandomAccessStream stream, uint width, uint height, BitmapPixelFormat pixelFormat)
{
    BitmapDecoder decoder = await BitmapDecoder.CreateAsync(stream);

    var transform = new BitmapTransform() { ScaledWidth = width, ScaledHeight = height, InterpolationMode = BitmapInterpolationMode.NearestNeighbor };
    var softwareBitmap = await decoder.GetSoftwareBitmapAsync(pixelFormat, BitmapAlphaMode.Premultiplied, transform, ExifOrientationMode.IgnoreExifOrientation, ColorManagementMode.DoNotColorManage);

    return softwareBitmap;
}
</code></pre>
<p>This code loads an image from the specified <code>IRandomAccessStream</code> and uses a <code>BitmapTransform</code> and a <code>BitmapPixelFormat</code> to transform the source image to the desired size and pixel format for displaying in the UI. Finally it returns a <code>SoftwareBitmap</code> which can be conveniently displayed.</p>
<h4 id="getimageasbytearrayasync">GetImageAsByteArrayAsync</h4>
<pre><code class="language-c#">private async Task&lt;byte[]&gt; GetImageAsByteArrayAsync(IRandomAccessStream stream, uint width, uint height, BitmapPixelFormat pixelFormat)
{
    BitmapDecoder decoder = await BitmapDecoder.CreateAsync(stream);

    var transform = new BitmapTransform() { ScaledWidth = width, ScaledHeight = height, InterpolationMode = BitmapInterpolationMode.NearestNeighbor };
    var data = await decoder.GetPixelDataAsync(pixelFormat, BitmapAlphaMode.Premultiplied, transform, ExifOrientationMode.IgnoreExifOrientation, ColorManagementMode.DoNotColorManage);

    return data.DetachPixelData();
}
</code></pre>
<p>This code loads an image from the specified <code>IRandomAccessStream</code> and uses a <code>BitmapTransform</code> and a <code>BitmapPixelFormat</code> to transform the source image to the desired size and pixel format for convenient translation into our ONNX model. Finally it returns a <code>byte[]</code> representing the transformed image.</p>
<p>Now comes the tricky bits...</p>
<h3 id="transform-the-source-image-into-u2-nets-input-tensor">Transform the source image into U²-Net's input tensor.</h3>
<p>To perform inference, the input image needs to be translated into a &quot;Tensor&quot;. Don't let the terminology scare you here, a &quot;tensor&quot; is simply a multi-dimensional array of floating point numbers with a defined &quot;shape&quot; (i.e. the size of each dimension). We can see the desired &quot;shape&quot; of the input tensor by looking at the <code>u2netInput</code> class which contains the following:</p>
<pre><code class="language-c#">public sealed class u2netInput
{
    public TensorFloat input; // shape(1,3,320,320)
}
</code></pre>
<p>In case it's not apparent, the sizes of each dimension relate to the values per pixel (3 - red, green &amp; blue) along with the height (320 pixels) and width (320 pixels) dimensions of the source image. We needn't worry about the initial dimension here which - in this instance - just acts as a &quot;container&quot; for the other dimensions and will always have a size of 1.</p>
<p>Now, while translating our input image into this tensor, it's important to ensure we provide the tensor values in the format/order the underlying model expects them. Specifically here we must:</p>
<ol>
<li>Provide multiple greyscale images<br />
Given the shape of this input tensor - (3, 320, 320) - we can see the model is expecting to see 3 greyscale images, sized 320x320 apiece, with each &quot;grayscale&quot; image calculated from one of the input image's colour channels. Furthermore, careful examination of <a href="https://github.com/NathanUA/U-2-Net/blob/0b27f5cc958bac88825b1001f8245f663faeb1b8/data_loader.py#L218"><code>data_loader.py</code></a> shows that the model is expecting these images in blue, red, green order. This means that our (scaled) input image needs to be translated such that the index [1,1,1] - which would ordinarily return the red component of the top left pixel - returns the blue component of the top left pixel instead, and the index [2,1,1] - which would return the red component of the second pixel from the left on the top row of the image - instead returns the red component of the top left pixel of the image. And so on and so forth.</li>
<li>&quot;Normalize&quot; pixel values<br />
Pixels in our input image are in the Rgba8 format (as shown in the call to <code>GetImageAsByteArrayAsync</code>) meaning each pixel is composed of 4 channels (red, green, blue and alpha) and each channel is represented by a single byte ranging in value from 0 to 255. Each of these pixel values need to be translated into a value between 0 and 1 and &quot;normalized&quot; with a <a href="https://github.com/NathanUA/U-2-Net/blob/0b27f5cc958bac88825b1001f8245f663faeb1b8/data_loader.py#L212">channel specific divisor</a>.</li>
</ol>
<p>I implement these considerations in the <code>TensorBrg</code> method as shown here:</p>
<pre><code class="language-c#">public IEnumerable&lt;float&gt; TensorBrg(byte[] bytes)
{
    // Original in rgb (0,1,2), we want brg(2,0,1)

    // Return the blue channel
    for (int i = 2; i &lt; bytes.Length; i += 4)
    {
        var b = Convert.ToSingle(((bytes[i] / 255.0) - 0.406) / 0.225);
        yield return b;
    }

    // Return the red channel
    for (int i = 0; i &lt; bytes.Length; i += 4)
    {
        var r = Convert.ToSingle(((bytes[i] / 255.0) - 0.485) / 0.229);
        yield return r;
    }

    // Return the green channel
    for (int i = 1; i &lt; bytes.Length; i += 4)
    {
        var g = Convert.ToSingle(((bytes[i] / 255.0) - 0.456) / 0.224);
        yield return g;
    }
}
</code></pre>
<p>This method uses the <code>yield return</code> keyword to return the result of the mapping as an IEnumerable<float> thereby alleviating the need for an intermediate buffer and is used as follows:</p>
<pre><code class="language-c#">// Convert rgba-rgba-...-rgba to bb...b-rr...r-gg...g as colour weighted tensor (0..1)
TensorFloat input = TensorFloat.CreateFromIterable(new long[] { 1, 3, 320, 320 }, TensorBrg(bytes));
</code></pre>
<h3 id="perform-inference">Perform inference</h3>
<p>Now we have a tensor of the expected shape containing the expected values, we're able to use our ONNX model to perform inference. This is - thanks to the &quot;Windows Machine Learning Code Generator&quot; - extremely easy as shown below:</p>
<pre><code class="language-c#">// Load model &amp; perform inference
StorageFile modelFile = await StorageFile.GetFileFromApplicationUriAsync(new Uri($&quot;ms-appx:///Assets/u2net.onnx&quot;));
u2netModel model = await u2netModel.CreateFromStreamAsync(modelFile);
u2netOutput output = await model.EvaluateAsync(new u2netInput { input = input });
</code></pre>
<p>And that's it. We have successfully used an ONNX model to perform inference using a state-of-the-art machine learning model. Just one small thing left... interpreting the results.</p>
<h3 id="transform-the-source-image-into-target-image-using-u2-nets-output-tensor-as-an-alpha-channel">Transform the source image into target image using U²-Net's output tensor as an alpha channel.</h3>
<p>We have two methods left to implement: <code>ToImage</code> &amp; <code>ToBlendedImage</code>. The first of these takes an output tensor and converts it to a grey scale image. This method is used with the &quot;intermediate&quot; output tensors to show the progression towards the result and is really just used for debugging purposes or out of interest. The code is shown here:</p>
<pre><code class="language-c#">private async Task ToImage(TensorFloat tensorFloat, Image image)
{
    var pixels = tensorFloat
            .GetAsVectorView()
            .SelectMany(
                f =&gt;
                {
                    byte v = Convert.ToByte(f * 255);
                    return new byte[] { v, v, v, 255 };
                })
            .ToArray();

    var writeableBitmap = new WriteableBitmap(320, 320);

    // Open a stream to copy the image contents to the WriteableBitmap's pixel buffer 
    using (Stream stream = writeableBitmap.PixelBuffer.AsStream())
    {
        await stream.WriteAsync(pixels, 0, pixels.Length);
    }

    var dest = SoftwareBitmap.CreateCopyFromBuffer(writeableBitmap.PixelBuffer, BitmapPixelFormat.Bgra8, 320, 320, BitmapAlphaMode.Premultiplied);
    var destSouce = new SoftwareBitmapSource();
    await destSouce.SetBitmapAsync(dest);

    image.Source = destSouce;
}
</code></pre>
<p>Conversely, <code>ToBlendedImage</code> composes our desired output image by using the final output tensor of the U²-Net model as both a mask and an alpha channel for the input image. This is shown below:</p>
<pre><code class="language-c#">private IEnumerable&lt;byte&gt; ApplyTensorAsMask(byte[] data, TensorFloat tensorFloat, float cutoff)
{
    var tensorData = tensorFloat.GetAsVectorView().ToArray();

    for (int i = 0; i &lt; data.Length; i += 4)
    {
        var alpha = Math.Clamp(tensorData[i / 4], 0, 1);

        if (alpha &gt; cutoff)
        {
            yield return Convert.ToByte(data[i + 2] * alpha);
            yield return Convert.ToByte(data[i + 1] * alpha);
            yield return Convert.ToByte(data[i + 0] * alpha);
            yield return Convert.ToByte(alpha * 255);
        }
        else
        {
            yield return 0;
            yield return 0;
            yield return 0;
            yield return 0;
        }

    }
}

private async Task ToBlendedImage(byte[] data, TensorFloat tensorFloat, Image target)
{
    var image = ApplyTensorAsMask(data, tensorFloat, 0.0f).ToArray();
    var writeableBitmap = new WriteableBitmap(320, 320);

    // Open a stream to copy the image contents to the WriteableBitmap's pixel buffer 
    using (Stream stream = writeableBitmap.PixelBuffer.AsStream())
    {
        await stream.WriteAsync(image, 0, image.Length);
    }

    var dest = SoftwareBitmap.CreateCopyFromBuffer(writeableBitmap.PixelBuffer, BitmapPixelFormat.Bgra8, 320, 320, BitmapAlphaMode.Premultiplied);
    var destSouce = new SoftwareBitmapSource();
    await destSouce.SetBitmapAsync(dest);

    target.Source = destSouce;
}
</code></pre>
<p>With these methods implemented, there should be no more sqigglies in our <code>MainPage.xaml.cs</code> and we should be able to successfully compile and run the project.</p>
<h3 id="testing">Testing</h3>
<p>Run the project and click the &quot;Go!&quot; button. While you are free to use any source image you like to test the code above, I would suggest that, when prompted for a source image, you use one of the <a href="https://github.com/NathanUA/U-2-Net/tree/master/test_data/test_images">test images provided by U²-Net</a>; in the screen shot below I've used <a href="https://github.com/NathanUA/U-2-Net/blob/master/test_data/test_images/bike.jpg">bike.jpg</a>.</p>
<p>After selecting the image, it will be scaled and displayed in the UI before performing inference and displaying the output images. It should only take a few seconds for the output image to appear and, of this time, inference via the ONNX model should - depending on your hardware - be less than a second. This shows that there is significant potential for optimization in the preparation of the input tensor and the processing of the output tensor but, given <a href="https://stackify.com/premature-optimization-evil/">premature optimisation is the root of all evil</a>, I didn't attempt to optimize these processes and instead just focused on getting the solution running.</p>
<p>Anyway, once processing is complete, you should see something similar to this:</p>
<img src="/Content/MLinUWP/UWP Background Removal.png" class="img-responsive" style="margin: auto; max-width:80%; margin-top: 6px; margin-bottom: 6px;" alt="UWP Background Removal"/>
<p>Nice! Let's compare it to the docker produced image of my girl above:</p>
<img src="/Content/MLinUWP/CrossEyed-u2net-UWP.png" class="img-responsive" style="margin: auto; max-width:80%; margin-top: 6px; margin-bottom: 6px;" alt="CrossEyed Background Removal from UWP"/>
<p>That is pretty good, just as fast as as the Docker solution and doesn't require an internet connection. Sweet!</p>
<h1 id="bonus">Bonus</h1>
<p>Now we're able to remove backgrounds using a state-of-the-art machine learning model both in and out of process, let's revisit the &quot;Premier League Player of the Month&quot; to see if we can easily create one of our own. Quickly combining the following XAML with a processed image of my boy gives us:</p>
<pre><code class="language-xaml">&lt;Viewbox&gt;
    &lt;Canvas xmlns=&quot;http://schemas.microsoft.com/winfx/2006/xaml/presentation&quot; xmlns:x=&quot;http://schemas.microsoft.com/winfx/2006/xaml&quot; x:Name=&quot;Layer_3_0&quot; Width=&quot;640.089&quot; Height=&quot;896.125&quot; Canvas.Left=&quot;0&quot; Canvas.Top=&quot;0&quot;&gt;
        &lt;Path Width=&quot;594.812&quot; Height=&quot;887.356&quot; Canvas.Left=&quot;0.00104256&quot; Canvas.Top=&quot;6.10352e-005&quot; Stretch=&quot;Fill&quot; Fill=&quot;#FF3C9A24&quot; Data=&quot;...&quot;/&gt;
        &lt;controls:DropShadowPanel BlurRadius=&quot;50.0&quot; ShadowOpacity=&quot;0.80&quot; OffsetX=&quot;114.0&quot; OffsetY=&quot;0.0&quot; Color=&quot;#AF000000&quot;&gt;
            &lt;Image Source=&quot;Assets/Poopi.png&quot; Width=&quot;640&quot; Height=&quot;547&quot; /&gt;
        &lt;/controls:DropShadowPanel&gt;
        &lt;Rectangle Width=&quot;595&quot; Height=&quot;50&quot; Canvas.Top=&quot;494&quot; &gt;
            &lt;Rectangle.Fill&gt;
                &lt;LinearGradientBrush EndPoint=&quot;0.5,1&quot; StartPoint=&quot;0.5,0&quot;&gt;
                    &lt;GradientStop Color=&quot;#00000000&quot;/&gt;
                    &lt;GradientStop Color=&quot;#7F000000&quot; Offset=&quot;1&quot;/&gt;
                &lt;/LinearGradientBrush&gt;
            &lt;/Rectangle.Fill&gt;
        &lt;/Rectangle&gt;
        &lt;Canvas x:Name=&quot;Layer_4&quot; Width=&quot;640.089&quot; Height=&quot;855&quot; Canvas.Left=&quot;0&quot; Canvas.Top=&quot;91&quot;&gt;
            &lt;Path Width=&quot;868.328&quot; Height=&quot;171.341&quot; Canvas.Left=&quot;-138.987&quot; Canvas.Top=&quot;437.023&quot; Stretch=&quot;Fill&quot; Fill=&quot;#FFD7533E&quot; Data=&quot;...&quot;/&gt;
        &lt;/Canvas&gt;
        &lt;TextBlock Text=&quot;POOPI&quot; Canvas.Left=&quot;29&quot; Canvas.Top=&quot;553&quot; Height=&quot;119&quot; Width=&quot;532&quot; FontFamily=&quot;Impact&quot; FontSize=&quot;96&quot; Foreground=&quot;White&quot; TextAlignment=&quot;Center&quot; /&gt;
    &lt;/Canvas&gt;
&lt;/Viewbox&gt;
</code></pre>
<img src="/Content/MLinUWP/Poopi.png" class="img-responsive" style="margin: auto; max-width:50%; margin-top: 6px; margin-bottom: 6px;" alt="Poopi"/>
<p>Yup, that works.</p>
<p>(Sorry son but, after what happened Sunday, you deserve it ;0)</p>
<h2 id="conclusion">Conclusion</h2>
<p>As you can see, using state-of-the machine learning models from UWP is fairly straight forward and certainly not any more complicated than using them from Python. UWP - via &quot;Microsoft.AI.MachineLearning&quot; - has excellent support for the very latest versions of ONNX and, given most mainstream machine learning frameworks can export to ONNX, allows UWP developers to easily leverage the entire vista of modern machine learning algorithms for their purposes (resources permitting).</p>
<p>The source code for this article can be found in my <a href="https://github.com/ibebbs/UwpMl">UwpMl repository</a> on GitHub; please star it if you find it helpful or informative. Should you have any questions or comments, please feel free to drop me a line using any of the links below or from my <a href="https://ian.bebbs.co.uk/about">about page</a>.</p>



                                </div>
                        </div>
                </div>
                
                <hr>
                
                <!-- Footer -->
                <footer>
                        <div class="container">
        <div class="row">
                <div class="col-md-12">
                        <ul class="list-inline text-center">
    <li>
        <a href="https://twitter.com/ibebbs">
            <span class="fa-stack fa-lg">
                <i class="fa fa-circle fa-stack-2x"></i>
                <i class="fa fa-twitter fa-stack-1x fa-inverse"></i>
            </span>
        </a>
    </li>
    <li>
        <a href="https://www.linkedin.com/in/ianbebbington/">
            <span class="fa-stack fa-lg">
                <i class="fa fa-circle fa-stack-2x"></i>
                <i class="fa fa-linkedin fa-stack-1x fa-inverse"></i>
            </span>
        </a>
    </li>
    <li />
    <li>
        <a href="https://github.com/ibebbs">
            <span class="fa-stack fa-lg">
                <i class="fa fa-circle fa-stack-2x"></i>
                <i class="fa fa-github fa-stack-1x fa-inverse"></i>
            </span>
        </a>
    </li>
    <li>
        <a href="https://stackoverflow.com/users/628821/ibebbs">
            <span class="fa-stack fa-lg">
                <i class="fa fa-circle fa-stack-2x"></i>
                <i class="fa fa-stack-overflow fa-stack-1x fa-inverse"></i>
            </span>
        </a>
    </li>
</ul>
                        <br />
                        <ul class="list-inline text-center">
                                <li>                                
                                        <!-- Buy Me A Coffee Button -->
                                        <style>.bmc-button img{width: 27px !important;margin-bottom: 1px !important;box-shadow: none !important;border: none !important;vertical-align: middle !important;}.bmc-button{line-height: 36px !important;height:37px !important;text-decoration: none !important;display:inline-flex !important;color:#FFFFFF !important;background-color:#FF813F !important;border-radius: 3px !important;border: 1px solid transparent !important;padding: 1px 9px !important;font-size: 22px !important;letter-spacing: 0.6px !important;box-shadow: 0px 1px 2px rgba(190, 190, 190, 0.5) !important;-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;margin: 0 auto !important;font-family:'Cookie', cursive !important;-webkit-box-sizing: border-box !important;box-sizing: border-box !important;-o-transition: 0.3s all linear !important;-webkit-transition: 0.3s all linear !important;-moz-transition: 0.3s all linear !important;-ms-transition: 0.3s all linear !important;transition: 0.3s all linear !important;}.bmc-button:hover, .bmc-button:active, .bmc-button:focus {-webkit-box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;text-decoration: none !important;box-shadow: 0px 1px 2px 2px rgba(190, 190, 190, 0.5) !important;opacity: 0.85 !important;color:#FFFFFF !important;}</style><link href="https://fonts.googleapis.com/css?family=Cookie" rel="stylesheet"><a class="bmc-button" target="_blank" href="https://www.buymeacoffee.com/BQKYdkpaN"><img src="https://bmc-cdn.nyc3.digitaloceanspaces.com/BMC-button-images/BMC-btn-logo.svg" alt="Buy me a coffee"><span style="margin-left:5px">Buy me a coffee</span></a>
                                </li>
                        </ul>
                        <p class="copyright text-muted">
                                Copyright © 2020. The opinions expressed herein are my own and do not represent those of my employer or any other third-party views in any way. This work is licensed under a <a href="http://creativecommons.org/licenses/by/4.0/" rel="license">Creative Commons Attribution 4.0 International License</a>.
                                <br />
                                <a href="/feed.rss"><i class="fa fa-rss"></i> RSS Feed</a> | <a href="/feed.atom"><i class="fa fa-rss"></i> Atom Feed</a>
                                <br />
                                <strong><a href="https://wyam.io">Generated by Wyam</a></strong>
                                <br />
                        <br />
                        </p>
                </div>
        </div>
</div>
                </footer> 

                <script src="/assets/js/jquery.min.js"></script>
                <script src="/assets/js/bootstrap.min.js"></script>     
                <script src="/assets/js/highlight.pack.js"></script>   
                <script src="/assets/js/clean-blog.js"></script>
                <script src="/assets/js/d3.v3.min.js"></script>
                <script src="/assets/js/trianglify.min.js"></script>
                <script src="/assets/js/Please-compressed.js"></script>
                <script src="/assets/js/background-check.min.js"></script>

                <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
                <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
                <!--[if lt IE 9]>
                        <script src="/assets/js/html5shiv.js"></script>
                        <script src="/assets/js/respond.min.js"></script>
                <![endif]-->
                
                
                <script>hljs.initHighlightingOnLoad();</script>

                        <script type="text/javascript">                
                                // Header background                        
                                var colors = Please.make_color({
                                        colors_returned: 3,
                                        saturation: .6
                                });
                                var t = new Trianglify({
                                        x_gradient: colors,
                                        y_gradient: ["#FFFFFF"]
                                });
                                var header = document.getElementById("intro-header");
                                var pattern = t.generate(header.clientWidth, header.clientHeight);
                                header.setAttribute('style', 'background-image: ' + pattern.dataUrl);                        
                        </script>

                <script>
                        BackgroundCheck.init({
                                targets: '.intro-header,.navbar',
                                images: '.intro-header'
                        });
                </script>
        </body>
</html>


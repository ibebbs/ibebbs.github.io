<?xml version="1.0" encoding="utf-8"?>
<rss xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
	<channel>
		<title>Ian Bebbington</title>
		<link>http://ian.bebbs.co.uk/</link>
		<description>IObservable&lt;Opinion&gt;</description>
		<copyright>2017</copyright>
		<pubDate>Wed, 21 Jun 2017 22:39:01 GMT</pubDate>
		<lastBuildDate>Wed, 21 Jun 2017 22:39:01 GMT</lastBuildDate>
		<item>
			<title>Getting started with Docker and Apache Kafka</title>
			<link>http://ian.bebbs.co.uk/posts/DockerAndKafka</link>
			<description>&lt;p&gt;For my first blog post of the new year (Happy New Year everyone!!!), I'd like to share some of my recent adventures with Docker on Windows, or, more specifically, Docker on Windows using Nanoserver as the container OS.&lt;/p&gt;</description>
			<guid>http://ian.bebbs.co.uk/posts/DockerAndKafka</guid>
			<pubDate>Wed, 04 Jan 2017 00:00:00 GMT</pubDate>
			<content:encoded>
                                        


&lt;p&gt;For my first blog post of the new year (Happy New Year everyone!!!), I'd like to share some of my recent adventures with Docker on Windows, or, more specifically, Docker on Windows using Nanoserver as the container OS.&lt;/p&gt;
&lt;p&gt;I've been meaning to get up to speed with Docker for a while and, having &lt;a href="http://ian.bebbs.co.uk/posts/ARipStoringTime"&gt;recently acquired a decent new server for the purpose&lt;/a&gt;, decided that a festive period break from some of my &lt;a href="http://ian.bebbs.co.uk/posts/CqrsEsMvvmRxEfSqlUwpPcl"&gt;longer term projects&lt;/a&gt;, would be an ideal time to finally dive in. In typical Bebbs style, "diving in" invariably involves the "deep end" and, as such, it seemed that a great initiation into the containerization waters would be to take &lt;a href="https://kafka.apache.org/"&gt;Apache Kafka&lt;/a&gt; - a service typically run on Linux - and deploy it within a Windows &lt;a href="https://blogs.technet.microsoft.com/windowsserver/2015/04/08/microsoft-announces-nano-server-for-modern-apps-and-cloud/"&gt;Nanoserver&lt;/a&gt; container - a recent release from Microsoft and still a very-much bleeding-edge OS.&lt;/p&gt;
&lt;p&gt;I've been interested in Apache Kafka for quite a while. Described as a "distributed streaming platform" it very much resonates with my "everything is a stream" philosophy. Furthermore, some of &lt;a href="https://www.confluent.io/product/connectors/"&gt;it's connectors&lt;/a&gt; to various traditional RDBMS's offer an intriguing means of moving between 'state store' and 'event store' methodologies.&lt;/p&gt;
&lt;h2&gt;Getting started&lt;/h2&gt;
&lt;p&gt;For the host system, I started with a fresh install of Windows Server 2016 (Desktop Experience for convenience) on a &lt;a href="http://www.dell.com/uk/business/p/poweredge-t20/pd"&gt;Dell T20 Xeon&lt;/a&gt;. Following &lt;a href="https://msdn.microsoft.com/en-gb/virtualization/windowscontainers/quick_start/quick_start_windows_server"&gt;this quick start guide&lt;/a&gt; quickly led to an issue whereby the Docker package couldn't be verified by it's SHA256 hash and therefore refused to install. Fortunately I found a report of the issue and a work around &lt;a href="https://github.com/OneGet/MicrosoftDockerProvider/issues/15"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;I have since reinstalled docker on Windows Server 2016 and did not experience the issue again so it must have been resolved.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;With Docker installed and the &lt;a href="https://hub.docker.com/r/microsoft/dotnet-samples/"&gt;dotnet-samples&lt;/a&gt; example container running, my attention turned to Nanoserver.&lt;/p&gt;
&lt;p&gt;A quick pull and run of the &lt;a href="https://hub.docker.com/r/microsoft/nanoserver/"&gt;Nanoserver image&lt;/a&gt; and I found myself at an interactive command prompt of a deployed container running Nanoserver. This can be done as follows:&lt;/p&gt;
&lt;pre class="prettyprint"&gt;&lt;code&gt;docker pull microsoft/nanoserver:latest
docker run -it --rm microsoft/nanoserver:latest cmd
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Kafka &amp;amp; Zookeeper&lt;/h2&gt;
&lt;p&gt;While looking for a pre-built image of Kafka running on Nanoserver, it quickly became apparent that in order to get an instance of &lt;a href="https://kafka.apache.org/"&gt;Apache Kafka&lt;/a&gt; running, you first need a running instance of &lt;a href="https://zookeeper.apache.org/"&gt;Apache Zookeeper&lt;/a&gt;. While you could technically run both services from within a single container (indeed, Kafka is pre-configured to look for a Zookeeper instance on localhost) I wanted to utilize the core value propositions of containers vs VM instances; namely minimal overhead and composability.&lt;/p&gt;
&lt;p&gt;This meant that I would therefore be building two container images, one for Zookeeper and one for Kafka, both of which would be running on Nanoserver.&lt;/p&gt;
&lt;h2&gt;Building the Zookeeper image&lt;/h2&gt;
&lt;h3&gt;Take 1&lt;/h3&gt;
&lt;img src="https://mbt4mw-dm2306.files.1drv.com/y3m4bDPZgQ871xh0PU_QAcxaL1v9QKVFYFi8Q2uAb82woT97il9OZ_njULBYVK8aNohSJIgAAawJaj-tNunCWe4oXs5LpggjuVv41JGAQ2TFPco7IB1Xx57j6y2X0TUAtNfOQvoJWLxFFHjL5eSgzVeooe_OjURV4VJ8q_QDhN7coA?width=660&amp;amp;height=363&amp;amp;cropmode=none" class="img-responsive" style="margin: auto; width:600px; margin-top: 6px; margin-bottom: 6px;" alt="I built this container up from nothing. When I started here, all there was was nanoserver. Other developers said it was daft to build Zookeeper on nanoserver, but I did it all the same. Just to show'em."&gt;
&lt;blockquote&gt;
&lt;p&gt;I built this [container] up from nothing. When I started here, all there was was [nanoserver]. Other [developers] said it was daft to build [Zookeeper] on [nanoserver], but I did it all the same. Just to show'em.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So, from most of everything I have read about building docker images, it seemed the thing to do was use a &lt;a href="https://docs.docker.com/engine/reference/builder/"&gt;Dockerfile&lt;/a&gt; to start an intermediate container based on the source image (Microsoft/Nanoserver in this instance) then run a script within the intermediate container (as part of the dockerfile) to download, install and configure all the required components. The output of this docker build process would be a new image with the appropriate services running on startup.&lt;/p&gt;
&lt;p&gt;I therefore started by preparing a powershell script that would do just that. Following &lt;a href="http://stackoverflow.com/a/38895811"&gt;this post on StackOverflow&lt;/a&gt; I developed and tested a script on a Windows Server 2016 (Desktop Experience) Virtual Machine. This was done so that I could use &lt;a href="https://technet.microsoft.com/en-us/library/dn818483(v=ws.11).aspx"&gt;snapshotting&lt;/a&gt; in order to roll-back to a clean image anytime a issue with the script was encountered.&lt;/p&gt;
&lt;p&gt;Unfortunately, when it came time to try running Zookeeper I hit the following error at start-up:&lt;/p&gt;
&lt;pre class="prettyprint"&gt;&lt;code&gt;log4j:WARN No appenders could be found for logger (org.apache.zookeeper.server.quorum.QuorumPeerConfig).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
Invalid config, exiting abnormally
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Some quick googling turned up &lt;a href="https://bugzilla.redhat.com/show_bug.cgi?id=1181487#c5"&gt;this issue&lt;/a&gt; but every subsequent comment seemed to suggest that the issue had been resolved. I tried a &lt;a href="https://bugzilla.redhat.com/show_bug.cgi?id=1181487#c8"&gt;frustrating&lt;/a&gt; &lt;a href="https://bugzilla.redhat.com/show_bug.cgi?id=1181487#c9"&gt;number&lt;/a&gt; of &lt;a href="https://bugzilla.redhat.com/show_bug.cgi?id=1181487#c4"&gt;unsuccessful&lt;/a&gt; &lt;a href="http://tech.donghao.org/tag/zookeeper/"&gt;workarounds&lt;/a&gt; until I realized that it was a &lt;a href="https://en.wiktionary.org/wiki/PICNIC"&gt;PICNIC error&lt;/a&gt;. Specifically, while following the StackOverflow post above, I had failed to realize that the version of Zookeeper they specified wasn't actually the latest version and that the issue really had been resolved in a later version. This took a frustratingly and embarrassingly long time but hey, &lt;a href="http://www.goodreads.com/quotes/7678-when-people-say-it-s-always-the-last-place-you-look"&gt;it's always the last place you look&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Anyway, a morning of trial and error resulted in a thing of beauty; a script that would - completely automatically - download, extract, configure, install (as a service!) and run a Zookeeper instance. This is shown below:&lt;/p&gt;
&lt;pre class="prettyprint"&gt;&lt;code&gt;## Download sources
$zipUri = "http://homeserver/download/7z1604-x64.exe" # http://www.7-zip.org/a/7z1604-x64.exe";
$nssmUri = "http://homeserver/download/nssm-2.24.zip" # "https://nssm.cc/release/nssm-2.24.zip"
$javaUri = "http://homeserver/download/jre-8u111-windows-x64.exe" # "http://download.oracle.com/otn-pub/java/jdk/8u111-b14/jre-8u111-windows-x64.exe"
$zookeeperUri = "http://homeserver/download/zookeeper-3.4.9.tar.gz" # "http://apache.mirrors.nublue.co.uk/zookeeper/zookeeper-3.4.9/zookeeper-3.4.9.tar.gz"
$kafkaUri = "http://homeserver/download/kafka_2.11-0.10.1.0.tgz" # "http://apache.mirror.anlx.net/kafka/0.10.1.0/kafka_2.11-0.10.1.0.tgz"

## Application locations
$appDir = "c:\Apps"
$zipDir = $appDir + "\7zip"
$nssmDir = $appDir + "\nssm"
$zookeeperDir = $appDir + "\Zookeeper"

## Data locations
$zookeeperDataDir = $zookeeperDir + "\Data"

## Application executables
$zip = $zipDir + "\7z.exe"
$nssm = $nssmDir + "\nssm.exe"
$zookeeper = $zookeeperDir + "\bin\zkServer.cmd"

function New-TempPath()
{
    if (!(Test-Path -Path C:\Temp))
    {
        New-Item c:\Temp -ItemType Directory
    }
}

function Expand-File($zipFile, $targetPath)
{
    $args = @("e", $zipFile, "-o$targetPath", '-y')
    &amp;amp;$zip $args
}

function Expand-Directory($zipFile, $targetPath)
{
    $args = @("x", $zipFile, "-o$targetPath", '-aoa')
    &amp;amp;$zip $args
}

function Install-7zip()
{
    New-Item "c:\Temp\7zip" -ItemType Directory -Force
    Invoke-WebRequest -Uri $zipUri -OutFile c:\Temp\7zip\7zip.exe
    &amp;amp;"C:\Temp\7zip\7zip.exe" /S /D=$zipDir | Out-Null
    Remove-Item -Path "c:\Temp\7zip\7zip.exe"
}

function Install-NSSM()
{
    New-Item "c:\Temp\NSSM" -ItemType Directory -Force
    Invoke-WebRequest -Uri $nssmUri -OutFile c:\Temp\NSSM\NSSM.zip

    Expand-Directory c:\Temp\NSSM\NSSM.zip c:\Temp\NSSM

    ## Above will expand to a directory containing version name which we want to remove
    ## so we'll move everything up a directory
    $folder = Get-ChildItem -Path c:\Temp\NSSM -Filter "nssm-*"
    Get-ChildItem -Path $folder.FullName -Recurse | Move-Item -destination c:\Temp\NSSM -Force

    New-Item $nssmDir -ItemType Directory -Force
    Copy-Item -Path "c:\Temp\NSSM\win64\nssm.exe" $nssm -Force
}

function Install-Java()
{
    New-Item c:\Temp\Java -ItemType Directory -Force
    Invoke-WebRequest -Uri $javaUri -OutFile c:\temp\Java\Java.exe

    Start-Process "C:\Temp\Java\Java.exe" -ArgumentList "INSTALL_SILENT=Enable INSTALLDIR=C:\Java\Jre AUTO_UPDATE=Disable WEB_JAVA=Disable WEB_ANALYTICS=Disable EULA=Disable REBOOT=Disable NOSTARTMENU=Enable SPONSORS=Disable REMOVEOUTOFDATEJRES=0" -NoNewWindow -Wait

    [Environment]::SetEnvironmentVariable("JAVA_HOME", "C:\Java\Jre", "Machine")

    Remove-Item -Path "C:\Temp\Java\Java.exe"
}

function Get-Zookeeper()
{
    New-Item c:\Temp\Zookeeper -ItemType Directory -Force
    Invoke-WebRequest -Uri $zookeeperUri -OutFile c:\temp\Zookeeper\Zookeeper.tar.gz
    Expand-File c:\temp\Zookeeper\Zookeeper.tar.gz c:\temp\Zookeeper
    Expand-Directory c:\temp\Zookeeper\Zookeeper.tar $zookeeperDir

    ## Above will expand to a directory containing version name which we want to remove
    ## so we'll move everything up a directory
    $folder = Get-ChildItem -Path $zookeeperDir -Filter "zookeeper-*"
    Get-ChildItem -Path $folder.FullName -Recurse | Move-Item -destination $zookeeperDir -Force

    Remove-Item -Path $folder.FullName
    Remove-Item -Path "c:\temp\Zookeeper" -Recurse
}

function Initialize-Zookeeper()
{
    New-Item -Path $zookeeperDataDir -ItemType Directory -Force
    $zookeeperDataLinuxDir = $zookeeperDataDir.Replace('\', '/')

    Copy-Item -Path ($zookeeperDir + '\conf\zoo_sample.cfg') -Destination ($zookeeperDir + '\conf\zoo.cfg') -Force

    $configFile = $zookeeperDir + '\conf\zoo.cfg'
    $logFile = $zookeeperDir + '\conf\log4j.properties'

    $config = [IO.File]::ReadAllText($configFile) -replace "dataDir=[\/\w]*", ("dataDir=" + $zookeeperDataLinuxDir)
    [IO.File]::WriteAllText($configFile, $config)

    $logProperties = [IO.File]::ReadAllText($logFile) -replace "#log4j.rootLogger=DEBUG, CONSOLE, ROLLINGFILE", "log4j.rootLogger=DEBUG, CONSOLE, ROLLINGFILE"
    [IO.File]::WriteAllText($logFile, $logProperties)
}

function Install-Zookeeper()
{
    &amp;amp;$nssm install Zookeeper $zookeeper | Out-Null
    &amp;amp;$nssm set Zookeeper AppDirectory $zookeeperDir | Out-Null

    &amp;amp;$nssm set Zookeeper DisplayName "Zookeeper" | Out-Null
    &amp;amp;$nssm set Zookeeper Description "Apache Zookeeper. Running from $zookeeperDir" | Out-Null
    &amp;amp;$nssm set Zookeeper Start SERVICE_AUTO_START | Out-Null
    &amp;amp;$nssm set Zookeeper ObjectName LocalSystem | Out-Null
    &amp;amp;$nssm set Zookeeper Type SERVICE_WIN32_OWN_PROCESS | Out-Null
}

function Start-Zookeeper()
{
    &amp;amp;$nssm start Zookeeper | Out-Null
}

function Stop-Zookeeper()
{
    &amp;amp;$nssm stop Zookeeper | Out-Null
}

New-TempPath

Install-7zip
Install-NSSM
Install-Java

Get-Zookeeper
Initialize-Zookeeper
Install-Zookeeper
Start-Zookeeper
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With this mighty script in hand I prepared the following dockerfile:&lt;/p&gt;
&lt;pre class="prettyprint"&gt;&lt;code&gt;FROM microsoft/nanoserver
MAINTAINER Ian Bebbington &amp;lt;docker@bebbs.co.uk&amp;gt;
LABEL Description="Zookeeper running on Microsoft Nanoserver" Version="0.1"
ADD Install-Zookeeper.ps1 /
RUN [ "powershell.exe", "C:/Install-Zookeeper.ps1" ]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And watched in dismay as it completely failed to build a container.&lt;/p&gt;
&lt;p&gt;You see, while the script ran perfectly on Windows Server 2016, Nanoserver is a far more constrained environment. It has neither support for 32-bit assemblies nor any graphic stack to speak of so, in short-order, the 7zip utility, Java installer and &lt;a href="https://nssm.cc/"&gt;Non-Sucking Service Manager&lt;/a&gt; executables all failed.&lt;/p&gt;
&lt;p&gt;Well, crap.&lt;/p&gt;
&lt;h3&gt;Take 2&lt;/h3&gt;
&lt;img src="https://mbt5mw-dm2306.files.1drv.com/y3mDYUaDMt02GzcHWl0DE1ASfBA6QbYzEwY-koD_MSkQGr3oRavQLf5jyRBH5TVFEBASZyRxAL00cuoRKuNNJ6lvSfEJD42p0QkZNzUQAFV-TKzdglya78e_ON8lHg7vQBS96aSQL-Hz0AobNgzQ83uHZN1T3nyDQKlumWgM50OZBc?width=660&amp;amp;height=440&amp;amp;cropmode=none" class="img-responsive" style="margin: auto; width:600px; margin-top: 6px; margin-bottom: 6px;" alt="So! I built a second one!"&gt;
&lt;blockquote&gt;
&lt;p&gt;So! I built a second one!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;My next thought was to try salvaging as much of the script as possible by using &lt;a href="https://technet.microsoft.com/en-us/library/ff700227.aspx"&gt;Powershell Remoting&lt;/a&gt; to interactively install the required components and then &lt;a href="https://docs.docker.com/engine/reference/commandline/commit/"&gt;committing&lt;/a&gt; the changes to a new image.&lt;/p&gt;
&lt;p&gt;While, in retrospect, this was undoubtedly the wrong way forward, I was simultaneously fortunate and frustrated by the fact that it simply doesn't seem possible to use powershell remoting with Nanoserver when running within a container. Indeed, after learning more about &lt;a href="https://msdn.microsoft.com/en-us/library/aa384426(v=vs.85).aspx"&gt;WinRM&lt;/a&gt; than I thought possible, posting on &lt;a href="https://social.msdn.microsoft.com/Forums/en-US/e0652324-30e4-4ebb-8689-55205e6d8bc9/enterpssession-to-nanoserver-container-in-docker-access-is-denied?forum=windowscontainers"&gt;Microsoft's Windows Container forums&lt;/a&gt; and even offering my &lt;a href="http://stackoverflow.com/questions/39195068/powershell-remote-access-to-nanoserver-on-docker"&gt;first bounty on StackOverflow&lt;/a&gt; I simply could not find an answer to why it wasn't possible to establish a remote session.&lt;/p&gt;
&lt;p&gt;In the mean time...&lt;/p&gt;
&lt;h3&gt;Take 3&lt;/h3&gt;
&lt;img src="https://mbt3mw-dm2306.files.1drv.com/y3m5x6G99So08jPRwR0n1Msb-8SrSKNaOsB6aLd-CYAk8kSo9xRsa6i9Kd44QzRHin_EKMOpsNMZpacjCkoeGaMsODu6S7zI3NavsVz89vKF4Ot3b9pLEwaHxmg2mxXzvG8rQqCyf7C769ScaTkbPQ5WG2RwwcTHOyRqm0A-4RDNuQ?width=660&amp;amp;height=441&amp;amp;cropmode=none" class="img-responsive" style="margin: auto; width:600px; margin-top: 6px; margin-bottom: 6px;" alt="So, I built a third one..."&gt;
&lt;blockquote&gt;
&lt;p&gt;I built a third one...&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Powershell remoting works beautifully with Nanoserver when running in a Hyper-V virtual machine but Hyper-V networking and Docker networking configurations don't seem to play well together. Indeed, after creating a new virtual-router so that I could access the Nanoserver virtual machine from the host PC, the Docker NAT network became inaccessible. Now, I'm sure it would be possible to dig into the virtual networking configuration and find a way to resolve this but, having spent an incredibly frustrating few hours reconfiguring WinRM, I decided it would be quicker to simply re-install the host OS and start from scratch.&lt;/p&gt;
&lt;h3&gt;Take 4&lt;/h3&gt;
&lt;img src="https://oltxmw-dm2306.files.1drv.com/y3mGaNRc9vDgPZnXfcJZfjYwIEFL32s3nCdk_kA84gB5NGOAoC3SLqP5D7ZffgKPO1VHXQEaRXvVuGCwFjoLKyo7o-gaUXFvGRcZPRPQrifhGNlBmU8RfqR5ZTgKIhxRvzJIMPoXydQ_N5UROZmaUXtKHH3jKtxIobDubPEdWsp-GM?width=660&amp;amp;height=495&amp;amp;cropmode=none" class="img-responsive" style="margin: auto; width:600px; margin-top: 6px; margin-bottom: 6px;" alt="But the fourth one stayed up!"&gt;
&lt;blockquote&gt;
&lt;p&gt;But the fourth one stayed up!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;To accompany the fresh host environment, I decided to employ a fresh approach to building the container image. Namely, use a script to build the container's file system structure on the host PC and then simply copy it wholesale to the container from within the dockerfile. This meant deploying the Java Runtime Environment from a compressed archive rather than silent executable and using the &lt;a href="https://docs.docker.com/engine/reference/builder/#/entrypoint"&gt;dockerfile entrypoint&lt;/a&gt; instruction to run Zookeeper rather than installing it as a service.&lt;/p&gt;
&lt;p&gt;After all the faff and frustration of the previous two attempts (not to mention reinstallation of OS on host PC), this approach was remarkably smooth. Again, in retrospect this was undoubtedly the correct approach but this approach almost certainly benefited from all the knowledge I had accrued from the previous failed attempts. As always, &lt;a href="http://www.goodreads.com/quotes/390439-we-learn-wisdom-from-failure-much-more-than-from-success"&gt;you can learn more from failure than success&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Anyway, in relatively short order, I had a script that prepared and configured the container's file system structure on the host PC and a dockerfile that copied this structure to a new image and set the Zookeeper service as the entrypoint for the image. These are shown below:&lt;/p&gt;
&lt;pre class="prettyprint"&gt;&lt;code&gt;## Download sources
$zipUri = "http://homeserver/download/7z1604-x64.exe" # http://www.7-zip.org/a/7z1604-x64.exe";
$javaUri = "http://homeserver/download/jre-8u111-windows-x64.tar.gz" # "http://download.oracle.com/otn-pub/java/jdk/8u111-b14/jre-8u111-windows-x64.tar.gz"
$zookeeperUri = "http://homeserver/download/zookeeper-3.4.9.tar.gz" # "http://apache.mirrors.nublue.co.uk/zookeeper/zookeeper-3.4.9/zookeeper-3.4.9.tar.gz"
$dockerModuleUri = "http://homeserver/download/Docker.0.1.0.zip" # "https://github.com/Microsoft/Docker-PowerShell/releases/download/v0.1.0/Docker.0.1.0.zip"

## Build location
$buildDir = Get-Location
$tmpDir = $buildDir.Path + "\Temp"
$rootDir = $buildDir.Path + "\Root"
$biuldAppDir = $rootDir + "\Apps"
$buildDataDir = $rootDir + "\Data"
$buildDockerZip = $tmpDir + "\Docker.zip"
$buildDockerModule = $tmpDir + "\Docker"
$buildZipDir = $tmpDir + "\7zip"
$buildJreDir = $biuldAppDir + "\Jre"
$buildZookeeperDir = $biuldAppDir + "\Zookeeper"
$buildZookeeperDataDir = $buildDataDir + "\Zookeeper"

## Temp files
$zipInstaller = $tmpDir + "\7zInstaller.exe"
$jreGzip = $tmpDir + "\Jre.tar.gz"
$jreTar = $tmpDir + "\Jre.tar"
$zooKeeperGzip = $tmpDir + "\Zookeeper.tar.gz"
$zooKeeperTar = $tmpDir + "\Zookeeper.tar"

## Target locations
$targetDir = "C:\"
$appDir = $targetDir + "\Apps"
$dataDir = $targetDir + "\Data"
$jreDir = $appDir + "\Jre"
$zookeeperDir = $appDir + "\Zookeeper"
$zookeeperDataDir = $dataDir + "\Zookeeper"

## Executables
$zip = $buildZipDir + "\7z.exe"
$zookeeper = $zookeeperDir + "\bin\zkServer.cmd"
$docker = "docker"

function New-TempPath()
{
    if (!(Test-Path -Path $tmpDir))
    {
        New-Item $tmpDir -ItemType Directory
    }
}

function Remove-TempPath()
{
    Remove-Item $tmpDir -Recurse -Force
}

function New-RootPath()
{
    Remove-Item $rootDir -Recurse -Force
    New-Item $rootDir -ItemType Directory
}

function Remove-RootPath()
{
    Remove-Item $rootDir -Recurse -Force
}

function Expand-File($zipFile, $targetPath)
{
    $args = @("e", $zipFile, "-o$targetPath", '-y')
    &amp;amp;$zip $args | Out-Host
}

function Expand-Directory($zipFile, $targetPath)
{
    $args = @("x", $zipFile, "-o$targetPath", '-aoa')
    &amp;amp;$zip $args | Out-Host
}

function Install-DockerModule()
{
    Invoke-WebRequest -Uri $dockerModuleUri -OutFile $buildDockerZip
    Expand-Archive -Path $buildDockerZip -DestinationPath $buildDockerModule -Force

    Import-Module $buildDockerModule
}

function Remove-DockerModule()
{
    Remove-Module $buildDockerModule
}

function Install-7zip()
{
    $folder = New-Item $buildZipDir -ItemType Directory -Force
    Invoke-WebRequest -Uri $zipUri -OutFile $zipInstaller
    &amp;amp;$zipInstaller /S /D=$folder | Out-Null
    Remove-Item -Path $zipInstaller
}

function Remove-7zip()
{
    Remove-Item $buildZipDir -Recurse -Force
}

function Get-Java()
{
    Invoke-WebRequest -Uri $javaUri -OutFile $jreGzip
    Expand-File $jreGzip $tmpDir
    Expand-Directory $jreTar $buildJreDir

    ## Above will expand to a directory containing version name which we want to remove
    ## so we'll move everything up a directory
    $folder = Get-ChildItem -Path $buildJreDir -Filter "jre*"
    Get-ChildItem -Path $folder.FullName -Recurse | Move-Item -destination $buildJreDir -Force

    Remove-Item -Path $folder.FullName -Force
    Remove-Item -Path $jreGzip -Force
    Remove-Item -Path $jreTar -Force
}

function Get-Zookeeper()
{
    Invoke-WebRequest -Uri $zookeeperUri -OutFile $zooKeeperGzip
    Expand-File $zooKeeperGzip $tmpDir
    Expand-Directory $zooKeeperTar $buildZookeeperDir

    ## Above will expand to a directory containing version name which we want to remove
    ## so we'll move everything up a directory
    $folder = Get-ChildItem -Path $buildZookeeperDir -Filter "zookeeper-*"
    Get-ChildItem -Path $folder.FullName -Recurse | Move-Item -destination $buildZookeeperDir -Force

    Remove-Item -Path $folder.FullName -Force
    Remove-Item -Path $zooKeeperTar -Force
    Remove-Item -Path $zooKeeperGzip -Force
}

function Initialize-Zookeeper()
{
    New-Item -Path $buildDataDir -ItemType Directory -Force
    New-Item -Path $buildZookeeperDataDir -ItemType Directory -Force

    $zookeeperDataLinuxDir = $zookeeperDataDir.Replace('\', '/')

    Copy-Item -Path ($buildZookeeperDir + '\conf\zoo_sample.cfg') -Destination ($buildZookeeperDir + '\conf\zoo.cfg') -Force

    $configFile = $buildZookeeperDir + '\conf\zoo.cfg'
    $logFile = $buildZookeeperDir + '\conf\log4j.properties'

    $config = [IO.File]::ReadAllText($configFile) -replace "dataDir=[\/\w]*", ("dataDir=" + $zookeeperDataLinuxDir)
    [IO.File]::WriteAllText($configFile, $config)

    $logProperties = [IO.File]::ReadAllText($logFile) -replace "#log4j.rootLogger=DEBUG, CONSOLE, ROLLINGFILE", "log4j.rootLogger=DEBUG, CONSOLE, ROLLINGFILE"
    [IO.File]::WriteAllText($logFile, $logProperties)
}

function New-DockerImage()
{
    Build-ContainerImage -Path $buildDir -Repository "ibebbs/nanozoo:latest"
}


# Setup directory structure
New-TempPath
New-RootPath

# Install required tools
Install-DockerModule
Install-7zip

# Get components
Get-Java
Get-Zookeeper
Initialize-Zookeeper

# Build docker image
New-DockerImage

# Cleanup
Remove-DockerModule
Remove-7zip
Remove-TempPath
Remove-RootPath
&lt;/code&gt;&lt;/pre&gt;
&lt;pre class="prettyprint"&gt;&lt;code&gt;FROM microsoft/nanoserver
MAINTAINER Ian Bebbington &amp;lt;docker@bebbs.co.uk&amp;gt;
LABEL Description="Zookeeper running on Microsoft Nanoserver" Version="0.1"
ADD Root /
ADD Start-Zookeeper.ps1 /
RUN setx /M JAVA_HOME C:\Apps\Jre
EXPOSE 2181
ENTRYPOINT [ "powershell.exe", "C:/Start-Zookeeper.ps1" ]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And this one worked. This one started. This one stayed up!&lt;/p&gt;
&lt;h2&gt;Building the Kafka image&lt;/h2&gt;
&lt;p&gt;With the Zookeeper scripts as a pattern, it was ludicrously easy to script up another image for Kafka. Just a few changes to file names and configuration parameters and Kafka started almost first time.&lt;/p&gt;
&lt;p&gt;I won't copy the script or dockerfile here as they're extremely similar to the Zookeeper versions. Instead, all scripts and files used above can be found in my &lt;a href="https://github.com/ibebbs/Docker"&gt;Docker repository on Github&lt;/a&gt; and the resultant images can be found on &lt;a href="https://hub.docker.com/r/ibebbs/"&gt;Docker hub&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Moving forward&lt;/h2&gt;
&lt;img src="https://oltymw-dm2306.files.1drv.com/y3mIddKC619ngQYu9mwpyXXi9YgUp-MVwZ9_lR8JRla2AKvMm91LwnW2p0G3GkOZj5X_TXQUap_XULDAaiCWMC5Hx0ZvjKdAjeW7YWqacnwEAgCSZgmgBF1DCH83Zcywki6qKmOXbKkZO_SWmvQHpajyOQRHO1pJNB3ak2YpT6DmX8?width=660&amp;amp;height=371&amp;amp;cropmode=none" class="img-responsive" style="margin: auto; width:600px; margin-top: 6px; margin-bottom: 6px;" alt="But I don't want any of that!"&gt;
&lt;blockquote&gt;
&lt;p&gt;But I don't want any of that!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Moving forward, I need to address a couple of short-comings in the Kafka script (specifically the hard-coded IP address for the Zookeeper container) and then look to use Docker Compose to automatically bring up Zookeeper and Kafka on demand.&lt;/p&gt;
&lt;p&gt;It's been an interesting journey so far and I've not even begun to actually use the deployed services yet! Still, it is truly magical to run a docker container and see it boot an entire Windows server and service in just 10-20 seconds and a few hundred Mb.&lt;/p&gt;



                                </content:encoded>
		</item>
		<item>
			<title>A sentiment(al) analysis of why Red Dwarf is no longer funny</title>
			<link>http://ian.bebbs.co.uk/posts/ASentimentalAnalysisOfRedDwarfPartII</link>
			<description>&lt;p&gt;I've recently been working on a project that required some natural language processing. After a surprisingly brief search I came upon the &lt;a href="https://stanfordnlp.github.io/CoreNLP/"&gt;Stanford CoreNLP&lt;/a&gt; suite of tools and after playing with their &lt;a href="http://corenlp.run/"&gt;online demo&lt;/a&gt; was astounded at the capabilities it provided. Furthermore, it was free, could be run such that it provided a basic HTTP API and came packaged with everything it needed save a copy of the JRE.&lt;/p&gt;</description>
			<guid>http://ian.bebbs.co.uk/posts/ASentimentalAnalysisOfRedDwarfPartII</guid>
			<pubDate>Wed, 12 Apr 2017 00:00:00 GMT</pubDate>
			<content:encoded>
                                        


&lt;p&gt;I've recently been working on a project that required some natural language processing. After a surprisingly brief search I came upon the &lt;a href="https://stanfordnlp.github.io/CoreNLP/"&gt;Stanford CoreNLP&lt;/a&gt; suite of tools and after playing with their &lt;a href="http://corenlp.run/"&gt;online demo&lt;/a&gt; was astounded at the capabilities it provided. Furthermore, it was free, could be run such that it provided a basic HTTP API and came packaged with everything it needed save a copy of the JRE.&lt;/p&gt;
&lt;p&gt;Having recently &lt;a href="http://ian.bebbs.co.uk/posts/DockerAndKafka"&gt;enjoyed a lot of success&lt;/a&gt; running &lt;a href="https://zookeeper.apache.org/"&gt;various&lt;/a&gt; &lt;a href="https://kafka.apache.org/"&gt;JAVA&lt;/a&gt; &lt;a href="https://neo4j.com/product/"&gt;services&lt;/a&gt; inside a &lt;a href="https://www.docker.com/"&gt;Docker container&lt;/a&gt; running &lt;a href="https://technet.microsoft.com/en-us/windows-server-docs/get-started/getting-started-with-nano-server"&gt;Windows Nano Server&lt;/a&gt;, I decided to see if CoreNLP could be run like this too. Copying my previous &lt;a href="https://github.com/ibebbs/Docker/blob/master/Nanoserver-Zookeeper/Build.ps1"&gt;build script&lt;/a&gt; and &lt;a href="https://github.com/ibebbs/Docker/blob/master/Nanoserver-CoreNLP/Build.ps1"&gt;amending it&lt;/a&gt; to build a &lt;a href="https://hub.docker.com/r/ibebbs/nanonlp/"&gt;container running CoreNLP&lt;/a&gt; was ludicrously easy and in no time I had a local API I could hit to perform all the natural language processing I needed.&lt;/p&gt;
&lt;p&gt;Now, while the project I was working on mainly required the &lt;a href="https://stanfordnlp.github.io/CoreNLP/ner.html"&gt;"Named Entity Recognition"&lt;/a&gt; and &lt;a href="https://stanfordnlp.github.io/CoreNLP/openie.html"&gt;"Open IE"&lt;/a&gt; annotators, I was intrigued to see that CoreNLP also included a basic &lt;a href="https://stanfordnlp.github.io/CoreNLP/sentiment.html"&gt;Sentiment&lt;/a&gt; annotator. Given that I had written part one of this post back in January, had noted at the time how much I'd like to do sentiment analysis on the transcripts of Red Dwarf, and that I hadn't written a blog post since, I decided to take some time out and perform the sentiment analysis so that I could write this post.&lt;/p&gt;
&lt;p&gt;Again employing &lt;a href="https://jupyter.org/"&gt;Project Jupyter&lt;/a&gt; hosted on &lt;a href="https://notebooks.azure.com/"&gt;Azure Notebooks&lt;/a&gt; and using &lt;a href="http://fsharp.org/"&gt;F#&lt;/a&gt; coupled with &lt;a href="https://fslab.org/"&gt;FsLab&lt;/a&gt; as my primary language and toolkit, I had a lot of fun performing the analysis. Like last time, you can find the &lt;a href="https://github.com/ibebbs/RedDwarfAnalysis/blob/master/SentimentAnalysisWithCoreNLP.ipynb"&gt;full notebook&lt;/a&gt; and source material in my &lt;a href="https://github.com/ibebbs/RedDwarfAnalysis"&gt;Github repository&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Note: As before, Github provides a "limited rendering only" so, to see all the charts running correctly you need to use the 'nbviewer' link shown below to see a full rendering of the notepad.&lt;/p&gt;
&lt;img src="https://mvpfyw-dm2306.files.1drv.com/y3mppldGfaYEhvWkV7mdUw26-lP3SOzlMTGFbf8slchIfjBL57IH-GrJev6ai_rISiHBKrom7Abg9YFjfhZ1ArOFT7a7mh4gJuGq-CErv1dun48GQC_BdhMV08fh6hbw400d9nHSEXJ0jA2nPBIrpOPNrOz0I3lVY1tu_L656ylQKg?width=660&amp;amp;height=283&amp;amp;cropmode=none" class="img-responsive" style="margin: auto; width:660; margin-top: 6px; margin-bottom: 6px;" alt="Open external view with nbviewer"&gt;
&lt;p&gt;The best bit of all (note: spoilers ahead!) is that it seems my original conclusion may indeed have been wrong... or at least mis-attributed.&lt;/p&gt;



                                </content:encoded>
		</item>
		<item>
			<title>MonsterPi</title>
			<link>http://ian.bebbs.co.uk/posts/MonsterPi</link>
			<description>&lt;p&gt;I've long loved the idea of home automation. From X10 and LightwaveRF through to modern Bluetooth and Wifi connected devices, I have played with dozens of technologies in search of home automation nirvana. But recently I have watched with growing bewilderment at the incredible number of "cloud-connected" home automation devices being released and the eagerness with which they're snapped up by naive consumers hungry to control everything from the carefree comfort of their iPhone.&lt;/p&gt;</description>
			<enclosure url="http://ian.bebbs.co.uk/y3mJgriVgVEFWAdldfy7Bj9MEJznwpJtJ4NFHotqnwZ0rWbC2h8ewxTr9MSxr6LCGyQBB-TvmYCV9j4YVZbu2EtVHagPnyc6O_LIZV2NqAiPfrfNkZ8P-XBV6SI0GiL7zx0iuOTniKjZ-Gq9_lN6SLkkIMHMx14EE3l2XuZQ1cTPfI%253Fwidth=967&amp;height=273&amp;cropmode=none" length="0" type="image" />
			<guid>http://ian.bebbs.co.uk/posts/MonsterPi</guid>
			<pubDate>Sat, 08 Oct 2016 00:00:00 GMT</pubDate>
			<content:encoded>
                                        


&lt;h2&gt;Preface&lt;/h2&gt;
&lt;p&gt;I've long loved the idea of home automation. From X10 and LightwaveRF through to modern Bluetooth and Wifi connected devices, I have played with dozens of technologies in search of home automation nirvana. But recently I have watched with growing bewilderment at the incredible number of "cloud-connected" home automation devices being released and the eagerness with which they're snapped up by naive consumers hungry to control everything from the carefree comfort of their iPhone.&lt;/p&gt;
&lt;p&gt;You see, while you can buy a myriad of IoT devices off the shelf nowadays, they nearly all come with some form of "cloud-service" that is necessary in order for the device to work as sold. As the more wily of reader will no doubt be aware, this exposes your home network to innumerable &lt;a href="https://it.slashdot.org/story/16/10/03/1359200/source-code-for-iot-botnet-mirai-which-took-down-krebs-on-security-website-with-ddos-attack-released"&gt;security concerns&lt;/a&gt;, &lt;a href="https://it.slashdot.org/story/16/08/08/1449221/hackers-make-the-first-ever-ransomware-for-smart-thermostats"&gt;potential abuses&lt;/a&gt; and an &lt;a href="https://tech.slashdot.org/story/16/01/14/1347243/nest-thermostat-bug-leaves-owners-without-heating"&gt;external point of failure&lt;/a&gt; that cannot be closed/fixed without sacrificing some or all of the functionality of the new fangled device.&lt;/p&gt;
&lt;p&gt;While I understand the ostensible benefits of this approach (ease of setup, remote use without manually opening firewall ports, centralised patching and upgrading, etc), sacrificing control of (potentially hazardous or invasive) devices within my home is not a value proposition I am comfortable with. I would very much like to see a new (or is that old?) breed of device that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;does not require an internet connection to operate with all features intact,&lt;/li&gt;
&lt;li&gt;with which the owner can proactively decide what control and data is available to servers outside of the home network and,&lt;/li&gt;
&lt;li&gt;which surface simple, open interfaces to the owner allowing them to completely control all aspects of the device.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In short, I'd like to drop the 'Inter' from IoT and expand the 'net' to become a 'Network of Things', or NoT.&lt;/p&gt;
&lt;p&gt;In terms of the consumer market, I am undoubtedly swimming against the tide here. Fortunately with the rise of hobbyist devices such as Arduino and RaspberryPi, IoT technologies have been democratised to the extent that anyone with just the smallest ability with a soldering iron and faculty with an IDE can create such devices for themselves.&lt;/p&gt;
&lt;p&gt;Ladies and gentlemen, I present my first (finished) NoT device, the...&lt;/p&gt;
&lt;h1&gt;MonsterPi&lt;/h1&gt;
&lt;p&gt;Years ago, while setting up my (living room) home cinema, I came across the &lt;a href="http://www.adverts.ie/home-audio/monster-hts-1600-home-theatre-powercenter-5-outlet-with-stage-2-clean-power-purge-protector-uk-version/7055360"&gt;Monster Power HTS 1600&lt;/a&gt; on &lt;a href="https://www.scan.co.uk/"&gt;Scan's website&lt;/a&gt;. It seems Monster Products were end-of-lining their power products in the UK (the above link was literally the only one I could find that still works!) and Scan were selling them off at £30 a piece. It was exactly what I was looking for at the time and, not being one to pass up a bargain, I bought two.&lt;/p&gt;
&lt;img src="https://0oiczq.dm2302.livefilestore.com/y3mwcGE45bQwRPaCXRNuB9tGejIT6aSpahgDN02u9ag661uCa_ZmYbNnaKz2aW81rjG8BTgsUhtZ0aDQ8ioIsagbcFP6LtM-IgV_rcPoCs8UI4dZNegv8JembGIuIr7ETkwTZwOTEUjsAAbQHy0jCyAGNkGu3NJZLkXjnGwgzUMlTI?width=700&amp;amp;height=466&amp;amp;cropmode=none" class="img-responsive" style="margin: auto; width:600px; margin-top: 6px; margin-bottom: 6px;" alt="One of the few remaining pictures online of the Monster Power HTS 1600 UK version"&gt;
&lt;p&gt;Both devices found homes powering and protected my front room and study AV equipment and, to this day, I am still very happy with them. However, I always felt that Monster had missed a trick with this product: the ability to individually and - ideally - remotely turn each of the sockets on the unit on and off as desired.&lt;/p&gt;
&lt;p&gt;When Microsoft announced that Windows IoT Core would run on a RaspberryPi the learning curve ahead of making this happen evaporated and I knew it was something I wanted to do. Unfortunately both Monster Power devices were in full time use and I didn't want to potentially sacrifice one on a project that might not work and could consign the device to the scrap yard. Given that you couldn't purchase the UK version of these devices any more and that, when they occasionally appeared on eBay, they'd invariably be priced at £100+ the project was indefinitely parked.&lt;/p&gt;
&lt;p&gt;Finally, a couple of months ago, a 'used' one appeared on eBay without a reserve and with bidding at thirty odd quid having just a few hours to go. I placed a bid without really hoping it would be successful and, to my surprise, ended up winning the auction for £40'ish including delivery. When it arrived I was delighted to find that it seemed to be brand new and unused (twisty wire on all the cables, supplementary cables still in sealed bags, etc). Plugging it in proved that everything worked as it should so everything was in place to build the MonsterPi.&lt;/p&gt;
&lt;h2&gt;Software&lt;/h2&gt;
&lt;p&gt;While waiting for the time and space required to start hacking up the hardware, I took a look into what was required to write a headless UWP app, self-hosting a HTTP REST endpoint that could be deployed to the RaspberryPi and set values on the GPIO pins necessary to control the &lt;a href="http://www.waveshare.com/rpi-relay-board.htm"&gt;Waveshare RaspberryPi Relay Board&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It didn't take long to find &lt;a href="https://github.com/tomkuijsten/restup"&gt;Restup&lt;/a&gt;, a beautifully simple if somewhat minimal 'Webserver for Universal Windows Apps'. The github repository provided ample samples for writing various types of 'controller' and in less than an hour, I had a working solution.&lt;/p&gt;
&lt;p&gt;Given the Waveshare relay board just required GPIO pins to be set in order to control the relay (low to close, high to open) this was achieved very simply using the classes available in the &lt;a href="https://msdn.microsoft.com/library/windows/apps/windows.devices.gpio.aspx"&gt;Windows.Devices.Gpio namespace&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Deployment was taken care of by Visual Studio and configuration performed using the Windows IoT Core Device Portal. The Device Portal allows you to configure deployed apps to run at start up thereby creating a "service-like" experience for headless apps.&lt;/p&gt;
&lt;img src="https://0oidzq.dm2302.livefilestore.com/y3myuJMpQJ0iee-DzmucnuUu2r69IZ2c_KNukHEjoPB0--qRcndLbk0EwUszv8MLYj207fNIWEelQn4TYtW40FVp_y77RW-aTL3AcxQWcowLMB_zsw9jYdH0SwepJ0l6_XC0z9erqkYt4k4TaUBdnqxPYUlSf786xKVx1zwTC4jEsg?width=1363&amp;amp;height=905&amp;amp;cropmode=none" class="img-responsive" style="margin: auto; width:600px; margin-top: 6px; margin-bottom: 6px;" alt="Using the device portal to set Start-up apps"&gt;
&lt;p&gt;Full source code is available on my &lt;a href="https://github.com/ibebbs/MonsterPi"&gt;Github account&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Hardware&lt;/h2&gt;
&lt;p&gt;The hardware side of things proved to be both easier and more tricky than I expected.&lt;/p&gt;
&lt;p&gt;Initially I thought fitting the Pi was going to be child's-play when, upon opening the Monster Power chasis I found that the configuration of sockets providing isolation for RJ11 and RJ45 devices (phone and ethernet respectively) matched the layout of the ethernet and USB connectors of the Pi almost perfectly. Furthermore, these sockets were mounted on an easily removable daughter-board providing an obvious way to fix the pi in place.&lt;/p&gt;
&lt;img src="https://nhtcwg.dm2302.livefilestore.com/y3mKq_74GixCoND7XvyavcDTjnNPdW2xd58RqEk1KZtO4f8nqKhcwyGbMRJqYhtuIHOmJCrJo6O-pltKB1pW0DFAHNy2oi6CNuyOle6TUxrjb4UzQo5_uDIPhNAP1uF8llVVLds4J6Bcp5b2s0GRI_T1uLx__qES0eUimFSxaTWddk?width=1024&amp;amp;height=576&amp;amp;cropmode=none" class="img-responsive" style="margin: auto; width:600px; margin-top: 6px; margin-bottom: 6px;" alt="The sockets on the RaspberryPi aligned almost perfectly with the RJ11 isolator sockets"&gt;
&lt;p&gt;While alignment wasn't perfect, filing the holes in the chasis to provide access to all ports would be easy.&lt;/p&gt;
&lt;p&gt;Complexity came when I realised that, although the Monster Power included a micro-controller driven digital display, it was unlikely that the DC subsystem in place to power this would be adequate for the RaspberryPi. So here I had to get creative.&lt;/p&gt;
&lt;p&gt;I bought a small &lt;a href="https://www.amazon.co.uk/gp/product/B01H0OH3PI/ref=oh_aui_detailpage_o02_s01?ie=UTF8&amp;amp;psc=1"&gt;12v 80W switching power supply&lt;/a&gt; which would fit down one side of the Moster Power chasis and wired it to the 240v input of using the conveniently provided ring terminals. I used a &lt;a href="http://ian.bebbs.co.uk/posts/3DPrintingWithTheCelRobox"&gt;custom printed bracket&lt;/a&gt; to secure the PSU in place while providing a platform to mount the &lt;a href="http://www.waveshare.com/rpi-relay-board.htm"&gt;Waveshare RaspberryPi Relay Board&lt;/a&gt; above, in close proximity to the sockets I wanted to control with the relays.&lt;/p&gt;
&lt;p&gt;The live wire to the three sockets to be controlled by relays were daisy-chained together so one side of this wire was cut from each socket and connected to the normally-closed output of the relay (extending where necessary using crimp connectors).&lt;/p&gt;
&lt;p&gt;This is shown below:&lt;/p&gt;
&lt;img src="https://zphbnq.dm2302.livefilestore.com/y3mBGmj_-221eHr5ChhkWQfyEOjewU-6JEKOKlqHR-UqR0eMZdf2xPtI5qfP_K1rjguEplhOWObkMJ2fiGI4T4WSQOGh8GfodMLL2lPWD5QZFnBFPYwPUID3FZgVTYndwwL5jDybtRBC6JE_hhcTokWGrmppuLoHccNLsmFZyUsAvM?width=576&amp;amp;height=1024&amp;amp;cropmode=none" class="img-responsive" style="margin: auto; width:600px; margin-top: 6px; margin-bottom: 6px;" alt="12v switching PSU with relay board mounted in custom bracket"&gt;
&lt;p&gt;While the perspective in the picture makes it look like it hangs over the edge of the chasis, it does actually fit. It's tight, but it fits.&lt;/p&gt;
&lt;p&gt;I used a &lt;a href="https://www.amazon.co.uk/gp/product/B01CUA5Q74/ref=oh_aui_detailpage_o02_s00?ie=UTF8&amp;amp;psc=1"&gt;12V To 5V 3A 15W Power Converter Regulator with Micro USB Cable&lt;/a&gt; to power the RaspberryPi and - given I now had plenty of power to spare, decided to also fit the &lt;a href="https://www.amazon.co.uk/Step-Down-Step-down-Power-Module/dp/B00ENE55SQ/ref=pd_nav_hcs_bia_t_1?ie=UTF8&amp;amp;psc=1&amp;amp;refRID=J91844QA901NNMPP9XHJ"&gt;4-port USB power module&lt;/a&gt; I had bought, but not used, for our &lt;a href="http://ian.bebbs.co.uk/posts/WotNoBlogPosts"&gt;travels earlier in the year&lt;/a&gt;. Again, this aligned almost perfectly in the remaining holes in the Monster Power chasis.&lt;/p&gt;
&lt;img src="https://nhtcwg.dm2302.livefilestore.com/y3mKq_74GixCoND7XvyavcDTjnNPdW2xd58RqEk1KZtO4f8nqKhcwyGbMRJqYhtuIHOmJCrJo6O-pltKB1pW0DFAHNy2oi6CNuyOle6TUxrjb4UzQo5_uDIPhNAP1uF8llVVLds4J6Bcp5b2s0GRI_T1uLx__qES0eUimFSxaTWddk?width=1024&amp;amp;height=576&amp;amp;cropmode=none" class="img-responsive" style="margin: auto; width:600px; margin-top: 6px; margin-bottom: 6px;" alt="The sockets on the USB power module aligned almost perfectly with the RJ11 isolator sockets"&gt;
&lt;p&gt;Both these were mounted - upside down - on a custom printed bracket using the screw holes for the original daughter board to secure it in place. Terminal connectors were used to provide connection to the power supply just in case I needed to remove the components at a later date for some reason.&lt;/p&gt;
&lt;img src="https://zphcnq.dm2302.livefilestore.com/y3mMyQU1JCq5r9g0pnmuLGrrVm1-_D8Bq9UVm5uZgUD2lx9t3LVnjEIgWzqzwKeKbju29iHfBntmTX0FPZFKzWK6RAnFwWw-rxMfWyslJf52FN4XkFlglfGHc97-z24asPfR-NLEAd8nEE9e8X9V-tEP_wen7kF0zsAchvQJYwOjJk?width=576&amp;amp;height=1024&amp;amp;cropmode=none" class="img-responsive" style="margin: auto; width:600px; margin-top: 6px; margin-bottom: 6px;" alt="Mounting the RaspberryPi and USB power module on a custom bracket"&gt;
&lt;p&gt;Finally, boards were fixed in place and a ribbon cable connected between the GPIO socket on the RaspberryPi and the relay board to allow control of the relays.&lt;/p&gt;
&lt;img src="https://nhtewg.dm2302.livefilestore.com/y3mYTQzOsTukjrHcb69XX-50V18rcvHJUP4R8u6hmsgR1yAm2DCpFzoK9UdUNyfxseH59qtuRbVeIVVX9AM1h8lde7IbZDOPRYz0le7cac0bFkoC4pzsyjVKNO3QapjDdlCK-awMaL_NOhYEw6rW4_7lMTobDNRqGrK17yE1SmeRHA?width=1024&amp;amp;height=576&amp;amp;cropmode=none" class="img-responsive" style="margin: auto; width:600px; margin-top: 6px; margin-bottom: 6px;" alt="Brackets in place and RaspberryPi fitted"&gt;
&lt;p&gt;After a thorough check to make sure there were no shorts on any of the connections and that I had connected polarity correctly, I plugged it in and turned it on....&lt;/p&gt;
&lt;h2&gt;Testing&lt;/h2&gt;
&lt;p&gt;... and everything worked.&lt;/p&gt;
&lt;p&gt;The RaspberryPi booted but didn't switch the sockets on as - I think - not having a network connection meant it couldn't get an IP address which meant the software hadn't enabled to relays due to being configured to fail safe (leave the sockets off). The power indicator on the USB power module suggested it was working fine too.&lt;/p&gt;
&lt;p&gt;Next I relocated to another room where a network connection was available. I plugged it in, turned it on and saw the network connection LED's start blinking. After 15-20 seconds or so while the RaspberryPi booted, the relays tickets on and I was able to hit the REST endpoint from &lt;a href="https://www.getpostman.com/"&gt;Postman&lt;/a&gt;. I'd even managed to get the names of the sockets on the rest service to match the sockets the relays were controlling, although I admit this was more luck than judgement (I had completely expected to have to re-configure the software to match after the hardware was complete).&lt;/p&gt;
&lt;p&gt;I plugged in a small table lamp and recorded this:&lt;/p&gt;
&lt;iframe src="https://onedrive.live.com/embed?cid=03DF8A28BD9D0BC9&amp;amp;resid=3DF8A28BD9D0BC9%21266322&amp;amp;authkey=AI5IzCXupiYl-CU" width="600px" height="340px" frameborder="0" scrolling="no" allowfullscreen=""&gt;&lt;/iframe&gt;
&lt;p&gt;Boom! (Figuratively speaking of course... the MonsterPi didn't literally go boom... yet :0)&lt;/p&gt;



                                </content:encoded>
		</item>
		<item>
			<title>Nano2Docker</title>
			<link>http://ian.bebbs.co.uk/posts/Nano2Docker</link>
			<description>&lt;p&gt;While my current contract doesn't leave much time for personal projects, I have made some progress on my current project (details on exactly what this is to follow). In fact, some of the smaller, peripheral services have their primary use-cases functionally complete and are ready for deployment and I am now faced with the question: Deployment to where?&lt;/p&gt;</description>
			<guid>http://ian.bebbs.co.uk/posts/Nano2Docker</guid>
			<pubDate>Mon, 01 Jan 0001 00:00:00 GMT</pubDate>
			<content:encoded>
                                        


&lt;h1&gt;Deployment at last&lt;/h1&gt;
&lt;p&gt;While my current contract doesn't leave much time for personal projects, I have made some progress on my current project (details on exactly what this is to follow). In fact, some of the smaller, peripheral services have their primary use-cases functionally complete and are ready for deployment and I am now faced with the question: Deployment to where?&lt;/p&gt;
&lt;h1&gt;Fabric or Container&lt;/h1&gt;
&lt;p&gt;Given this project constitutes multiple micro-services using message based asynchronous communication with the potential to scale services horizontally, I required some form of elastic service fabric. Furthermore, I wanted a local development environment which would simulate a cluster of machines but with which I could monkey about as much as I liked without fear of accidentally incurring massive hosting fees in the cloud.&lt;/p&gt;
&lt;p&gt;As I had just upgraded my home server with plenty of memory, I decided to use one of more virtual machines running on this server to host the environment, but which technology to use?&lt;/p&gt;
&lt;p&gt;Initially I had intended to use a &lt;a href="https://docs.microsoft.com/en-us/azure/service-fabric/service-fabric-get-started-with-a-local-cluster"&gt;local Service Fabric cluster&lt;/a&gt;. However, upon further investigation I found that the SDK and API introduced significant friction to the development process (needing additional projects for supplying manifest / configuration data for services, overly complex deployment scripts, etc). Even the 'guest executable' approach seemed overly complex and I quickly went off this approach.&lt;/p&gt;
&lt;p&gt;My second thought was &lt;a href="https://www.docker.com/"&gt;Docker&lt;/a&gt;; specifically the creation of a local &lt;a href="https://docs.docker.com/engine/swarm/"&gt;Docker Swarm&lt;/a&gt; which I could deploy servies to with &lt;a href="https://docs.docker.com/compose/"&gt;Docker Compose&lt;/a&gt;. &lt;a href="https://docs.docker.com/machine/"&gt;Docker Machine&lt;/a&gt; made short work of provisioning Docker hosts in Hyper-V but with one caveat: it's &lt;a href="http://boot2docker.io/"&gt;boot2docker&lt;/a&gt; image would only run Linux based containers and, while many of the services I have written / will write run quite happily on .NET Core, some require packages that do not yet provide support for .NET Core / Standard.&lt;/p&gt;
&lt;p&gt;Given that a recent update made &lt;a href="https://docs.microsoft.com/en-us/virtualization/windowscontainers/manage-containers/swarm-mode"&gt;Swarm mode available to Windows Server 2016&lt;/a&gt; host operating systems, I decided I would look into provisioning a series of Windows Server VM's with container support and configure Docker on these VM's to operate in swarm mode.&lt;/p&gt;
&lt;h1&gt;Using Nano Server as a Docker host&lt;/h1&gt;
&lt;p&gt;While I had previously used Microsoft Nano Server as a guest OS for containerized apps, I hadn't realised that it was possible to use it as a host OS for Docker until I came across &lt;a href="https://docs.microsoft.com/en-us/virtualization/windowscontainers/deploy-containers/deploy-containers-on-nano"&gt;this article&lt;/a&gt;. For those not familiar with Nano Server it is an extremely slimmed down (the OS image is less than 170Mb) and fast booting (5-10 seconds), headless version of Windows Server 2016 which, given it is capable of acting as a Docker host, makes it 'boot2docker' but for Windows containers.&lt;/p&gt;
&lt;p&gt;Nano Server is shipped with Windows Server 2016 and is accompanied by a Powershell module which provides some amazing facilities for working with Nano Server images. &lt;a href="https://docs.microsoft.com/en-us/windows-server/get-started/deploy-nano-server"&gt;This document&lt;/a&gt; shows how to use this Powershell module to create customised Nano Server images as either a '.wim', a '.iso' or - most interestingly for me - a '.vhdx'. In short, the following powershell command will create a virtual HD that you can directly attach to a virtual machine and will boot directly into Nano Server with support for (but no utilites to provide) containers:&lt;/p&gt;
&lt;pre class="prettyprint"&gt;&lt;code&gt;New-NanoServerImage -Edition Standard -DeploymentType Guest -MediaPath &amp;lt;path to root of media&amp;gt; -BasePath &amp;lt;path in which to build the image&amp;gt; -TargetPath &amp;lt;destination path&amp;gt;\NanoServer.vhdx -ComputerName &amp;lt;computer name&amp;gt; -Containers -EnableRemoteManagementPort
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This command will also open &lt;a href="https://msdn.microsoft.com/en-us/library/aa384426(v=vs.85).aspx"&gt;WinRM&lt;/a&gt; ports on the Nano Server which allows you to use &lt;a href="https://technet.microsoft.com/en-us/library/ff700227.aspx"&gt;PS Remoting&lt;/a&gt; to remote into the virtual machine and examine it's state; indispensable for debugging purposes.&lt;/p&gt;
&lt;h1&gt;Installing Docker as part of a Nano Server image&lt;/h1&gt;
&lt;p&gt;Just like 'boot2docker' we want our Nano Server to be ready to host Windows Containers as soon as it's booted and without further manual configuration. To this end, I needed to find a way to install Docker as part of the deployment process. Fortunately the 'New-NanoServer' command provides a &lt;code&gt;-SetupCompleteCommand&lt;/code&gt; argument which allows you to 'run custom commands as part of setupcomplete.cmd' (i.e. on first boot). Great, so now to prepare a script to deploy Docker which we can call via the &lt;code&gt;-SetupCompleteCommand&lt;/code&gt; argument.&lt;/p&gt;
&lt;p&gt;Conveniently, Docker's &lt;a href="https://docs.docker.com/docker-ee-for-windows/install/#using-a-script-to-install-docker-ee"&gt;documentation for installing Docker EE&lt;/a&gt; (the version supported by Windows) provides exactly the script required, copied with some additional configuration copied from the 'Prepare Container Host' section of &lt;a href=""&gt;'Deploy Containers on Nano'&lt;/a&gt; article:&lt;/p&gt;
&lt;pre class="prettyprint"&gt;&lt;code&gt;# On an online machine, download the zip file
Invoke-Webrequest -UseBasicparsing -Outfile docker.zip https://download.docker.com/components/engine/windows-server/17.03/docker-17.03.0-ee.zip

# Extract the archive.
Expand-Archive docker.zip -DestinationPath $Env:ProgramFiles

# Clean up the zip file.
Remove-Item -Force docker.zip

# Install Docker. This will require rebooting.
# This is not required as we have already prepared out image with container support
# $null = Install-WindowsFeature containers

# Add Docker to the path for the current session.
$env:path += ";$env:ProgramFiles\docker"

# Modify PATH to persist across sessions.
# Note: Nano Server's SetEnvironmentVariable method does not take a scope parameter 
[Environment]::SetEnvironmentVariable("PATH", $env:path)

# Open an inbound port for the docker daemon  
netsh advfirewall firewall add rule name="Docker daemon " dir=in action=allow protocol=TCP localport=2375

# Create and populate docker daemon's configuration file
New-Item -Type File 'C:\ProgramData\docker\config\daemon.json' -Force
Add-Content 'C:\programdata\docker\config\daemon.json' '{ "hosts": ["tcp://0.0.0.0:2375", "npipe://"] }'

# Register the Docker daemon as a service.
dockerd --register-service

# Start the daemon.
Start-Service docker
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now this script requires that the Nano Server be online when the script is run so that it can download the Docker binaries. Unfortunately, given that this script will run as part of the deployment process, this is unlikely to be the case. Instead, we'll need to lean on another feature of the &lt;code&gt;New-NanoServerImage&lt;/code&gt;, &lt;code&gt;-CopyPath&lt;/code&gt;. This argument allows you to specify one or more files to copy to the Nano Server image as part of it's creation and we'll use it to copy a pre-downloaded copy of the docker binaries (along with a copy of this script) to the root of the images C:\ drive, as shown here:&lt;/p&gt;
&lt;pre class="prettyprint"&gt;&lt;code&gt;New-NanoServerImage -Edition Standard -DeploymentType Guest -MediaPath &amp;lt;path to root of media&amp;gt; -BasePath &amp;lt;path in which to build the image&amp;gt; -TargetPath &amp;lt;destination path&amp;gt;\NanoServer.vhdx -ComputerName &amp;lt;computer name&amp;gt; -Containers -EnableRemoteManagementPort -CopyPath @('&amp;lt;path to deployment script&amp;gt;\DeployDocker.ps1', '&amp;lt;path in which docker is downloaded&amp;gt;\docker.zip') -SetupCompleteCommand @('Powershell.exe -Command .\DeployDocker.ps1') 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Great, now we can comment out the first line of the script above and it'll run fine, right? Unfortunately not. It seems that, at the stage of the boot process at which this script, powershell isn't quite ready to execute scripts. Fortunately, others have encountered this issue before and Sergey Babkin provides &lt;a href="https://blogs.msdn.microsoft.com/sergey_babkins_blog/2017/01/05/how-to-run-powershell-from-setupcomplete-cmd/"&gt;this solution&lt;/a&gt;, copied below with customisation for our requirements:&lt;/p&gt;
&lt;pre class="prettyprint"&gt;&lt;code&gt;set LOCALAPPDATA=%USERPROFILE%\AppData\Local
set PSExecutionPolicyPreference=Unrestricted
Powershell -Command C:\DeployDocker.ps1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Ok, so we'll add and deploy this batch file as 'DeployDocker.bat' and then execute this instead of the powershell script as the &lt;code&gt;-SetupCompleteCommand&lt;/code&gt;, as shown here:&lt;/p&gt;
&lt;pre class="prettyprint"&gt;&lt;code&gt;New-NanoServerImage -Edition Standard -DeploymentType Guest -MediaPath &amp;lt;path to root of media&amp;gt; -BasePath &amp;lt;path in which to build the image&amp;gt; -TargetPath &amp;lt;destination path&amp;gt;\NanoServer.vhdx -ComputerName &amp;lt;computer name&amp;gt; -Containers -EnableRemoteManagementPort -CopyPath @('&amp;lt;path to deployment batch file'\DeployDocker.bat', '&amp;lt;path to deployment script&amp;gt;\DeployDocker.ps1', '&amp;lt;path in which docker is downloaded&amp;gt;\docker.zip') -SetupCompleteCommand 'C:\DeployDocker.bat' 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you now create a virtual machine with the 'NanoServer.vhdx' image as it's boot drive, you should find that, once it's booted you're able to communicate with the Docker daemon on the VM. For example:&lt;/p&gt;
&lt;pre class="prettyprint"&gt;&lt;code&gt;docker -H &amp;lt;IP Address of VM&amp;gt; ps
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Should return a (empty) list of containers present on the Nano Server host.&lt;/p&gt;
&lt;h1&gt;Scripting the creation of a VM&lt;/h1&gt;
&lt;h1&gt;Updating Nano Server&lt;/h1&gt;
&lt;p&gt;Referring back to the &lt;a href="https://docs.microsoft.com/en-us/virtualization/windowscontainers/manage-containers/swarm-mode"&gt;'Getting Started with Swarm Mode'&lt;/a&gt; article, it became apparent that a &lt;a href="https://support.microsoft.com/en-us/help/4015217/windows-10-update-kb4015217"&gt;relatively recent update&lt;/a&gt; is required to run Docker Swarms on Windows Server 2016 based OS's.&lt;/p&gt;
&lt;h1&gt;Scripting the deployment of a Docker Swarm&lt;/h1&gt;



                                </content:encoded>
		</item>
		<item>
			<title>Breaking News - Forgot to commit</title>
			<link>http://ian.bebbs.co.uk/posts/EndOfStreak</link>
			<description>&lt;p&gt;After 41 days of continuous commits to Github, yesterday I forgot. What's really annoying is that I actually spent a long time working yesterday but found myself battling a stupid limitation of VisualStates in UWP apps (blog post incoming) and didn't have anything finished.&lt;/p&gt;</description>
			<guid>http://ian.bebbs.co.uk/posts/EndOfStreak</guid>
			<pubDate>Mon, 07 Dec 2015 00:00:00 GMT</pubDate>
			<content:encoded>
                                        


&lt;p&gt;After 41 days of continuous commits to Github, yesterday I forgot. What's really annoying is that I actually spent a long time working yesterday but found myself battling a stupid limitation of VisualStates in UWP apps (blog post incoming) and didn't have anything finished.&lt;/p&gt;
&lt;p&gt;Ho-hum. 41 days was a pretty good streak and it was inevitably going to end over the Xmas period. New Year's resolution: Beat it!&lt;/p&gt;



                                </content:encoded>
		</item>
		<item>
			<title>Home Network Monitoring - Part I</title>
			<link>http://ian.bebbs.co.uk/posts/HomeNetworkMonitoring-PartI</link>
			<description>&lt;p&gt;Home networks are becoming increasingly complex. It is no longer just geeks and techies who have pervasive WiFi through-out their home to which a myriad of devices connect and communicate. When things go wrong or, worse still, the network is compromised by rouge hardware or software it's extremely difficult to work out what has happened and where to start troubleshooting the issue.&lt;/p&gt;</description>
			<guid>http://ian.bebbs.co.uk/posts/HomeNetworkMonitoring-PartI</guid>
			<pubDate>Fri, 08 Apr 2016 00:00:00 GMT</pubDate>
			<content:encoded>
                                        


&lt;p&gt;Home networks are becoming increasingly complex. It is no longer just geeks and techies who have pervasive WiFi through-out their home to which a myriad of devices connect and communicate. When things go wrong or, worse still, the network is compromised by rouge hardware or software it's extremely difficult to work out what has happened and where to start troubleshooting the issue.&lt;/p&gt;
&lt;p&gt;In the next few posts, I'm going to be showing how I used free and open-source software to build a home network monitoring solution that allows me to:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Monitor the devices on my networks&lt;/li&gt;
&lt;li&gt;Monitor my network connectivity and utilisation&lt;/li&gt;
&lt;li&gt;Monitor which devices are connecting to which remote sites.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These posts will use the following components:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;An internet router capable of sending Syslog messages&lt;/li&gt;
&lt;li&gt;A mid-spec (i.e. dual-core 2Ghz) server with plenty of storage&lt;/li&gt;
&lt;li&gt;Java Runime Environment installed on the server&lt;/li&gt;
&lt;li&gt;The ElasticSearch stack consisting of:
&lt;ul&gt;
&lt;li&gt;Logstash (I will be using v2.4.1)&lt;/li&gt;
&lt;li&gt;ElasticSearch (I will be using v2.4.1)&lt;/li&gt;
&lt;li&gt;Kibana (I will be using v4.5.0)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h1&gt;the router&lt;/h1&gt;
&lt;p&gt;At the core of my solution is a router that supports writing &lt;a href="https://en.wikipedia.org/wiki/Syslog"&gt;Syslog&lt;/a&gt; messages to a Syslog server. While once a rarity, this functionality is becoming increasingly prevalent in home / SOHO routers. Personally I use a &lt;a href="http://www.draytek.co.uk/products/business/vigor-2830"&gt;DrayTek Vigor 2830&lt;/a&gt;, a versatile and - most importantly - extremely reliable router that can be purchased for just over £100 in the UK.&lt;/p&gt;
&lt;p&gt;From the router administration web interface, you can set the router to write a variety of Syslog messages to a Syslog server by specifying the servers IP address. This can be seen below:&lt;/p&gt;
&lt;img src="/Content/HomeNetworkMonitoring/DrayTek-Syslog.png" class="img-responsive" style="margin: auto; width:600px; margin-top: 6px; margin-bottom: 6px;" alt="DrayTek Vigor Syslog Settings"&gt;
&lt;p&gt;I simply tick all the boxes and add the IP address of the PC hosting the Syslog server.&lt;/p&gt;
&lt;h1&gt;the syslog server&lt;/h1&gt;
&lt;p&gt;Now we have a router that is sending connectivity information via Syslog, we need a service running on a server that is capable of receiving these messages. For this I am using Elastic's &lt;a href="https://www.elastic.co/products/logstash"&gt;Logstash&lt;/a&gt; to receive the syslog messages and enrich them before forwarding them to an indexed store.&lt;/p&gt;
&lt;p&gt;Logstash is an extremely versatile tool capable of consuming data from a variety of sources. A configuration file is used to set up a pipeline of inputs, operations (known as filters) and outputs which can do some truly fantastic things. If you're new to Logstash it might be worthwhile giving the ("Getting Started")[https://www.elastic.co/guide/en/logstash/current/getting-started-with-logstash.html] guide a quick read.&lt;/p&gt;
&lt;p&gt;To start with, we'll get Logstash to simply accept Syslog input from a given port and write it to the console. To do this, simply &lt;a href="https://www.elastic.co/downloads/logstash"&gt;download Logstash&lt;/a&gt; and extract it to a directory on the PC. Next, open notepad and copy paste the following:&lt;/p&gt;
&lt;pre class="prettyprint"&gt;&lt;code&gt;input {
  tcp {
    port =&amp;gt; 5000
    type =&amp;gt; syslog
  }
  udp {
    port =&amp;gt; 5000
    type =&amp;gt; syslog
  }
}

filter {
}

output {
  stdout {
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Save this file named 'Syslog.config' and, from a command prompt, start Logstash with the following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;[LogStashDirectory]\bin\logstash.bat agent -f [ConfigFilePath]\Syslog.config&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;If all goes well, you should see Logstash start receiving Syslog messages from the router which should appear something like the following:&lt;/p&gt;
&lt;pre class="prettyprint"&gt;&lt;code&gt;io/console not supported; tty will not be manipulated
Settings: Default pipeline workers: 2
Pipeline main started
2016-04-08T13:52:21.903Z 192.168.1.1 &amp;lt;150&amp;gt;Apr  8 14:51:59 Vigor: Local User (MAC=ZZ-ZZ-ZZ-ZZ-ZZ-ZZ): 192.168.1.62 DNS -&amp;gt; 192.168.1.1 inquire a.root-servers.net
2016-04-08T13:52:22.528Z 192.168.1.1 &amp;lt;150&amp;gt;Apr  8 14:51:59 Vigor: Local User (MAC=ZZ-ZZ-ZZ-ZZ-ZZ-ZZ): 192.168.1.51 DNS -&amp;gt; 192.168.1.1 inquire api-global.netflix.com
2016-04-08T13:52:22.528Z 192.168.1.1 &amp;lt;150&amp;gt;Apr  8 14:51:59 Vigor: Local User (MAC=ZZ-ZZ-ZZ-ZZ-ZZ-ZZ): 192.168.1.51 DNS -&amp;gt; 213.120.234.54 inquire api-global.netflix.com
2016-04-08T13:52:22.903Z 192.168.1.1 &amp;lt;150&amp;gt;Apr  8 14:52:00 Vigor: Local User (MAC=ZZ-ZZ-ZZ-ZZ-ZZ-ZZ): 192.168.1.62 DNS -&amp;gt; 192.168.1.1 inquire a.root-servers.net
2016-04-08T13:52:24.356Z 192.168.1.1 &amp;lt;150&amp;gt;Apr  8 14:52:01 Vigor: Local User (MAC=ZZ-ZZ-ZZ-ZZ-ZZ-ZZ): 192.168.1.100 DNS -&amp;gt; 192.168.1.1 inquire sls.update.microsoft.com
2016-04-08T13:52:24.356Z 192.168.1.1 &amp;lt;150&amp;gt;Apr  8 14:52:01 Vigor: Local User (MAC=ZZ-ZZ-ZZ-ZZ-ZZ-ZZ): 192.168.1.100 DNS -&amp;gt; 213.120.234.54 inquire sls.update.microsoft.com
2016-04-08T13:52:24.356Z 192.168.1.1 &amp;lt;158&amp;gt;Apr  8 14:52:01 Vigor: Load_balance 192.168.1.100 --(DNS)--&amp;gt; 213.120.234.54 go WAN1
2016-04-08T13:52:24.731Z 192.168.1.1 &amp;lt;150&amp;gt;Apr  8 14:52:01 Vigor: Local User (MAC=ZZ-ZZ-ZZ-ZZ-ZZ-ZZ): 192.168.1.100:53432 -&amp;gt; 157.56.77.138:443 (TCP)
2016-04-08T13:52:24.731Z 192.168.1.1 &amp;lt;158&amp;gt;Apr  8 14:52:01 Vigor: Load_balance 192.168.1.100 --(BAL)--&amp;gt; 157.56.77.138 go WAN1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you're not receiving Syslog messages try the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Ensure to check the firewall settings on the server you're using. You need to allow incomming TCP and UDP connections on port 5000.&lt;/li&gt;
&lt;li&gt;Attempt to telnet to port 5000 on the Syslog server. If Logstash is running correctly, you should be able to connect and see anything you sent from Telnet mirrored in the Logstash console window.&lt;/li&gt;
&lt;li&gt;If you're still unable to see any output, try using a network analysis too like &lt;a href="https://www.wireshark.org/"&gt;Wireshark&lt;/a&gt; to see if your router is actually sending any messages.&lt;/li&gt;
&lt;/ol&gt;
&lt;h1&gt;the syslog store&lt;/h1&gt;
&lt;p&gt;Now we're able to receive Syslog messages, we need to store and index them. For this, we will use ElasticSearch. Simply &lt;a href="https://www.elastic.co/downloads/elasticsearch"&gt;download ElasticSearch&lt;/a&gt;, extract it to a directory and start it. If necessary you can change the directory used to store  ElasticSearch data or the interface/port on which ElasticSearch listens for incomming connections by modifying the &lt;code&gt;config\ElasticSearch.yml&lt;/code&gt; file. Modifying this file is pretty straight forward but for help the &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html"&gt;ElasticSearch documentation&lt;/a&gt; is available online and very thorough.&lt;/p&gt;
&lt;p&gt;Additionally, rather than having to manually start ElasticSearch everytime you want to use it, you can easily install it as a Windows service simply - as I have done - by issuing the following commands from a command prompt:&lt;/p&gt;
&lt;pre class="prettyprint"&gt;&lt;code&gt;[PathToElasticSearch]\bin\service.bat install
[PathToElasticSearch]\bin\service.bat start
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ElasticSearch should no be running. You can check this calling REST methods on ElasticSearch's web interface; by default on port 9200. In a browser, simple enter &lt;code&gt;http://[server-ip]:9200&lt;/code&gt; and you should see something like the following:&lt;/p&gt;
&lt;pre class="prettyprint"&gt;&lt;code&gt;"name" : "Shellshock",
"cluster_name" : "cluster",
"version" : {
  "number" : "2.3.1",
  "build_hash" : "bd980929010aef404e7cb0843e61d0665269fc39",
  "build_timestamp" : "2016-04-04T12:25:05Z",
  "build_snapshot" : false,
  "lucene_version" : "5.5.0"
},
"tagline" : "You Know, for Search"
}
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;NOTE: You will be using the REST API extensively in future steps so I suggest finding a toolset that make querying and posting to REST endpoints easier. I use the excellent &lt;a href="https://chrome.google.com/webstore/detail/postman/fhbjgbiflinjbdggehcddcbncdddomop?hl=en"&gt;&lt;code&gt;Postman&lt;/code&gt;&lt;/a&gt; Chrome application.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;With ElasticSearch running, we now need to modify Logstash to forward Syslog messages to ElasticSearch for indexing. As both tools are part of the Elastic Stack, this is every bit as easy as you might expect it to be. Simply open the &lt;code&gt;syslog.config&lt;/code&gt; file we created earler and change it to the following:&lt;/p&gt;
&lt;pre class="prettyprint"&gt;&lt;code&gt;input {
  tcp {
    port =&amp;gt; 5000
    type =&amp;gt; syslog
  }
  udp {
    port =&amp;gt; 5000
    type =&amp;gt; syslog
  }
}

filter {
}

output {
  elasticsearch {
    hosts =&amp;gt; ["[ElasticSearchServer-NameOrIPAddress]:9200"]
    index =&amp;gt; "syslog-%{+YYYY.MM.dd}"
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With this in place, restart the Logstart service and, instead of writing received messages to the console, they will be stored in ElasticSearch.&lt;/p&gt;
&lt;h1&gt;the dashboard&lt;/h1&gt;
&lt;p&gt;Now we have syslog messages in a central store, we will look how to set up a simple (for now) dashboard that lets us see a minimally useful feature: the number of Syslog messages being received over time.&lt;/p&gt;
&lt;p&gt;In order to do this, we will be using ElasticSearch's Kibana tool to query messages from ElasticSearch and display a histogram of messages on a dashboard. To get started, simply download and extract Kibana to a directory on the server. Kibana comes with a default configuration that allows it to run correctly when co-located on the same server as ElasticSearch. If you are not running Kibana on the same server as ElasticSearch, you will need to modify the Kibana configuration file as described in the &lt;a href="https://www.elastic.co/guide/en/kibana/current/index.html"&gt;documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To start Kibana, simply open a command prompt and execute the following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;[PathToKibana]\bin\Kibana.bat&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;You should see something like the following:&lt;/p&gt;
&lt;pre class="prettyprint"&gt;&lt;code&gt;log   [15:51:32.912] [info][status][plugin:kibana] Status changed from uniniti
alized to green - Ready
log   [15:51:32.975] [info][status][plugin:elasticsearch] Status changed from
uninitialized to yellow - Waiting for Elasticsearch
log   [15:51:33.006] [info][status][plugin:kbn_vislib_vis_types] Status change
d from uninitialized to green - Ready
log   [15:51:33.022] [info][status][plugin:markdown_vis] Status changed from u
ninitialized to green - Ready
log   [15:51:33.037] [info][status][plugin:metric_vis] Status changed from uni
nitialized to green - Ready
log   [15:51:33.053] [info][status][plugin:spyModes] Status changed from unini
tialized to green - Ready
log   [15:51:33.068] [info][status][plugin:statusPage] Status changed from uni
nitialized to green - Ready
log   [15:51:33.068] [info][status][plugin:table_vis] Status changed from unin
itialized to green - Ready
log   [15:51:33.100] [info][listening] Server running at http://0.0.0.0:5601
log   [15:51:38.131] [info][status][plugin:elasticsearch] Status changed from
yellow to yellow - No existing Kibana index found
log   [15:51:41.053] [info][status][plugin:elasticsearch] Status changed from
yellow to green - Kibana index ready
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With Kibana started, use a browser to navigate to the Kibana web interface, typically on port 5601.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;NOTE: If you are connected to Kibana from another PC, you will need to open firewall port 5602 on the server to allow connections to Kibana.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;After a short pause while Kibana is initialised, you should see the following screen:&lt;/p&gt;
&lt;img src="/Content/HomeNetworkMonitoring/Kibana-InitialIndex.png" class="img-responsive" style="margin: auto; width:600px; margin-top: 6px; margin-bottom: 6px;" alt="Kibana Initial Index Settings"&gt;
&lt;p&gt;This screen allows you to add an index to Kibana that it can query messages in order to discover, visualise and ultimately produce a dashboard from information stored in the index. It is currently set to examine an index called &lt;code&gt;logstash-*&lt;/code&gt; and shows a disable button at the bottom of the screen containing the text 'Unable to fetch mapping. Do you have indices matching the pattern' as we do not have a logstash index stored in ElasticSearch. We want Kibana to query our Syslog index so we change the 'Index name or pattern' to &lt;code&gt;Syslog-*&lt;/code&gt; and, shortly after changing this value, we should see the button at the bottom change to 'Create' as shown here:&lt;/p&gt;
&lt;img src="/Content/HomeNetworkMonitoring/Kibana-SyslogIndex.png" class="img-responsive" style="margin: auto; width:600px; margin-top: 6px; margin-bottom: 6px;" alt="Kibana Syslog Index Settings"&gt;
&lt;p&gt;As Kibana has defaulted to the correct timestamp field (more on this later) you can simply click the 'Create' button to add the index to Kibana.&lt;/p&gt;
&lt;p&gt;After creating the index, you are taken to a screen that allows you modify how Kibana displays the fields within the index as shown below:&lt;/p&gt;
 &lt;img src="/Content/HomeNetworkMonitoring/Kibana-SyslogIndexMapping.png" class="img-responsive" style="margin: auto; width:600px; margin-top: 6px; margin-bottom: 6px;" alt="Kibana Syslog Index Mapping"&gt;
&lt;p&gt;We do not need to change anything here so can immediately start 'discovering' information in our Syslog index by clicking the 'Discover' tab as shown below:&lt;/p&gt;
 &lt;img src="/Content/HomeNetworkMonitoring/Kibana-DiscoverSyslog.png" class="img-responsive" style="margin: auto; width:600px; margin-top: 6px; margin-bottom: 6px;" alt="Kibana Discover Syslog"&gt;
&lt;p&gt;Here we're able to see the detail of the Syslog messages stored within the Syslog index on ElasticSearch. However, for now the messages are just strings so there's not a great deal we can do with them other than count them to produce a histogram of messages over time. To do this, we
first want to add the fields of the index we're interested in (just timestamp, host, message and type for now) to the selected fields area as shown below:&lt;/p&gt;
&lt;img src="/Content/HomeNetworkMonitoring/Kibana-SelectSyslogFields.png" class="img-responsive" style="margin: auto; width:600px; margin-top: 6px; margin-bottom: 6px;" alt="Kibana Select Syslog Fields"&gt;
&lt;p&gt;Once we have the fields we're interested in selected, we save them as a new search by clicking the 'Save Search' button in the top right of the window as shown below:&lt;/p&gt;
&lt;img src="/Content/HomeNetworkMonitoring/Kibana-SaveSyslogSearch.png" class="img-responsive" style="margin: auto; width:600px; margin-top: 6px; margin-bottom: 6px;" alt="Kibana Save Syslog Search"&gt;
&lt;p&gt;Once the search is saved, we can proceed straight to the 'Visualise' tab to create the histogram as shown below:&lt;/p&gt;
&lt;img src="/Content/HomeNetworkMonitoring/Kibana-VisualiseSyslogAsVerticalBarChart.png" class="img-responsive" style="margin: auto; width:600px; margin-top: 6px; margin-bottom: 6px;" alt="Kibana Visualise Syslog As Vertical Bar Chart"&gt;
&lt;p&gt;To create a histogram, we use a "Vertical bar chart". Note it's description:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The goto chart for oh-so-many needs. Great for time and non-time data. Stacked or grouped, exact numbers or percentages. If you are not sure which chart you need, you could do worse than to start here.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So lets start here by clicking this option and selecting 'From a saved search' as shown below:&lt;/p&gt;
&lt;img src="/Content/HomeNetworkMonitoring/Kibana-VisualiseFromSavedSearch.png" class="img-responsive" style="margin: auto; width:600px; margin-top: 6px; margin-bottom: 6px;" alt="Kibana Visualise From Saved Search"&gt;
&lt;p&gt;Once you select the 'Syslog Messages' save search you will immediately be taken to the Visualization screen with a vertical bar chart showing a single bar with the total count of all messages. Here we want to customise the visualisation to display the count of messages over time so we first need to define the X-axis as a 'Date Histogram' as shown below.&lt;/p&gt;
&lt;img src="/Content/HomeNetworkMonitoring/Kibana-VisualiseDateHistogram.png" class="img-responsive" style="margin: auto; width:600px; margin-top: 6px; margin-bottom: 6px;" alt="Kibana Visualise Date Histogram"&gt;
&lt;p&gt;With these settings in place, click the 'Apply Changes' button and you should immediately see a chart of the count of syslog messages over time. Note that Kibana has automatically selected an appropriate resolution of column grouping (messages 'per 30 seconds' in the example above) but that this can be changed later if required.&lt;/p&gt;
&lt;p&gt;For now, we want to add this chart to a new dashboard so we can have it available to us at a moments notice. To do this, we first save our visualization by clicking the 'Save Visuaization' button as shown below:&lt;/p&gt;
&lt;img src="/Content/HomeNetworkMonitoring/Kibana-VisualiseSaveSyslogMessagesOverTime.png" class="img-responsive" style="margin: auto; width:600px; margin-top: 6px; margin-bottom: 6px;" alt="Kibana Visualise Save Syslog Messages Over Time"&gt;
&lt;p&gt;Next we navigate to the 'Dashboard' tab and click the '+' button as prompted. Again, we selected the visualization we just saved as shown below:&lt;/p&gt;
&lt;img src="/Content/HomeNetworkMonitoring/Kibana-DashboardAddVisualization.png" class="img-responsive" style="margin: auto; width:600px; margin-top: 6px; margin-bottom: 6px;" alt="Kibana Dashboard Add Visualization"&gt;
&lt;p&gt;With the "Syslog Messages Over Time" visualization added, we make it a usable size by dragging the resize control (button right corner of the visualization) to extend the visualization across the width of the window as shown below:&lt;/p&gt;
&lt;img src="/Content/HomeNetworkMonitoring/Kibana-DashboardSizeVisualization.png" class="img-responsive" style="margin: auto; width:600px; margin-top: 6px; margin-bottom: 6px;" alt="Kibana Dashboard Size Visualization"&gt;
&lt;p&gt;Finally, to make it easier to see the messages the histogram refers to, we'll add a table of related Syslog messages onto our dashboard below the histogram. To do this, click the '+' button, selected 'Searches' and then our 'Syslog Messages' search. When the table is added to the dashboard, make it a similar size as the histogram as shown below:&lt;/p&gt;
&lt;img src="/Content/HomeNetworkMonitoring/Kibana-DashboardWithSearch.png" class="img-responsive" style="margin: auto; width:600px; margin-top: 6px; margin-bottom: 6px;" alt="Kibana Dashboard With Search"&gt;
&lt;p&gt;Once done, we need to save the dashboard so we can reload it any time we need it. Simply click the 'Save Dashboard' button as shown below:&lt;/p&gt;
&lt;img src="/Content/HomeNetworkMonitoring/Kibana-DashboardSaveSyslogMessages.png" class="img-responsive" style="margin: auto; width:600px; margin-top: 6px; margin-bottom: 6px;" alt="Kibana Dashboard Save Syslog Messages"&gt;
&lt;p&gt;Once saved you can bookmark the page and get back to your dashboard any time you like. Furthermore, once saved, you can monkey with the dashboard (explore the data by highlighting various areas of the chart, change time frames, set autorefresh etc) as much as you like knowing you can return to saved version any time.&lt;/p&gt;
&lt;h1&gt;summary&lt;/h1&gt;
&lt;p&gt;In this post, I have outlined how to use a Syslog capable router to send Syslog messages to Logstash and have Logstash store these messages in ElasticSearch for querying. Furthermore we then created a dashboard in which we can explore the number of Syslog messages we received over various timeframes.&lt;/p&gt;
&lt;p&gt;In the next post we'll increase the granularity of the messages we store so that we can start creating more interesting dashboard.&lt;/p&gt;



                                </content:encoded>
		</item>
		<item>
			<title>Wot No Blog Posts?</title>
			<link>http://ian.bebbs.co.uk/posts/WotNoBlogPosts</link>
			<description>&lt;p&gt;Yes, it's been over three months since my last blog post. "What have you been doing?" I hear you (the hypothetical reader) ask. Well... well I'll tell you.&lt;/p&gt;</description>
			<enclosure url="http://ian.bebbs.co.uk/y3mKlqOBjK9u-QOTjr8e6t2pOev7BN2vWZHyEDf2l27_HhaNgR_aVCXi2-GJh_KZMQV-naegShjBydS0blOk2kSndI2eTXnRhuqA5Ry0VYn8a0HdOEj_RvSUJ8uVdzsiDmcn4XRkyaAn7kScarmtvlf5nua4L9lkP_bWrKG5Ai7JdQ%253Fwidth=660&amp;height=371&amp;cropmode=none" length="0" type="image" />
			<guid>http://ian.bebbs.co.uk/posts/WotNoBlogPosts</guid>
			<pubDate>Tue, 26 Jul 2016 00:00:00 GMT</pubDate>
			<content:encoded>
                                        


&lt;p&gt;Yes, it's been over three months since my last blog post. "What have you been doing?" I hear you (the hypothetical reader) ask. Well... well I'll tell you.&lt;/p&gt;
&lt;p&gt;Having left my previous permanent position in January, I decided the time was right for a &lt;strong&gt;career break&lt;/strong&gt;. Having been &lt;a href="https://www.ted.com/talks/stefan_sagmeister_the_power_of_time_off?language=en"&gt;inspired to take a career break&lt;/a&gt; some six years ago, I knew how rejuvenating time off work can be and how reinvigorating it is when you finally have some time to pick and choose what it is you want to do and, more importantly, how. Indeed, last time I took a career break I ended up founding a company which - while not successful enough to prevent me having to return to employment - was massively educational, incredibly liberating and a whole lot of fun.&lt;/p&gt;
&lt;p&gt;However, my motivations for this career break are vastly different to those of six years ago. At that time I had just emerged from a somewhat turbulent relationship and was looking to - &lt;em&gt;sorry for the cliché&lt;/em&gt; - "re-find" myself; this time I am in a stable and happy relationship with my partner who last year gave birth to our little baby girl. And while, six years ago, I could - and was happy to - survive on very meagre savings, I am now lucky enough to be in a position where financial pressures are not an issue, at least in the short-term. Finally, while I had very little idea of what I was going to do during my last career break (which itself was extremely liberating in that you are suddenly open to being able to say "yes" to pretty much anything) this time I have a much more developed concept of how I want to use this opportunity.&lt;/p&gt;
&lt;p&gt;While my motivations and desires for the coming months will be the subject of future blog posts, I have started putting together a &lt;a href="https://trello.com/b/KoTWuFUi/public-board"&gt;public Trello board&lt;/a&gt; of ideas/projects I want to undertake. It's just a start and by no means comprehensive but I thought some transparency around my ideas and goals (not to mention progress!) would be good for me.&lt;/p&gt;
&lt;p&gt;For now though I want to share what I've been doing - and why there have been no blog posts - for the last three months. You see, a primary motivation for taking time out of my career now was to spend time with my young family and maximise the use of my partner's remaining maternity leave. To this end, shortly after leaving my previous job, my partner and I decided to &lt;strong&gt;take our baby on a road trip around Europe&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The timing really couldn't have been better; my partner still had a few months of maternity leave remaining, the baby was still happy to have long naps in the back of the car whenever we drove anywhere and I was still keen on buying a day-van (it's been a &lt;em&gt;thing&lt;/em&gt; with me for a while now). So in February we started planning and looking for a motor that could comfortably cart the three of us around Europe and by mid March we'd bought an imported Nissan Elgrand E51 Ryder Autech.&lt;/p&gt;
&lt;img src="https://zdfcta.dm2302.livefilestore.com/y3mRDVqstf6XsjN73biGoYnN5s-SzVcHx9pEdYl2S_wM803HpiosWK5Skxm-kskeHTMkahk_PIdejAp70x14KCaOwmZb6OvAvAig8uqI4bdBs2Pb_lAUsm-O_2lO8krW_18ReDVMLXqVkJWUGUSJ-d80bOIi8ciWIgyxDzL5fw4qF4?width=1796&amp;amp;height=1347&amp;amp;cropmode=none" class="img-responsive" style="margin: auto; width:600px; margin-top: 6px; margin-bottom: 6px;" alt="My other baby"&gt;
&lt;p&gt;Now the Elgrand is an MPV and not a campervan (a mostly pragmatic decision based on the fact we could use it on a daily basis after the trip and only a little bit due to "love at first drive") so there was a lot of work to do in order to prepare it for such a long journey. Therefore, between March and May, I researched, built and installed a bunch of modifications for the Elgrand to support us on our adventure. This included a custom built leisure battery / charging system enclosure, custom built frame to mount the electrical systems and luggage, routing of cabling around the Elgrand and several internal modifications for enhancing the navigation / ICE systems.&lt;/p&gt;
&lt;p&gt;I documented these modifications on the &lt;a href="http://elgrandoc.uk/"&gt;Elgrand Owners Club website&lt;/a&gt;. This site proved invaluable in terms of information about working on the Elgrand and I wanted to give something back to the active and very helpful community there. Due to limitations in the website's forum, the details had to be split across five posts which - if you're interested - can be found here: &lt;a href="http://elgrandoc.uk/forum/index.php?threads/touring-europe-in-an-elgrand-part-i.3311/"&gt;Part I&lt;/a&gt;, &lt;a href="http://elgrandoc.uk/forum/index.php?threads/touring-europe-in-an-elgrand-part-ii.3312/"&gt;Part II&lt;/a&gt;, &lt;a href="http://elgrandoc.uk/forum/index.php?threads/touring-europe-in-an-elgrand-part-iii.3313/"&gt;Part III&lt;/a&gt;, &lt;a href="http://elgrandoc.uk/forum/index.php?threads/touring-europe-in-an-elgrand-part-iv.3314/"&gt;Part IV&lt;/a&gt;, &lt;a href="http://elgrandoc.uk/forum/index.php?threads/touring-europe-in-an-elgrand-part-v.3315/"&gt;Part V&lt;/a&gt;. After just a couple of days these posts have already garnered significant praise and numerous follow on questions so I feel pretty good about having spent the time writing these posts.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;As an aside, it occurred to me that this work could almost have been a case-study in Agile development. Not being a carpenter or mechanic and my work on cars to date having been pretty much limited to topping up the oil, I started this project from an absolute level of maximum ignorance. Fortunately as both a stake holder and engineer, it was easy to outline use-cases for the work which, while I didn't actually document them at the time, would have included some choice ones such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;"As a driver I need a non-Japanese navigation system so I know where the smeg I'm supposed to be going!",&lt;/li&gt;
&lt;li&gt;"As a parent I need a fridge in the van so that we can keep food for the baby food fresh and - hopefully - the baby healthy" and,&lt;/li&gt;
&lt;li&gt;"As a caffeine junkie I need a means of making coffee in the van so that I don't murder everyone the morning after we spend a night in it!"&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These could then be naturally prioritized and, where possible, undertaken concurrently by my partner and I. If something went well, we could close it off and move onto the next thing; if it didn't we could reevaluate it's value / priority in light of what we had. This way we managed to hit our deadline (the ferry departure - which we put off "committing" to until as late as possible) with as much high value work done as possible. More importantly, at departure (aka deployment) time, we had a significantly lower level of ignorance such that, should anything go wrong while we were "live", we'd actually stand a chance of fixing it without having to - possibly literally - "roll back" to England.&lt;/p&gt;
&lt;p&gt;Yes, yes, some of this terminology is undoubtedly tenuous but, having had hours and hours to reflect on this work while driving around Europe, it was rewarding to see how much more effective an iterative approach to problem solving can be. Who knows if we'd ever have got away if we'd tried to plan &lt;em&gt;everything&lt;/em&gt; up front.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Anyway, we departed England in mid-May and returned mid-July and had a truly fantastic adventure. In &lt;strong&gt;58 days&lt;/strong&gt; we travelled &lt;strong&gt;7912 miles&lt;/strong&gt; and visited &lt;strong&gt;20 countries&lt;/strong&gt;. Along the way we met a number of old friends, made a lot of new friends and saw some amazing sites and scenery. We learned a lot, not only about the various cultures / histories around Europe but also about ourselves and our daughter. And we laughed a lot, mostly about the quirks of the various cultures we were experiencing but also - in retrospect at least - about some of the challenges we faced along the way; like having to park up in a field at a border crossing in order to buy the vignette that would allow us to drive the car in the country we were about to cross in to, and getting visited by both a swarm of monstrously huge locust type insects and a heavily armed and somewhat suspicious (the Elgrand has very dark privacy glass) border guard.&lt;/p&gt;
&lt;p&gt;We really did experience so much that I frequently find myself remembering something from the trip that a) I had somehow already forgotten and b) seems like both a lifetime &lt;em&gt;and&lt;/em&gt; just a few days ago. Fortunately my partner kept a very detailed (and anonymised) blog of our journey in which she does an terrific job of capturing the fun and freedom we enjoyed while away. Should you be interested, you can read it &lt;a href="https://bigspune.wordpress.com/"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;And now we're home. The baby has started nursery and my partner is returning to work. While I'm still having to consciously work out where I am when I wake up in the night (seriously, after two months of staying somewhere different every night, this seems to be a thing at the moment) and we're still trying to get the house back into some semblance of order, life is returning to our own special approximation of normality.&lt;/p&gt;
&lt;p&gt;For four days a week and for the foreseeable future, I will have pretty much all day to do... well, pretty much anything I want. It's an exciting time in the world of .NET given the recent release of .NET Core and Microsoft's assimilation of Xamarin. Graph and document data stores (aka NoSQL) continue to make inroads on the traditional strong holds of legacy relational databases. And there are some profound changes in the broader world of software development such as the continuing move towards containerized deployment and cloud infrastructures. I'm very much looking forward to investigating / leveraging all these in the months to come.&lt;/p&gt;
&lt;p&gt;Who knows where it will lead. I will certainly be keeping my ear to the ground should an exciting job/contract come around. Ideally I'd want something remote / freelance which I could do on my terms and, should the right opportunity drop in my lap, I'd certainly be open to it. That said, what I'd really like to do is resurrect my company and see where I can take it. I have a couple of ideas with potential but will hold off on developing them until I've refamiliarised myself with the current trends in software development because, even though it's only been a few months, it feels like I've been out of the game for years.&lt;/p&gt;
&lt;p&gt;Well, game on.&lt;/p&gt;



                                </content:encoded>
		</item>
		<item>
			<title>The Year In Review</title>
			<link>http://ian.bebbs.co.uk/posts/TheYearInReview</link>
			<description>&lt;p&gt;Saturday morning, having realized that we had no commitments or prior engagements until the following Tuesday, my partner and I decided to go on a trip. We rapidly packed bags for ourselves and our little girl, threw them in the van and set off with no set destination other than "the south-west". We ended up staying in a small farm on Exmoor and having a terrific time walking on the moor and visiting nearby sites.&lt;/p&gt;</description>
			<guid>http://ian.bebbs.co.uk/posts/TheYearInReview</guid>
			<pubDate>Wed, 18 Jan 2017 00:00:00 GMT</pubDate>
			<content:encoded>
                                        


&lt;p&gt;Saturday morning, having realized that we had no commitments or prior engagements until the following Tuesday, my partner and I decided to go on a trip. We rapidly packed bags for ourselves and our little girl, threw them in the van and set off with no set destination other than "the south-west". We ended up staying in a small farm on Exmoor and having a terrific time walking on the moor and visiting nearby sites.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Other than the liberty of being able to just "get away", this weekend was notable as the previous week marked exactly one year since I left full-time employment.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Back in July, following an epic road-trip around Europe with my girls, I wrote &lt;a href="http://ian.bebbs.co.uk/posts/WotNoBlogPosts"&gt;a long blog post&lt;/a&gt; about the trip and my plans for the future. These plans centred on taking the rest of the year off as a career-break and using this time to support my family while investigating/evaluating emerging technologies in the .NET ecosystem.&lt;/p&gt;
&lt;p&gt;Now, six months on, it's time to look back over the last year and perform a retrospective; what went well, what could have been better and what should be changed moving forward.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Note: This is a long, detailed post with a lot of information/reflection about the previous year. As such I have published it as a MVP (Minimal Viable Post) and will iterate additional information into the post in the future.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;A Timeline&lt;/h2&gt;
&lt;p&gt;To aid the retrospective process, I spent last week learning &lt;a href="https://www.dartlang.org/"&gt;Dart&lt;/a&gt; and &lt;a href="http://www.pixijs.com/"&gt;Pixi.js&lt;/a&gt; in order to create a timeline visualization of the year. I'm not going to go into the process of making this visualization here as I want to focus on the retrospective, however I will be writing a "making of" follow-up post as the process was both very interesting and thoroughly enjoyable.&lt;/p&gt;
&lt;p&gt;Below you can see the visualization; a timeline covering the entirity of last year and showing the various aspects of my behaviour / productivity on a day-by-day basis. In order to fit into the horizontal space available in most browsers/platforms, the timeline is scaled-down along the horizontal axis. As this obscures much of the data, the timeline was designed to allow you to use your mouse or finger to interactively zoom in and and pan across the it. Give it a go.&lt;/p&gt;
&lt;p&gt;In the following sections I will discuss the various datasets displayed along with a summary of what they represent.&lt;/p&gt;
&lt;script defer="" src="../Content/Scripts/timeline.dart.js"&gt;&lt;/script&gt;
&lt;div id="timeline"&gt;&lt;/div&gt;
&lt;h2&gt;State&lt;/h2&gt;
&lt;p&gt;This timeline represents my primary oocupation at any given time. From a high-level, last year can be broadly split into two halves, travelling for the first half and working for the latter. However, the detail is more telling so the timeline has been further split into seven disctinct states:&lt;/p&gt;
&lt;h3&gt;Employment&lt;/h3&gt;
&lt;p&gt;My last day of employment was Friday 15th January, 2016. I had worked at my previous company for over four years and very much enjoyed my time there. Unfortunately an &lt;a href="http://www.daedtech.com/top-heavy-department-growth/"&gt;internal power struggle&lt;/a&gt; resulted in a change of senior management and a move away the progressive development methodologes which had made the company such a great place to work. This, to me, signalled that it was time to move on.&lt;/p&gt;
&lt;h3&gt;Sabatical&lt;/h3&gt;
&lt;p&gt;I considered the couple of months following ending full time employment as a sabatical; time to reflect, get my affairs in order and spend time with my young family before  considering how to move forward.&lt;/p&gt;
&lt;p&gt;At this time my daughter was just 4 months old so this presented a great opportunity to support my partner and engage with my baby. Having also taken 6 weeks of &lt;a href="https://www.gov.uk/shared-parental-leave-and-pay/overview"&gt;Shared Parental Leave&lt;/a&gt; the previous year, this meant that, of her first six months of life, I'd been able to spend almost four with her. I feel incredibly fortunate to be in a situation where this is could be the case. The first few months of a baby's life are incredibly tough for both the parents and the baby and I'm very happy that I was able to be with my girls throughout this time.&lt;/p&gt;
&lt;p&gt;It was towards the end of this period that my partner and I decided to capitalise on our unique situation and put the remainder of her maternity leave to use by taking our little girl on a road-trip around Europe. The implications of this decision were to dominate and, to a large degree, consume the entirity of the next three months.&lt;/p&gt;
&lt;h3&gt;Preparation&lt;/h3&gt;
&lt;p&gt;Having made the decision to spend a significant amount of time touring Europe, with an 8 month old baby no less, it became apparent that there would be a whole lot of preparation required to make this happen. In March we decided to buy a Nissan Elgrand E51 Rider Autech (shown below) and April onwards was spent preparing it for our trip.&lt;/p&gt;
&lt;img src="https://zdfcta-dm2306.files.1drv.com/y3mRDVqstf6XsjN73biGoYnN5s-SzVcHx9pEdYl2S_wM803HpiosWK5Skxm-kskeHTMkahk_PIdejAp70x14KCaOwmZb6OvAvAig8uqI4bdBs2Pb_lAUsm-O_2lO8krW_18ReDVMLXqVkJWUGUSJ-d80bOIi8ciWIgyxDzL5fw4qF4?width=1796&amp;amp;height=1347&amp;amp;cropmode=none" class="img-responsive" style="margin: auto; width:800; margin-top: 6px; margin-bottom: 6px;" alt="Nissan Elgrand E51 Rider Autech"&gt;
&lt;p&gt;This preparation was complex, with numerous requirements, constraints and learning curves across a variety of disciplines... but also a whole lot of fun. On our return I wrote a &lt;a href="http://elgrandoc.uk/forum/index.php?threads/touring-europe-in-an-elgrand-part-i-to-v.3311"&gt;very long post&lt;/a&gt; for the &lt;a href="http://elgrandoc.uk/forum/index.php"&gt;Elgrand Owners Club&lt;/a&gt; which detailed all the work I had done on the van. While the post took a significant amount of time to write, the forums were invaluable in providing the information I needed to do the work in the first place so I thought it was worth investing this time in case the post helped someone else in the same manner. To this day, the post is still one of the most viewed and commented on posts in the forum. It's well worth a read a read if you have time; if not, here are some pictures of the work instead:&lt;/p&gt;
&lt;img src="https://mkakza-dm2306.files.1drv.com/y3mTMvumgbiWf4cuNId6rdQdXFkrZEMe891yFE3uaLIn1eWi8gsAGfCe6dwKUZjSfNqkfJHNpdzXf60mJIb2RH1HlQ1ku-6IEk5IPtKNoDHcJnVFi1_AIGA-TvxRXvplxSaxmqN385ks_cd0YZ2dxxdpU3ylY7UlYPW2uP9sp8Sr9A?width=660&amp;amp;height=371&amp;amp;cropmode=none" class="img-responsive" style="margin: auto; width:800; margin-top: 6px; margin-bottom: 6px;" alt="Cargo frame"&gt;
&lt;img src="https://dotw7g-dm2306.files.1drv.com/y3mAeFtD-P9o5iDeydmal4JBd-518YT4V1-WPcdo33zt8xDn7vLENvSjzaMzFf5U52ZKreB49IRMJTdoMGXtxtSsLYUdF0mPvkYh-6ts_sihIt3aWMgd2hHlmkIzm-twj54_tTlpRJm2Xi3B58mGjSM_ZxF-aAQcQq842NpEBTlOVE?width=660&amp;amp;height=371&amp;amp;cropmode=none" class="img-responsive" style="margin: auto; width:800; margin-top: 6px; margin-bottom: 6px;" alt="Electrics box"&gt;
&lt;img src="https://adlakq-dm2306.files.1drv.com/y3mw8fNegXFskURlkB2qFnfOJA7BsmrmPW7hx3Xyflhqwi52YIR8NEdSqz-acBvO6cTO2X5Atjj6U2uTFjGiLhTofbHGbteqx1B1MxGWouRBLO1WF1lTUCVMLbsUJmwFy2jPKb84N9xG_-1YuOmhqovFb59UuxMYxasZ0yyVDJgAtY?width=660&amp;amp;height=371&amp;amp;cropmode=none" class="img-responsive" style="margin: auto; width:800; margin-top: 6px; margin-bottom: 6px;" alt="Electrics fitted"&gt;
&lt;img src="https://plsn1w-dm2306.files.1drv.com/y3mt5MSkFV_zVYge_IQZeDn02BvYd2ngjLePZYmte7YKsEIpa-e_n6BRgS0YFM5F1fge5qaJX5aAVLsyfhxVQo11B13NwfQ1hIQy47gdIcoBCjIWPPklSUKFIG59c2SXhfEaH0_xILPK35Awxm2RTs8bWvsLtkMtNTNs4V3t4qKXSw?width=660&amp;amp;height=371&amp;amp;cropmode=none" class="img-responsive" style="margin: auto; width:800; margin-top: 6px; margin-bottom: 6px;" alt="Cargo fitted"&gt;
&lt;img src="https://mhrszq-dm2306.files.1drv.com/y3m5m7EX2zp-dMeKdwNa-8RMrBapeb5hhLCpRCj8bfI277ok0RZOs7Rv3GAdXQRiTsVfE6L-skEtFtkcgoi8Nf5qo1vxuw4aSoLPyMgXr5kHtDZQ-D5Sp1Lz2_-FoBNl8Zcj9bO531eW6_WFtu_Z0SE_QiDH2CWFp_-6hFL3fljt74?width=660&amp;amp;height=371&amp;amp;cropmode=none" class="img-responsive" style="margin: auto; width:800; margin-top: 6px; margin-bottom: 6px;" alt="Cable routed"&gt;
&lt;p&gt;During this time I learned a lot about carpentry, electronics and engineering... but I almost didn't. Prior to undertaking the work, I had looked to find a conversion specialist who would do the work for me but most either only provided pattern-part conversions or were too busy to undertake a bespoke project. Well, "necessity being the mother of invention", I set to it and, in retrospect, am extremely glad I did.&lt;/p&gt;
&lt;p&gt;Despite barely having used an electric drill previously and never having dabbled in 240v electronics, I enjoyed every aspect of the work and am immensely proud of it. We didn't experience a single issue with my work while we were away and, even if we had, it was extremely comforting knowing that I could have resolved the issue in the field if I'd had to.&lt;/p&gt;
&lt;p&gt;Should a similar challenge ever present itself again, I would certainly look at taking it on and, furthermore, would encourage others to do so also as you undoubtedly &lt;a href="https://hbr.org/2016/07/if-youre-not-outside-your-comfort-zone-you-wont-learn-anything"&gt;learn a lot more when working outside of your comfort zone&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Touring Europe&lt;/h3&gt;
&lt;p&gt;All the prep done - well, as much as was possible in the time available - we left home on May 20th and didn't return again until July 17; totaling just over 8 weeks on the road. In this time we traveled &lt;strong&gt;in excess of 8000 miles and visited 22 countries&lt;/strong&gt; seeing numerous friends en-route and making many more in the process.&lt;/p&gt;
&lt;p&gt;Perhaps it was all the prep or perhaps we were just lucky but the trip went off without a hitch and everyone - including our little girl - had an amazing time. Both &lt;a href="https://bigspune.wordpress.com/"&gt;my partner&lt;/a&gt; and &lt;a href="http://ian.bebbs.co.uk/posts/WotNoBlogPosts"&gt;I&lt;/a&gt; blogged about the trip - for ourselves as much for any potential reader - and, six months on, are still reflecting on what a fantastic adventure it was.&lt;/p&gt;
&lt;p&gt;Without a shadow of a doubt, and even when considering the week's of preparation before leaving, this trip was time well spent. My partner and I learned a lot about each other ("hey, we &lt;em&gt;can&lt;/em&gt; spend several weeks in a confined space together, 24/7") and our daughter and have accumulated memories that will last a lifetime. We have already talked about doing another similar trip in the future and have begun planning shorter breaks around the UK as a stop-gap until an opportunity for a new adventure comes along.&lt;/p&gt;
&lt;h3&gt;Study&lt;/h3&gt;
&lt;p&gt;Upon my return, I came across a code for a free &lt;a href="https://borntolearn.mslearn.net/b/weblog/posts/sign-up-to-take-the-beta-exam-for-developing-mobile-apps-70-357-today"&gt;Microsoft beta exam about mobile application development&lt;/a&gt;. As I was interested in UWP app development and had already taken loads of these beta exams in previous years, I decided to sit the exam even though the code was only valid for a few weeks after signing up, which left very little time to study!!&lt;/p&gt;
&lt;p&gt;As I &lt;a href="https://en.wikipedia.org/wiki/Experiential_learning"&gt;learn&lt;/a&gt; &lt;a href="http://psycnet.apa.org/journals/bdb/17/1/1.pdf&amp;amp;productCode=pa"&gt;best&lt;/a&gt; &lt;a href="https://www.virgin.com/richard-branson/you-learn-by-doing-and-by-falling-over"&gt;by&lt;/a&gt; &lt;a href="https://news.uchicago.edu/article/2015/04/29/learning-doing-helps-students-perform-better-science"&gt;doing&lt;/a&gt; my study focused on writing a UWP app that incorporated as many of the &lt;a href="https://www.microsoft.com/en-us/learning/exam-70-357.aspx"&gt;examined technical aspects&lt;/a&gt; as possible. While the methodologies I used for this app were relatively new (i.e. CQRS/ES) the learning curve for this exam really came from some of the unique aspects of UWP app development such as designing for multiple devices/screen sizes, using &lt;a href="https://docs.microsoft.com/en-us/ef/core/"&gt;new persistence technologies&lt;/a&gt; and leveraging the huge new API surface available to UWP apps.&lt;/p&gt;
&lt;p&gt;Due to an awful dashboard implementation which &lt;a href="https://borntolearn.mslearn.net/b/weblog/posts/more-tips-about-beta-exams-what-to-expect-when-you-39-re-expecting-your-beta-results-that-is"&gt;causes much confusion for students who sit beta exams&lt;/a&gt;, I am still unsure as to whether I passed this exam or not. My dashboard has not changed to say I have passed the exam but I have not received a summary sheet from Microsoft with my scores nor am I able to access them from my MCP dashboard. In short, the current beta exam process is extremely poor and much worse than it was five or six years ago when I took (and passed) around a dozen such beta exams.&lt;/p&gt;
&lt;p&gt;Regardless, time spent on this study was a huge win. I learned a lot about a platform which is becoming increasingly prevalent in the Windows ecosystem. UWP apps are able to run on everything from Raspberry Pi to Hololens, and I have since deployed/released UWP apps for Raspberry Pi, Mobile (phone/table), PC and XBox. Indeed, after completing the exam, I continued to develop my UWP study app over the next few months.&lt;/p&gt;
&lt;h3&gt;Holiday&lt;/h3&gt;
&lt;p&gt;Across the year, I allocated around 40 days (including bank-holidays and weekends) as non-work days (aka holiday). This included snowboarding with friends, trips abroad, visiting relatives and a long festive break over Xmas. Although more than the holiday allowance at most full time jobs (depending on how it's allocated), I feel this represents a fairly decent split between work and play.&lt;/p&gt;
&lt;h3&gt;OneCog.Solutions&lt;/h3&gt;
&lt;p&gt;Once life had returned to normal following our tour of Europe and taking the Microsoft beta exam, I decided to work on a few projects that were interesting to me.&lt;/p&gt;
&lt;p&gt;Firstly, this was continuing my investigation of UWP, initially to &lt;a href="http://ian.bebbs.co.uk/posts/CqrsEsMvvmRxEfSqlUwpPcl"&gt;flesh out&lt;/a&gt; and &lt;a href="https://www.microsoft.com/en-us/store/p/littlelittle/9nblggh51b1b"&gt;release&lt;/a&gt; the UWP app I had written while studying for the UWP exam but then to also evaluate UWP as a &lt;a href="http://ian.bebbs.co.uk/posts/MonsterPi"&gt;platform for IoT devices&lt;/a&gt;. I then proceeded to broaden my area of investigation into &lt;a href="http://ian.bebbs.co.uk/posts/DockerAndKafka"&gt;non-Microsoft technologies&lt;/a&gt; and even into areas that, while still technical, were &lt;a href="http://ian.bebbs.co.uk/posts/3DPrintingWithTheCelRobox"&gt;not directly involved with software development&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;During this time I endeavored to keep an up-to-date public profile. Through a &lt;a href="https://trello.com/b/KoTWuFUi/public-board"&gt;public Trello board&lt;/a&gt;, products I had released (&lt;a href="https://www.microsoft.com/en-gb/store/p/littlelittle/9nblggh51b1b"&gt;LittleLittle&lt;/a&gt; &amp;amp; &lt;a href="https://www.microsoft.com/en-gb/store/p/toddlerbox/9nblggh3zr4l"&gt;ToddlerBox&lt;/a&gt;), posts to &lt;a href="http://ian.bebbs.co.uk/"&gt;my blog&lt;/a&gt; and contributions to both &lt;a href="https://github.com/ibebbs"&gt;Github&lt;/a&gt; and &lt;a href="http://stackoverflow.com/users/628821/ibebbs"&gt;StackOverflow&lt;/a&gt; I tried to make sure as much of my time as possible was surfaced publicly. Indeed, having become extremely delivery-oriented since first drinking the agile (&lt;a href="https://medium.com/swlh/agile-is-the-new-waterfall-f7baef5d026d#.jpmks6gi4"&gt;lower-case 'a'&lt;/a&gt;) kool-aid several years ago, I feel the public-facing nature of these interactions substituted as a form of delivery; or, in Agile parlance, became my &lt;a href="https://www.agilealliance.org/glossary/definition-of-done/"&gt;"definition of done"&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;While, in retrospect, I had hoped to have investigated many more technologies than I actually managed during this time, overall I would consider this time well spent. In addition to releasing two applications to the Windows Store and contributing to several public Github projects (more on this later) I managed to rack up over 1000 rep on StackOverflow in just a couple of months.&lt;/p&gt;
&lt;p&gt;Unfortunately, during this time, I didn't achieve my primary goal; namely finding a project that could potentially be grown into a marketable product and form the basis of a company. While there were no shortage of ideas and there still remain a couple of "coals in the fire", I don't think I've yet found the gap in the market I'm looking for. One slight positive note here is that I have, at the very least, determined a couple of markets I'm very keen on investigating further and in which I'd like to work in the future.&lt;/p&gt;
&lt;h2&gt;Health&lt;/h2&gt;
&lt;p&gt;For this visualization I have employed step count (vertical bars), weight (yellow line with dotted yellow representing ideal weight) and fat-mass (red line with dotted red representing ideal fat-mass) as a (very rough) approximation of health. While, in theory, I should have had more leisure time available to exercise over the course of the year, in real terms I found that I did far less than expected. This was due to a number of factors:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;My previous employer, being a company that revolves around football, had regular football matches/tournaments between company staff which were of a very high standard. I played in as many of these as possible and, once I left, I very much missed the exercise.&lt;/li&gt;
&lt;li&gt;My commute to work each day involved a (voluntary) walk from St Pancras station to Camden Lock - and back again - every day. This constituted 30 minutes/5000 steps of valuable exercise each day that was no longer necessary when I started working from home.&lt;/li&gt;
&lt;li&gt;I ran a football team for my village that played in the local Bedfordshire league. Unfortunately, due to lack of players this team folded at the end of last season leaving me entirely bereft of football for the majority of the year.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;At punching-weight, I aim to be 11st and 15% fat-mass. This used to be a fairly common occurrence while playing football regularly but, as can be seen, I've not managed to get back into a similar condition this year. Fortunately, I &lt;em&gt;have&lt;/em&gt; managed to avoid devolving into a complete bucket of lard thanks to several things:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Parenthood - As any parent will tell you, being a parent is both exhausting and relentless. While previously I would undertake very little exercise in the evening, nowadays I expend a significant amount of energy each evening running around after my daughter. While not often recognised, this constitutes a fairly high level of activity.&lt;/li&gt;
&lt;li&gt;Preparation for Touring Europe - During this time I did &lt;strong&gt;a lot&lt;/strong&gt; of very physical work. From lugging construction materials and tools around to disassembling and reassembling the van, the average day was extremely active, especially when compared to sitting in an office chair for eight hours.&lt;/li&gt;
&lt;li&gt;European Tour - as can be seen from the step count, although a great deal of time was spent driving, our time away involved a lot of other activity. This activity (particularly loading and unloading the van each day!) provided a lot of exercise and helped keep me fit over the summer.&lt;/li&gt;
&lt;li&gt;Swimming - In a desperate attempt to remain in some sort of shape, I have recently started swimming semi-regularly.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In retrospect, something that came as no surprise but for which I failed to anticipate the true impact, was the effect of not exercising on my state of mind and general happiness. Having always been an active and optimistic individual, I completely underestimated the tight correlation between these two characteristics. Mid-November saw me at my least active and, as I've come to realise, my least productive/focused (as evidenced by the sudden surge in StackOverflow rep!). Fortunately I try be somewhat self-reflective and, recognising that my diminished level of activity might be a possible cause for a perceived lack of progress, I decided to start swimming again as a means to address this.&lt;/p&gt;
&lt;p&gt;Now, while I'm still not exercise as much as I would like, I at least feel like my activity levels are sufficient to allow me to focus on projects and achieve my deliverables. Moving forward I very much hope to re-establish the village football team in time for next season or join another team such that I get at least a couple of games a month.&lt;/p&gt;
&lt;h2&gt;Finances&lt;/h2&gt;
&lt;p&gt;In counterpoint to the Health visualization described above, the Finances visualization shows a relative interpretation of my financial health across the year.&lt;/p&gt;
&lt;p&gt;At the beginning of the year, when considering a career-break, I tried to calculate a monthly "burn-rate" (i.e. total out-goings per month). Given I was not planning to change my lifestyle at all and my partner and I had already planned a number of holidays, I arrived at a rough estimate of £3k per month. This was affordable and, while it would put a dent in my savings, wouldn't leave me worrying about money at the end of the year.&lt;/p&gt;
&lt;p&gt;Through-out the year there were a number of factors that further affected my finances:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Touring Europe in the van cost significantly more than I estimated.&lt;/li&gt;
&lt;li&gt;My daughter's nursery care was cheaper than expected since moving her from four days a week to three (instead having an additional 'daddy-day-care' day).&lt;/li&gt;
&lt;li&gt;While I typically spend a significant amount on computer hardware, this year I required very little but for a relatively cheap new server.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now, looking back across the year (and excluding large one-off payments like buying the van) it seems my burn-rate was closer to £2k, one third less than I had planned. This coupled with receiving returns on previous investments meant I actually ended the year significantly better off than I started it. While this definitely wouldn't be the case were I to continue the career-break for a second year, it is very reassuring to know that I could, should I decide to do so.&lt;/p&gt;
&lt;h2&gt;Github contributions&lt;/h2&gt;
&lt;p&gt;Across the year - although predominantly in the last few months - I made 368 commits to Github. These were spread across a couple of dozen repositories but mostly focused on the (private) repository for &lt;a href="https://www.microsoft.com/en-gb/store/p/littlelittle/9nblggh51b1b"&gt;LittleLittle&lt;/a&gt; and my (statically generated &amp;amp; github-pages hosted) &lt;a href="https://github.com/ibebbs/ibebbs.github.io"&gt;blog&lt;/a&gt;. Additionally, I contributed to several open source repositories, most notably the &lt;a href="https://github.com/Microsoft/UWPCommunityToolkit"&gt;UWP Community Toolkit&lt;/a&gt; (1000 stars) and &lt;a href="https://github.com/beto-rodriguez/Live-Charts"&gt;Live-Charts&lt;/a&gt; (875 stars).&lt;/p&gt;
&lt;p&gt;Given 2016 was a leap-year, 368 commits in 366 days averages (just!) over one a day. Overall I'm quite pleased with this level of commitment and the quantity of work it represents, especially considering the amount of time this year spent not working. Additionally, for the last three months of this year, my partner and I have both elected to work four-day weeks as we felt it represented a better balance between parental and nursery care for our daughter. For me this meant Thursdays became "daddy-day-care" so that I could take my daughter swimming - something we both really enjoy - but which adversely affected my productivity.&lt;/p&gt;
&lt;p&gt;Moving forward I intend to start publishing much more of my work to public repositories including regular updates to my &lt;a href="https://github.com/ibebbs/Spikes"&gt;"Spikes"&lt;/a&gt; repository which contains investigative projects and examples for solutions to various StackOverflow questions.&lt;/p&gt;
&lt;h2&gt;StackOverflow&lt;/h2&gt;
&lt;p&gt;Talking of StackOverflow questions, the next visualization shows StackOverflow Reputation accumulation across the year (light blue line) with gains shown day-by-day (dark blue boxes). While I predominantly consider it a distraction, I really enjoy answering questions on StackOverflow, probably due to the awesome &lt;a href="https://en.wikipedia.org/wiki/Gamification"&gt;gamification&lt;/a&gt; employed on the &lt;a href="http://stackexchange.com/"&gt;StackExchange&lt;/a&gt; sites.&lt;/p&gt;
&lt;p&gt;While there is a low level of fairly constant reputation gain, most of the gains came in short bursts. As discussed above, I attribute this losing focus on my main projects due to inactivity. Still, there are far worse forms of procrastination than helping people and I consider this a relatively good use of time.&lt;/p&gt;
&lt;p&gt;I am currently endeavoring to get the 'Fanatic' badge (visit StackOverflow for 100 days consecutively) but trying not to let it get in the way of other work. This usually means only answering questions when I can quickly point the asker in the right direction but I will occasionally field a more challenging question when it's &lt;a href="http://stackoverflow.com/questions/tagged/system.reactive"&gt;something I'm particularly interested in&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Blog posts&lt;/h2&gt;
&lt;p&gt;Over the course of the year I have written and published 22 blog posts - totaling nearly 24,000 words - across a range of subjects including:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;3D PRINTING (1) APACHE KAFKA (1) APACHE ZOOKEEPER (1) ATTACHED PROPERTY (1) BDD (1) BEHAVIORAL (1) BLOG (3) CAREER BREAK (1) CLIENT-SIDE FRAMEWORK (2) CQRS (1) DART (1) DATAFLOW (1) DOCKER (1) DVD RIPPING (1) EF (1) ELASTIC STACK (4) ELASTICSEARCH (4) EVENT SOURCING (1) EVENTS (1) EVENTSOURCING (1) FUTURE (1) GITHUB (2) GROWTH (1) HYPERLINK (1) IOT (1) JAVASCRIPT (2) JS (1) KIBANA (4) LITTLELITTLE (1) LOGSTASH (4) MAPLIN (1) MONITORING (4) MVVM (1) NANOSERVER (1) NETWORKING (4) PARENTHOOD (1) PATTERNS (1) PRODUCTIVITY (1) RASPBERRYPI (1) REACTIVE (7) REST (1) ROBOX (1) RX (7) SQL (1) STATE MACHINES (1) SVG (1) SYSLOG (4) TESTING (1) TODDLERBOX (3) TOOLKIT (2) TPL (2) TRAVEL (1) TYPESCRIPT (1) UWP (7) VISUAL STUDIO (2) VISUAL STUDIO CODE (1) WEBRX (2) WINDOWS SERVER 2016 (1) WYAM (1) XAML (2) XBOX (3)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This content garners an average of around 110 unique visitors and 400 page views every month.&lt;/p&gt;
&lt;p&gt;One of these posts - &lt;a href="https://blogs.msdn.microsoft.com/dotnet/2016/09/07/the-week-in-net-972016/"&gt;"The absolute easiest way to use SVG icons in UWP apps"&lt;/a&gt; was featured in &lt;a href="https://blogs.msdn.microsoft.com/dotnet/2016/09/07/the-week-in-net-972016/"&gt;"The week in .NET – 9/7/2016"&lt;/a&gt;. Furthermore, blog posts I have shared with people on other platforms (i.e. StackOverflow, MSDN, etc) have been understood and well received.&lt;/p&gt;
&lt;p&gt;All in, I think I've added some valuable content to my blog this last year but feel like I should have tried harder to increase it's reach. While writing these blog posts inevitably takes a considerably amount of time, I intend to continue making as many new posts as possible, hopefully across an increased range of subjects and - by submitting to various aggregation blogs - with increased readership.&lt;/p&gt;
&lt;h2&gt;Commitments&lt;/h2&gt;
&lt;p&gt;This timeline displays any days I had expected to work yet was unable to for a variety of reasons. Over the course of the year this accounted for 42 days, mostly the Thursdays that have become my "daddy-day-care" day and during which I take my daughter swimming. While these "commitments" would have been worked around had I been in full-time employment, the flexibility of working when I can as opposed to when I have to has undoubtedly led to a greater degree of motiviation and concentration when I am in front of the computer.&lt;/p&gt;
&lt;h1&gt;Summary&lt;/h1&gt;
&lt;p&gt;Given this fairly comprehensive look back across the year, it's time to perform the retrospective.&lt;/p&gt;
&lt;h3&gt;What went well?&lt;/h3&gt;
&lt;p&gt;There were numerous wins across the year including:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Getting to spend the year with my partner and baby which included spending two months traveling Europe in a camper-van.&lt;/li&gt;
&lt;li&gt;Releasing applications to the Windows Store for phone, tablet, PC and XBox.&lt;/li&gt;
&lt;li&gt;Furthering the breadth of my technical knowledge of various platforms, technologies and methodologies.&lt;/li&gt;
&lt;li&gt;Adding a significant amount of content to my blog.&lt;/li&gt;
&lt;li&gt;Contributing to numerous repositories on Github including several for popular open-source projects.&lt;/li&gt;
&lt;li&gt;Increasing my StackOverflow Reputation level by answering questions on a variety of subjects.&lt;/li&gt;
&lt;li&gt;Fully realising the importance of exercise on productivity.&lt;/li&gt;
&lt;li&gt;Completing the year in better financial health than I had expected.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;What didn't go so well?&lt;/h3&gt;
&lt;p&gt;Given that, despite taking a career break, I continued to be productive in a variety of ways, I don't feel like too many things went badly this year. However there are a few things I would have liked to have achieved but didn't such as:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Not finding the "market gap" or having the "killer idea" I had hoped to translate into a start-up.&lt;/li&gt;
&lt;li&gt;Not investigating as many technologies as I had hoped across the course of the year.&lt;/li&gt;
&lt;li&gt;Lacking a significant amount of exercise and thereby loosing focus and momentum for ongoing projects.&lt;/li&gt;
&lt;li&gt;Shying away from increasing the exposure of my blog content via news aggregators, social media, etc.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;What needs to change moving forward&lt;/h3&gt;
&lt;p&gt;Well, I gave myself the year to come up with the "killer idea" but didn't manage it so now it's time to change tack. In the coming months I will be starting to look for contract work which, ideally, I can do remotely. Fortunately there is no pressure for me to return to work so I can be quite picky about the roles I take. The ideal contract would ideally feature one or all of the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;UWP/WPF&lt;/li&gt;
&lt;li&gt;Rx/Streaming technologies&lt;/li&gt;
&lt;li&gt;.NET Core&lt;/li&gt;
&lt;li&gt;IoT&lt;/li&gt;
&lt;li&gt;NoSQL datastores&lt;/li&gt;
&lt;li&gt;CQRS/ES&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Ideally in the following markets:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Baby/Toddler/Child care/entertainment&lt;/li&gt;
&lt;li&gt;Social Justice&lt;/li&gt;
&lt;li&gt;Automation&lt;/li&gt;
&lt;li&gt;Agriculture&lt;/li&gt;
&lt;li&gt;Renewable Energy&lt;/li&gt;
&lt;li&gt;Automotive&lt;/li&gt;
&lt;li&gt;Economics&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;That said, given the large number of &lt;a href="https://blog.rjmetrics.com/2014/04/30/the-big-opportunities-in-the-unknown-unknowns/"&gt;"unknown unknowns"&lt;/a&gt;, perhaps there are other technologies / markets I've not encountered yet that would be equally enthralling.&lt;/p&gt;
&lt;p&gt;In the mean time, I will continue to investigate other interesting technologies and release updates to my existing products as appropriate.&lt;/p&gt;
&lt;p&gt;Regarding family life, well, in this respect I hope to continue the extremely rewarding work/life balance my partner and I have achieved over the last year. This could be difficult once the demands of external deadlines become a reality again but it's something I would be extremely reluctant to change.&lt;/p&gt;
&lt;h2&gt;In conclusion&lt;/h2&gt;
&lt;p&gt;It's been an amazing year and one I will remember for a long time. Regardless of what I end up doing, it is my aim to take another career break in the not too distant future in which I will endeavor to repeat the successes of this year, remedy the failures and remind myself that it's good to look up from the computer from time to time.&lt;/p&gt;



                                </content:encoded>
		</item>
		<item>
			<title>The absolute easiest way to use SVG icons in UWP apps</title>
			<link>http://ian.bebbs.co.uk/posts/UsingSVGInUWP</link>
			<description>&lt;p&gt;There are &lt;a href="http://stackoverflow.com/a/3528493/628821"&gt;many&lt;/a&gt; (&lt;a href="http://stackoverflow.com/a/22107360/628821"&gt;many&lt;/a&gt;, &lt;a href="http://blogs.u2u.be/diederik/post/2012/07/26/Transforming-SVG-graphics-to-XAML-Metro-Icons.aspx"&gt;many&lt;/a&gt;) ways to use SVG assets as icons in UWP / XAML apps, most requiring some form of DataTemplate or UserControl. While these approaches work &lt;em&gt;ok&lt;/em&gt; they're normally a pain to author and use, often requiring custom converters to be written if the asset is to be used via any form of data binding. Here I present an extremely flexible way of using these assets that requires nothing more than drag-and-drop.&lt;/p&gt;</description>
			<guid>http://ian.bebbs.co.uk/posts/UsingSVGInUWP</guid>
			<pubDate>Thu, 01 Sep 2016 00:00:00 GMT</pubDate>
			<content:encoded>
                                        


&lt;p&gt;There are &lt;a href="http://stackoverflow.com/a/3528493/628821"&gt;many&lt;/a&gt; (&lt;a href="http://stackoverflow.com/a/22107360/628821"&gt;many&lt;/a&gt;, &lt;a href="http://blogs.u2u.be/diederik/post/2012/07/26/Transforming-SVG-graphics-to-XAML-Metro-Icons.aspx"&gt;many&lt;/a&gt;) ways to use SVG assets as icons in UWP / XAML apps, most requiring some form of DataTemplate or UserControl. While these approaches work &lt;em&gt;ok&lt;/em&gt; they're normally a pain to author and use, often requiring custom converters to be written if the asset is to be used via any form of data binding. Here I present an extremely flexible way of using these assets that requires nothing more than drag-and-drop.&lt;/p&gt;
&lt;p&gt;This approach uses &lt;a href="https://glyphter.com/"&gt;Glyphter&lt;/a&gt; - a free, online tool for converting SVG icons to fonts - to produce a custom font containing your SVG assets; similar to fonts like &lt;a href="http://modernicons.io/segoe-mdl2/cheatsheet/"&gt;Segoe MDL2&lt;/a&gt; and &lt;a href="http://fontawesome.io/"&gt;FontAwesome&lt;/a&gt;. Glypter's free tier allows you to craft a single font, containing just basic alpha-numerics. Should you need more than this, you can upgrade to a premium tier which allows you to work on multiple fonts of a much greater size.&lt;/p&gt;
&lt;p&gt;Once you've built your font, it can be embedded in the app package and icons displayed by simply using a TextBlock element with the Text property set to the alpha-numeric code of the icon to display and the FontFamily set to the custom font. Furthermore, you're easily able to present the required icons in the desired colour - by changing the TextBlock's Foreground brush - and size - by changing FontSize or embedding within the TextBlock within a ViewBox.&lt;/p&gt;
&lt;p&gt;To get started, simply locating the SVG asset you'd like to use. &lt;a href="https://thenounproject.com/"&gt;The Noun Project&lt;/a&gt; is a good resource containing an incredible number of high quality icons that can be used in commercial products for a small fee or via attribution. Once you've found the icon you want to use - lets use &lt;a href="https://thenounproject.com/localdomain/collection/memes/?oq=meme&amp;amp;cidx=0&amp;amp;i=105269"&gt;this one&lt;/a&gt; by &lt;a href="https://thenounproject.com/localdomain/"&gt;Gareth&lt;/a&gt; - simply download the icon as an SVG. Once it's downloaded, locate the file and drag it onto the Glypter grid in the desired location. After a short pause while the file is uploaded to Glypter and processed, it'll appear in the grid slot you selected; here I've added the icon to the 'A':&lt;/p&gt;
&lt;img src="/Content/UsingSVGInUWP/DraggedIntoGlyphter.png" class="img-responsive" style="margin: auto; width:600px; margin-top: 6px; margin-bottom: 6px;" alt="Glyphter custom font"&gt;
&lt;p&gt;Repeat this for all the icons you want to use - I'll just stick with the one icon for now - then download the font by clicking the font download button (the 'down arrow' in the 'FONT' button). This will compile all your assets into a zip file containing the font (in &lt;a href="https://en.wikipedia.org/wiki/TrueType"&gt;'.ttf'&lt;/a&gt; and &lt;a href="https://en.wikipedia.org/wiki/Web_Open_Font_Format"&gt;'.woff'&lt;/a&gt; formats), svg and css assets. We're only interested in the '.ttf' file so extract it from the zip file and copy it to the 'Assets' folder of your UWP app.&lt;/p&gt;
&lt;p&gt;From within VisualStudio - or better yet, Blend - add the '.ttf' file to the project, ensuring it's 'Build Action' is set to 'Content' and 'Copy to Output Directory' set to 'Do not copy' as shown below:&lt;/p&gt;
&lt;img src="/Content/UsingSVGInUWP/BlendWithGlyphterFont.png" class="img-responsive" style="margin: auto; width:600px; margin-top: 6px; margin-bottom: 6px;" alt="Custom font added to project in Blend"&gt;
&lt;p&gt;With this in place, when you drop a TextBlock on a page, you should be able to select your embedded font from the Font combobox as shown below:&lt;/p&gt;
&lt;img src="/Content/UsingSVGInUWP/SelectFont.png" class="img-responsive" style="margin: auto; width:600px; margin-top: 6px; margin-bottom: 6px;" alt="Custom font selection in Blend"&gt;
&lt;p&gt;The rest, as they say, is history. In a few minutes you too can use the following XAML:&lt;/p&gt;
&lt;pre class="prettyprint"&gt;&lt;code&gt;&amp;lt;Grid&amp;gt;
    &amp;lt;StackPanel HorizontalAlignment="Center" VerticalAlignment="Center"&amp;gt;
        &amp;lt;TextBlock Text="SVG in UWP EZ!" HorizontalAlignment="Center" FontFamily="Impact" Margin="10"/&amp;gt;
        &amp;lt;TextBlock Text="A" FontFamily="ms-appx:/Assets/Glyphter.ttf#Glyphter" FontSize="96" HorizontalAlignment="Center"/&amp;gt;
        &amp;lt;TextBlock Text="Y U NO USE!" HorizontalAlignment="Center" FontFamily="Impact"/&amp;gt;
    &amp;lt;/StackPanel&amp;gt;
&amp;lt;/Grid&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;Note: There seems to be an issue/inconsistency with the FontFamily value created when using the XAML designer to select your custom font. Sometimes, but not always, the designer will fail to add the "ms-appx" protocol to the FontFamily property value which, while it has no effect at design time, will prevent the app from locating the custom font at runtime. You should there ensure this value is present before deploying your app.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;To create this amazing UI:&lt;/p&gt;
&lt;img src="/Content/UsingSVGInUWP/SVGInUWPEZ.png" class="img-responsive" style="margin: auto; width:600px; margin-top: 6px; margin-bottom: 6px;" alt="SVG in UWP EZ! Y U NO USE!"&gt;
&lt;p&gt;Enjoy.&lt;/p&gt;



                                </content:encoded>
		</item>
		<item>
			<title>I contributed and all I got...</title>
			<link>http://ian.bebbs.co.uk/posts/IContributedAndAllIGot</link>
			<description>&lt;p&gt;A while back, I contributed my &lt;a href="http://ian.bebbs.co.uk/posts/UsingHyperlinkInMVVM"&gt;HyperlinkExtensions&lt;/a&gt; to the &lt;a href="http://ian.bebbs.co.uk/posts/UWPCommunityToolkitv1_1"&gt;UWP Community Toolkit&lt;/a&gt;. This morning postman brought me a very pleasant surprise:&lt;/p&gt;</description>
			<enclosure url="http://ian.bebbs.co.uk/y3mNzf5Sylp6SOZbv6t2Q_atZh0reZJW7RYL9EgdZ4V4aZSaf_8Sqe8UPzCEaq7t3RKQn5GX1DvkSWdsolooyOHZj9Ma389uQChI7uhzICOmFTm8aZaRBYS4jS0K8g07vYNjEJofO_Io8VnpaqgTRsLJwSIEMEzg1P1TOt9dbXzm6s%253Fwidth=2465&amp;height=1039&amp;cropmode=none" length="0" type="image" />
			<guid>http://ian.bebbs.co.uk/posts/IContributedAndAllIGot</guid>
			<pubDate>Sat, 12 Nov 2016 00:00:00 GMT</pubDate>
			<content:encoded>
                                        


&lt;p&gt;A while back, I contributed my &lt;a href="http://ian.bebbs.co.uk/posts/UsingHyperlinkInMVVM"&gt;HyperlinkExtensions&lt;/a&gt; to the &lt;a href="http://ian.bebbs.co.uk/posts/UWPCommunityToolkitv1_1"&gt;UWP Community Toolkit&lt;/a&gt;. This morning postman brought me a very pleasant surprise:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://orvy1q.dm2302.livefilestore.com/y3mcg6h9OGMl0tKB0ohOeyyBSVI-CwtcQF9XPWLXHBQI1inTRaPd9_k1jeoOpTIga1ZVnezoWRp34VnQuSmMduXhLro6OeJ4Unrgxe3PVOAh2kA3Pae1SN3wmxv3sxPDJlD1uTFxMKkKMk2lRjxKWZE1Ktqvs7k-mr6mCjq0UdKLh8?width=574&amp;amp;height=660&amp;amp;cropmode=none" alt="T-shirt front"&gt;&lt;img src="https://orvx1q.dm2302.livefilestore.com/y3mzcdee8-cKnRVTO-rrLFoC5rQMdorNPkUwiu49xlSHN6OeisdbwOFTzvPTLZzInaP0JBQLgOu9bkX8eZZZa4SuKd2ruJI80mI38NbMcTftpRMmE9zFx6DwWNGSctfhS6F2HRlKQ7Ve60D8Y6G94bbLVuFJ5hp4E9mYWQTG0QhxVk?width=594&amp;amp;height=660&amp;amp;cropmode=none" alt="T-shirt back"&gt;&lt;/p&gt;
&lt;p&gt;How cool is that?! Not too many open source projects provide such swag!&lt;/p&gt;
&lt;p&gt;Thanks to the &lt;a href="https://github.com/Microsoft/UWPCommunityToolkit"&gt;UWP Community Toolkit&lt;/a&gt; maintainers, keep up the good work!&lt;/p&gt;



                                </content:encoded>
		</item>
		<item>
			<title>On The Importance of Doing Something</title>
			<link>http://ian.bebbs.co.uk/posts/OnTheImportanceOfDoingSomething</link>
			<description>&lt;p&gt;I've just become a father. It's amazing and I'm loving every day, from waking up in the morning and being greeted by huge grin from my little girl to putting her to bed in the evening when she can barely keep her eyes open. My baby instantaneously became priority number one and has left little time for other passions like home programming projects.&lt;/p&gt;</description>
			<guid>http://ian.bebbs.co.uk/posts/OnTheImportanceOfDoingSomething</guid>
			<pubDate>Mon, 16 Nov 2015 00:00:00 GMT</pubDate>
			<content:encoded>
                                        


&lt;p&gt;I've just become a father. It's amazing and I'm loving every day, from waking up in the morning and being greeted by huge grin from my little girl to putting her to bed in the evening when she can barely keep her eyes open. My baby instantaneously became priority number one and has left little time for other passions like home programming projects.&lt;/p&gt;
&lt;p&gt;Yet, simultaneously, I have found the time since my baby was born to be some of the most productive time of my life. Since she her birth, I've spent every hour possible with her, worked regular hours - and been productive during those hours - at my day job, got an unexpectedly healthy amount of sleep and still managed to complete a significant amount of work on personal projects as well as creating and writing this blog.&lt;/p&gt;
&lt;p&gt;While reflecting on why this might be, I came up with one simple, inescapable conclusion: &lt;a href="https://github.com/ibebbs" title="ibebbs Github Profile"&gt;my Github profile&lt;/a&gt;. More specifically the "Current Streak" of contributions. You see, I am a sucker for gamification and seeing that "Current Streak" increase each day is remarkably rewarding. As such, I am finding time each day - no matter how little - to do &lt;em&gt;something&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Previously, I'd get home in the evening, think about the projects I had to work on, realise that there probably wouldn't be enough time to make significant progress and procrastinate about it until I had more time available to "get things done properly" - which was happening increasingly rarely. Now, instead of focusing on some distant goal, I simply think about finding &lt;em&gt;something&lt;/em&gt; I can do and publish to Github in whatever time I have available. It really doesn't matter what it is, as long as I get the &lt;a href="https://www.microsoft.com/surface/en-gb/devices/surface-pro-4" title="Microsoft Surface Pro 4"&gt;Surface&lt;/a&gt; out and start. Almost always, once I've started something, I get engrossed in it and am able to spend way more time than I thought might be available on it.&lt;/p&gt;
&lt;p&gt;For example, I'm writing this post from the sofa in my front room having spent a wonderful evening with my other half and little girl. Not having had time to make a commit today, I decided not to get an early night and instead tucked them into their respective beds before heading back down stairs to get a start on this post. Initially I had planned simply jot down a few ideas and possibly write a sentence or two before committing to Github and retiring for the evening. Yet here I am still writing after several paragraphs (and &lt;a href="https://music.microsoft.com/album/london-grammar/if-you-wait-deluxe-edition/bz.EE99E107-0100-11DB-89CA-0019B92A3933?action=play" title="London Grammar on Groove Music"&gt;one very enjoyable album&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;I probably won't finish tonight, and definitely won't publish this post until I've had time to re-read and edit it. But I've got my commit for the day and made an unexpected amount of progress on a blog post about making an unexpected amount of progress. In short, I got &lt;em&gt;something&lt;/em&gt; done and that feels good.&lt;/p&gt;
&lt;p&gt;Night!&lt;/p&gt;



                                </content:encoded>
		</item>
		<item>
			<title>On the perils of traversing parallel universes</title>
			<link>http://ian.bebbs.co.uk/posts/RxVsTpl</link>
			<description>&lt;p&gt;Despite taking some poetic license with the title of this post, the dramatics are not without merit. I have spent a significant portion of the last three days trying to write a series of tests around some asynchronous code to prove it managed - or, more specifically, limited - concurrency as intended. This code, while mainly Rx based, made calls to TPL methods and needed to wait, without blocking, for a result to be returned prior to allowing subsequent calls to be made. In short it mixed Rx and TPL to implement a multi-procuder / single consumer concurrency pattern, and this mix proved to be the source of much (much, much!) frustration.&lt;/p&gt;</description>
			<guid>http://ian.bebbs.co.uk/posts/RxVsTpl</guid>
			<pubDate>Mon, 01 Feb 2016 00:00:00 GMT</pubDate>
			<content:encoded>
                                        


&lt;p&gt;Despite taking some poetic license with the title of this post, the dramatics are not without merit. I have spent a significant portion of the last three days trying to write a series of tests around some asynchronous code to prove it managed - or, more specifically, limited - concurrency as intended. This code, while mainly Rx based, made calls to TPL methods and needed to wait, without blocking, for a result to be returned prior to allowing subsequent calls to be made. In short it mixed Rx and TPL to implement a multi-procuder / single consumer concurrency pattern, and this mix proved to be the source of much (much, much!) frustration.&lt;/p&gt;
&lt;p&gt;While I don't necessarily believe that &lt;a href="https://code.google.com/archive/p/fakeiteasy/issues/31"&gt;threading should be avoided in unit tests&lt;/a&gt; I do, as much as possible, endeavour to keep tests synchronous. Obviously this can be tricky when you're &lt;a href="http://stackoverflow.com/questions/20861305/should-i-unit-test-concurrency"&gt;specifically&lt;/a&gt; &lt;a href="http://stackoverflow.com/questions/12159/how-should-i-unit-test-threaded-code"&gt;testing&lt;/a&gt;  &lt;a href="http://stackoverflow.com/questions/1226779/how-to-run-concurrency-unit-test"&gt;concurrency&lt;/a&gt; but, fortunately, this has become immeasurably easier since Rx introduced testing in &lt;a href="http://blogs.msdn.com/b/rxteam/archive/2012/06/14/testing-rx-queries-using-virtual-time-scheduling.aspx"&gt;virtual time&lt;/a&gt; via the &lt;a href="http://www.introtorx.com/content/v1.0.10621.0/16_TestingRx.html"&gt;TestScheduler&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;So, with this magical mechanism for manipulating the motion of time in my mitts, I proceeded to write the test &lt;code&gt;ShouldOnlySendASingleCommandAtATime&lt;/code&gt;... which promptly failed. And I'm not talking the good, red-green kinda failure. Nooo, this was an old school &lt;em&gt;"I've written the code, better make sure it works... oh, that's weird!"&lt;/em&gt; kinda failure.&lt;/p&gt;
&lt;p&gt;Assertions on calls to faked objects were failing and, after a lot of digging I finally found out why: Despite diligently injecting and using a &lt;code&gt;TestScheduler&lt;/code&gt; through my Rx code and mocking calls to TPL code such that they returned &lt;code&gt;TaskCompletionSource&amp;lt;T&amp;gt;&lt;/code&gt; instances, I was still seeing my unit tests start a &lt;strong&gt;second worker thread&lt;/strong&gt;!&lt;/p&gt;
&lt;p&gt;Weird indeed.&lt;/p&gt;
&lt;p&gt;After a lot of hacking around with my code and other &lt;a href="http://www.theallium.com/engineering/computer-programming-to-be-officially-renamed-googling-stackoverflow/"&gt;"computer programming"&lt;/a&gt; type activities, I finally happened upon this &lt;a href="http://stackoverflow.com/questions/28183473/executing-tpl-code-in-a-reactive-pipeline-and-controlling-execution-via-test-sch/28236216#28236216"&gt;curiously titled question&lt;/a&gt;. The question very closely reflected what I was trying to achieve and was fortunately followed by an incredibly detailed answer by &lt;a href="http://stackoverflow.com/users/87427/james-world"&gt;James World&lt;/a&gt;. Of particular note was this paragraph:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;One particular pain point of Rx that leaves many testers scratching their heads, is the fact that the TPL -&amp;gt; Rx family of conversions introduce concurrency. e.g. ToObservable, SelectMany's overload accepting Task&lt;t&gt; etc. don't provide overloads with a scheduler and insidiously force you off the TestScheduler thread, even if mocking with TCS&lt;/t&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Bingo! Exactly the issue I was experiencing. James also provided a link to the following &lt;a href="https://github.com/Reactive-Extensions/Rx.NET/issues/21"&gt;bug report for Rx&lt;/a&gt; validating his answer and vindicating my confusion. This epiphany lead me to rewrite the interfaces to my dependencies such that they were Rx rather than TPL based. After which, lo and behold, my unit tests started passing.&lt;/p&gt;
&lt;p&gt;Phew!&lt;/p&gt;
&lt;p&gt;In conclusion, I guess the physicists are right: &lt;a href="https://www.newscientist.com/article/dn11745-could-black-holes-be-portals-to-other-universes/"&gt;If you want to move between parallel universes, be prepared to fall into a black hole!&lt;/a&gt;&lt;/p&gt;



                                </content:encoded>
		</item>
		<item>
			<title>WebRx and Typescript</title>
			<link>http://ian.bebbs.co.uk/posts/RxWebWithTypescript</link>
			<description>&lt;p&gt;In &lt;a href="./posts/RxWeb"&gt;part 1&lt;/a&gt; of this series I showed how to set up a project structure that allows you to start using WebRx from within Visual Studio. While fairly simple, the example provides a great illustration of you how WebRx allows you to separate your view and view model.&lt;/p&gt;</description>
			<guid>http://ian.bebbs.co.uk/posts/RxWebWithTypescript</guid>
			<pubDate>Tue, 08 Mar 2016 00:00:00 GMT</pubDate>
			<content:encoded>
                                        


&lt;h1&gt;Continuing with WebRx&lt;/h1&gt;
&lt;p&gt;In &lt;a href="./posts/RxWeb"&gt;part 1&lt;/a&gt; of this series I showed how to set up a project structure that allows you to start using WebRx from within Visual Studio. While fairly simple, the example provides a great illustration of you how WebRx allows you to separate your view and view model.&lt;/p&gt;
&lt;p&gt;In this article I further develop the structure to allow you to develop your application using Typescript.&lt;/p&gt;
&lt;h1&gt;From 'app.js' to 'app.ts'&lt;/h1&gt;
&lt;p&gt;Previously we copied a chunk of JavaScript from the WebRx &lt;a href="http://webrxjs.org/docs/getting-started.html"&gt;getting started guide&lt;/a&gt; into an &lt;code&gt;app.js&lt;/code&gt; script that was directly used from within the &lt;code&gt;index.html&lt;/code&gt; file. We now want to &lt;a href="https://en.wikipedia.org/wiki/Source-to-source_compiler"&gt;&lt;em&gt;transpile&lt;/em&gt;&lt;/a&gt; the &lt;code&gt;app.js&lt;/code&gt; script from a Typescript file so that we can further develop the application in a structured and type-safe manner.&lt;/p&gt;
&lt;p&gt;To do this simply follow the following steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Delete the existing &lt;code&gt;app.js&lt;/code&gt; file leaving the &lt;code&gt;js&lt;/code&gt; folder empty.&lt;/li&gt;
&lt;li&gt;Add and configure a &lt;code&gt;TypeScript JSON Configuration File&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Add a &lt;code&gt;TypeScript JSON Configuration File&lt;/code&gt; to the solution as shown below&lt;br&gt;
&lt;img src="/Content/RxWebWithTypescript/AddTypeScriptJsonConfigurationFile.png" class="img-responsive" style="margin: auto; width:600px; margin-top: 6px; margin-bottom: 6px;" alt="Add TypeScript Json Configuration File"&gt;&lt;/li&gt;
&lt;li&gt;Replace the &lt;code&gt;node_modules&lt;/code&gt; exclusion with &lt;code&gt;Scripts&lt;/code&gt;&lt;br&gt;
By default Visual Studio (or, more acurately, the TypeScript transpiler) with pick up all &lt;code&gt;ts&lt;/code&gt; files in the solution. As we don't want to attempt to re-transpile all the referenced typescript files we add &lt;code&gt;Scripts&lt;/code&gt; to the exclusion list. Further, as we added a reference to &lt;code&gt;WebRx&lt;/code&gt; via Nuget, our references are in the &lt;code&gt;Scripts&lt;/code&gt; folder, not &lt;code&gt;node_modules&lt;/code&gt;, so this exclusion can be removed.&lt;/li&gt;
&lt;li&gt;Add an &lt;code&gt;outDir&lt;/code&gt; setting to transpile to the &lt;code&gt;js&lt;/code&gt; folder&lt;br&gt;
This setting will force the TypeScript transpiler to output the transpiled JavaScript files to the &lt;code&gt;js&lt;/code&gt; folder where they can be used by the client browser.&lt;/li&gt;
&lt;li&gt;You should now have a &lt;code&gt;tsconfig.json&lt;/code&gt; file that looks like this:
&lt;pre class="prettyprint"&gt;&lt;code&gt;{
  "compilerOptions": {
    "noImplicitAny": false,
    "noEmitOnError": true,
    "removeComments": false,
    "sourceMap": true,
    "target": "es5",
    "outDir": "js"
  },
  "exclude": [
    "Scripts",
    "wwwroot"
  ]
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="3"&gt;
&lt;li&gt;Add a &lt;code&gt;ts&lt;/code&gt; folder to the solution.&lt;/li&gt;
&lt;li&gt;Add an &lt;code&gt;app.ts&lt;/code&gt; typescript file to the &lt;code&gt;ts&lt;/code&gt; folder.&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Add references to Rx and WebRx to the &lt;code&gt;app.ts&lt;/code&gt; file.&lt;br&gt;
WebRx requires that you add an explicit reference to &lt;code&gt;rx.all.d.ts&lt;/code&gt; prior to the reference to &lt;code&gt;web.rx.d.ts&lt;/code&gt; in order for the Rx module to be brought into scope. The references should therefore be added like this:
&lt;pre class="prettyprint"&gt;&lt;code&gt;/// &amp;lt;reference path="../Scripts/rx.all.d.ts"/&amp;gt;
/// &amp;lt;reference path="../Scripts/typings/web.rx.d.ts" /&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;Implement view / view model code&lt;br&gt;
You can now re-implement the code from &lt;code&gt;app.js&lt;/code&gt; as TypeScript virtually verbatim but do note how you get Intellisense for all the methods and properties of &lt;code&gt;wx&lt;/code&gt; module.&lt;/li&gt;
&lt;li&gt;Fix compilation error with call to &lt;code&gt;wx.applyBindings&lt;/code&gt;&lt;br&gt;
The &lt;code&gt;wx.applyBindings&lt;/code&gt; method &lt;em&gt;requires&lt;/em&gt; a &lt;code&gt;model&lt;/code&gt; parameter which, in JavaScript, is defaulted but in TypeScript causes a compilation error. To resolve this, simply pass an empty object to the method.&lt;/li&gt;
&lt;li&gt;Your &lt;code&gt;app.ts&lt;/code&gt; file should now look like this:
&lt;pre class="prettyprint"&gt;&lt;code&gt;/// &amp;lt;reference path="../Scripts/rx.all.d.ts"/&amp;gt;
/// &amp;lt;reference path="../Scripts/typings/web.rx.d.ts" /&amp;gt;

wx.app.component('hello', {
    viewModel: function () {
        this.firstName = 'Bart';
        this.lastName = 'Simpson';
    },
    template: 'The name is &amp;lt;span data-bind="text: firstName + \' \' + lastName"&amp;gt;&amp;lt;/span&amp;gt;'
});

wx.router.state({
    name: "$",
    views: { 'main': "hello" }
});

wx.router.reload();

wx.applyBindings({});
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start="5"&gt;
&lt;li&gt;Compile the project and include the generated &lt;code&gt;js/app.js&lt;/code&gt; and &lt;code&gt;js/app.js.map&lt;/code&gt; files into the project.&lt;/li&gt;
&lt;li&gt;Hit F5 and you should again see the message 'The name is Bart Simpson' displayed in your default browser.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Congratulations, you're now ready to develop your application using full Intellisense and in the comfort of the knowledge that the compiler (well, transpiler) will pick up any syntactic bugs you may inadvertently create.&lt;/p&gt;
&lt;p&gt;As always, the completed &lt;a href="https://github.com/ibebbs/BlogProjects/tree/master/WebRxWithTypeScript"&gt;source code for this post&lt;/a&gt; can be found in the &lt;a href="https://github.com/ibebbs/BlogProjects"&gt;BlogProjects repository&lt;/a&gt; on &lt;a href="https://github.com/ibebbs"&gt;Github&lt;/a&gt;&lt;/p&gt;



                                </content:encoded>
		</item>
		<item>
			<title>Home Network Monitoring - Part III</title>
			<link>http://ian.bebbs.co.uk/posts/HomeNetworkMonitoring-PartIII</link>
			<description>&lt;p&gt;In the last post, I configured Logstash to extracted source and destination address information from the "Client Access Log" Syslog messages sent by my router and added a number of visualizations to my Kibana dashboard which allow me to explore which local devices are access which remote servers.&lt;/p&gt;</description>
			<guid>http://ian.bebbs.co.uk/posts/HomeNetworkMonitoring-PartIII</guid>
			<pubDate>Tue, 12 Apr 2016 00:00:00 GMT</pubDate>
			<content:encoded>
                                        


&lt;p&gt;In the last post, I configured Logstash to extracted source and destination address information from the "Client Access Log" Syslog messages sent by my router and added a number of visualizations to my Kibana dashboard which allow me to explore which local devices are access which remote servers.&lt;/p&gt;
&lt;p&gt;While this is already very useful, it's almost impossible to remember which devices relate to which IP addresses on the local network, let alone the on the internet. As such, I really want the ability to translate the IP addresses (and, ideally, port numbers) into device, host or protocol names.&lt;/p&gt;
&lt;h1&gt;translating remote ip addresses to host names&lt;/h1&gt;
&lt;p&gt;When a local device accesses a remote server it will, ordinarily, do so by resolving an IP address for a host name, for example 'google.com' resolves to the address '216.58.213.110'. On my network, my router acts as a DNS server, resolving names it knows and forwarding unresolved names to Googles DNS servers. A the results of the host name to IP address lookup are cached in the DNS server (e.g. my router) I can perform a &lt;a href="https://en.wikipedia.org/wiki/Reverse_DNS_lookup"&gt;reverse DNS lookup&lt;/a&gt; at very little processing cost and without consuming any WAN bandwidth.&lt;/p&gt;
&lt;p&gt;As usual, Logstash comes with a filter that is able to perform this operation called, unsurprisingly, 'dns'. However, to provide a consistent set of fields to Kibana, it requires a couple of additional steps to ensure it functions consistently. Here is the amended &lt;code&gt;syslog.config&lt;/code&gt; with the reverse DNS lookup in place.&lt;/p&gt;
&lt;pre class="prettyprint"&gt;&lt;code&gt;input {
  tcp {
    port =&amp;gt; 5000
    type =&amp;gt; syslog
  }
  udp {
    port =&amp;gt; 5000
    type =&amp;gt; syslog
  }
}

filter {
    grok {
        match =&amp;gt; [ "message", "&amp;lt;%{POSINT:syslog_pri}&amp;gt;%{SYSLOGTIMESTAMP:syslog_timestamp} Vigor\: Local User \(MAC=%{MAC:source_mac}\): %{IP:source_address}(?::%{POSINT:source_port})? -&amp;gt; %{IP:destination_address}(?::%{POSINT:destination_port})? \((?&amp;lt;protocol&amp;gt;TCP|UDP)\)" ]
        add_tag =&amp;gt; "access"
    }
    if "access" in [tags] {
        mutate {
            add_field =&amp;gt; {
              "destination_host" =&amp;gt; "%{[destination_address]}"
            }
        }
        dns {
            reverse =&amp;gt; [ "destination_host" ]
            action =&amp;gt; "replace"
            nameserver =&amp;gt; "192.168.1.1"
        }
    }
}

output {
  elasticsearch {
    hosts =&amp;gt; ["192.168.1.30:9200"]
    index =&amp;gt; "syslog-%{+YYYY.MM.dd}"
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that, when an 'access' message is successfully grokked, I add a tag to the tags array field of the message called "access". If another type of message has been received (i.e. a DNS lookup) then the grok pattern won't match and the 'access' tag will not be added to tags.&lt;/p&gt;
&lt;p&gt;After the &lt;code&gt;grok&lt;/code&gt; filter, I check to see if the tags field contains the 'access' tag and, if so, use the &lt;code&gt;mutate&lt;/code&gt; filter to copy the 'destination_address' field value into a 'destination_host' field. This is done as the &lt;code&gt;dns&lt;/code&gt; filter will replace the field value if a successful reverse DNS lookup is performed but will leave the original value (i.e. the IP address) if a reverse DNS could not be performed. This way we either get the host name or IP address in the 'destination_host' field and it's never empty.&lt;/p&gt;
&lt;p&gt;With the changes to configuration in place, I restart Logstash. Then, in Kibana, I refresh the field list for the 'syslog-*' index, add 'destination_host' to the 'Syslog Messages' saved search, load 'Access By Destination Address' visualization and modify it to use 'destination_host' rather than the 'destination_address' field; and get the following:&lt;/p&gt;
&lt;img src="/Content/HomeNetworkMonitoring/Kibana-AccessByDestinationHostAnalysedVisualization.png" class="img-responsive" style="margin: auto; width:600px; margin-top: 6px; margin-bottom: 6px;" alt="Kibana Access By Destination Host Analysed Visualization"&gt;
&lt;p&gt;While initially it looks promising, a quick look at the list of hosts being accessed shows something peculiar: the domain names have been split into their component parts.&lt;/p&gt;
&lt;h1&gt;analysis, mappings and templates&lt;/h1&gt;
&lt;p&gt;The reason for the host names being split is because, by default, ElasticSearch performs 'analysis' on text strings. This analysis involves splitting the strings into discrete words which can be indexed more efficiently. Some strings however, for example domain names, should be treated as a single word and as such we need to prevent ElasticSearch from performing the analysis.&lt;/p&gt;
&lt;p&gt;How ElasticSearch treats various fields within a message can be controlled by modifying the index mapping. The current mapping for the 'syslog' index can be retrieved by a REST call to the address 'http://[ElasticSearchHost]:9200/syslog-2016.04.12/_mapping'. This returns:&lt;/p&gt;
&lt;pre class="prettyprint"&gt;&lt;code&gt;{
    "syslog-2016.04.12": {
        "mappings": {
            "syslog": {
                "properties": {
                    "@@timestamp": {
                        "type": "date",
                        "format": "strict_date_optional_time||epoch_millis"
                    },
                    "@@version": {
                        "type": "string"
                    },
                    "destination_address": {
                        "type": "string"
                    },
                    "destination_host": {
                        "type": "string"
                    },
                    "destination_port": {
                        "type": "string"
                    },
                    "host": {
                        "type": "string"
                    },
                    "message": {
                        "type": "string"
                    },
                    "protocol": {
                        "type": "string"
                    },
                    "source_address": {
                        "type": "string"
                    },
                    "source_mac": {
                        "type": "string"
                    },
                    "source_port": {
                        "type": "string"
                    },
                    "syslog_pri": {
                        "type": "string"
                    },
                    "syslog_timestamp": {
                        "type": "string"
                    },
                    "tags": {
                        "type": "string"
                    },
                    "type": {
                        "type": "string"
                    }
                }
            }
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In order to prevent ElasticSearch from analysing the 'destination_host' field, we need to add an 'index' key with the value 'not_analyzed'. Even though things have been mostly working correctly so far, I can save quite a bit of storage and processing time by marking almost all of the string fields as 'not_analyzed'. This is shown below:&lt;/p&gt;
&lt;pre class="prettyprint"&gt;&lt;code&gt;{
    "syslog-2016.04.12": {
        "mappings": {
            "syslog": {
                "properties": {
                    "@@timestamp": {
                        "type": "date",
                        "format": "strict_date_optional_time||epoch_millis"
                    },
                    "@@version": {
                        "type": "string"
                    },
                    "destination_address": {
                        "type": "string",
                        "index" : "not_analyzed"
                    },
                    "destination_host": {
                        "type": "string",
                        "index" : "not_analyzed"
                    },
                    "destination_port": {
                        "type": "integer"
                    },
                    "host": {
                        "type": "string",
                        "index" : "not_analyzed"
                    },
                    "message": {
                        "type": "string"
                    },
                    "protocol": {
                        "type": "string",
                        "index" : "not_analyzed"
                    },
                    "source_address": {
                        "type": "string",
                        "index" : "not_analyzed"
                    },
                    "source_mac": {
                        "type": "string",
                        "index" : "not_analyzed"
                    },
                    "source_port": {
                        "type": "integer"
                    },
                    "syslog_pri": {
                        "type": "integer"
                    },
                    "syslog_timestamp": {
                        "type": "string"
                    },
                    "tags": {
                        "type": "string"
                    },
                    "type": {
                        "type": "string"
                    }
                }
            }
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, while I could write this mapping directly to the ElasticSearch index, as the index is date-based, I'd have to resend the mapping manually everyday. Instead, I am going to create a mapping template that will match an index name based on pattern and automatically apply the template. This is done by crafting a PUT call to the ElasticSearch '_template' endpoint with the specific template name. In short, the following mapping template is posted to &lt;code&gt;http://[ElasticSearch:9200]/_templates/syslog_template&lt;/code&gt;&lt;/p&gt;
&lt;pre class="prettyprint"&gt;&lt;code&gt;{
        "template": "syslog-*",
        "mappings": {
            "syslog": {
                "properties": {
                    "@@timestamp": {
                        "type": "date",
                        "format": "strict_date_optional_time||epoch_millis"
                    },
                    "@@version": {
                        "type": "string"
                    },
                    "destination_address": {
                        "type": "string",
                        "index" : "not_analyzed"
                    },
                    "destination_host": {
                        "type": "string",
                        "index" : "not_analyzed"
                    },
                    "destination_port": {
                        "type": "integer"
                    },
                    "host": {
                        "type": "string",
                        "index" : "not_analyzed"
                    },
                    "message": {
                        "type": "string"
                    },
                    "protocol": {
                        "type": "string",
                        "index" : "not_analyzed"
                    },
                    "source_address": {
                        "type": "string",
                        "index" : "not_analyzed"
                    },
                    "source_mac": {
                        "type": "string",
                        "index" : "not_analyzed"
                    },
                    "source_port": {
                        "type": "integer"
                    },
                    "syslog_pri": {
                        "type": "integer"
                    },
                    "syslog_timestamp": {
                        "type": "string"
                    },
                    "tags": {
                        "type": "string"
                    },
                    "type": {
                        "type": "string"
                    }
                }
            }
        }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With this in place I need to delete todays syslog index so that it is recreated, using the mapping above, when the first syslog message is received. Once this has been done, the visualization looks like this:&lt;/p&gt;
&lt;img src="/Content/HomeNetworkMonitoring/Kibana-AccessByDestinationHostNotAnalysedVisualization.png" class="img-responsive" style="margin: auto; width:600px; margin-top: 6px; margin-bottom: 6px;" alt="Kibana Access By Destination Host Not-Analysed Visualization"&gt;
&lt;p&gt;Nice!&lt;/p&gt;
&lt;h1&gt;summary&lt;/h1&gt;
&lt;p&gt;In this post, I showed how to display host names for accessed servers rather than IP addresses. I also covered how to update ElasticSearch mapping such that field 'analysis' can be prevented and host names kept together.&lt;/p&gt;
&lt;p&gt;In the next post, I'll show how to translate local device IP addresses in to device names.&lt;/p&gt;



                                </content:encoded>
		</item>
		<item>
			<title>ToddlerBox Tops 10,000 Users!!</title>
			<link>http://ian.bebbs.co.uk/posts/ToddlerBoxTopsTenThousandUsers</link>
			<description>&lt;p&gt;Really that got out of hand fast! I had no idea so many people would be interested in letting their toddler loose on their XBox controller. In fact, there have been many things that have surprised me about this app:&lt;/p&gt;</description>
			<enclosure url="http://ian.bebbs.co.uk/y3mVAxtKFjUEGk7Hdhzjl4UZ2INnFdBbw9K-tnYZ8DYoJ-VoKxpAN6w8Ng0DFTYdSxpHY6IvL5-VwJpLkQl6qWRmMQXSExLXopz5CFuSxIbyaMLrnL2Vy3yPZlISAAknXZdT4HwiZJ55zg2UtKwucBL88-xHh6rn5Mh97yzfRsCPjI%253Fwidth=1024&amp;height=574&amp;cropmode=none" length="0" type="image" />
			<guid>http://ian.bebbs.co.uk/posts/ToddlerBoxTopsTenThousandUsers</guid>
			<pubDate>Tue, 20 Dec 2016 00:00:00 GMT</pubDate>
			<content:encoded>
                                        


&lt;p&gt;Really that got out of hand fast! I had no idea so many people would be interested in letting their toddler loose on their XBox controller. In fact, there have been many things that have surprised me about this app:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Acquisition rate has increased&lt;/p&gt;
&lt;p&gt;I kind of expected an initial burst of acquisitions then a slow tail off but this hasn't happened. As it's only been a little over a month since ToddlerBox was released to the store, I guess there's every possibility that it's still in the "burst" stage and theres just more interest in apps of this type. Well, here's hoping.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Reviews have been extremely polarized&lt;/p&gt;
&lt;p&gt;Almost without exception, reviews have been 4-5 stars or 1 star. Also, while a couple of the 1 star ratings are to do with app functionality (it seems a couple of people have issues running even this basic UWP app) the overwhelming majority of 1 star ratings are due to peoples dislike of ads; because...&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Ads are a very risky business&lt;/p&gt;
&lt;p&gt;As with most parents, I am extremely cautious about what my toddler is exposed to. Therefore I was extremely cautious about adding advertising as a means of revenue to ToddlerBox, regardless of how small or out of the way they were.
After completing the Ad-Mediation questions in the Windows Store and being assured that ads would be both age rated and not "tracking", I felt a lot better about the idea and decided a small banner ad on the instructions screen would be pretty harmless. Furthermore, throughout the process of adding the banner, I didn't see a single advert I would be concerned about putting in front of my child; most were simple flashing inbox icons or the like.
However, after discussing ToddlerBox with a friend, he decided to install it on his Xbox and I was very upset to see that the ads being displayed were both more intrusive and more "click-baity" than any I had seen previously. I am now investigating ways to generate some revenue from the app but without upsetting parents.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The reviews shown in the store are not "all reviews"&lt;/p&gt;
&lt;p&gt;As the publisher of ToddlerBox, I get to see all reviews left about the app on the store. Due to all the previous points, the app is currently averaging about a 2.6 star rating across 50 odd reviews. However, when viewing the app in the Store on my friend's XBox, it was shown as having a 4.5 star rating from just 2 reviews. Now, while some of the reviews I can see are translated from foreign languages, most are in English and I'm therefore at a loss to explain why they're not being displayed when an XBox in the English local browses the store.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;It's not just toddlers using ToddlerBox&lt;/p&gt;
&lt;p&gt;I've had more than one review stating how good this game is to play after consuming various illicit substances ;0P&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Anyway, as described above, while acquisition rate has fluctuated on a daily basis, the overall trend in acquisition shows unexpected growth:&lt;/p&gt;
&lt;img src="https://bnljqq-dm2306.files.1drv.com/y3msqT1msmkJ_32Jewx8Ysuoys0lC35GCLY-aQPyJ9YhR75KMswOLcznSPGOOBGhhsMRb6qK3_xh_qr3h1Xh-vFwmSog1_HoOS-Isj1lMoFPpuV42oAwJi3A5JiY-ToAirVoZDnz2gQ7N6jUed82zW83IZ_qQcbqWGw1ekaNh4wjBU?width=660&amp;amp;height=252&amp;amp;cropmode=none" class="img-responsive" style="margin: auto; width:660px; margin-top: 6px; margin-bottom: 6px;" alt="Acquisitions over App Lifetime"&gt;
&lt;p&gt;Daily usage also shows that a healthy number of acquisitions are being used regularly, with nearly 800 people using ToddlerBox nearly 1700 times just yesterday (19th December):&lt;/p&gt;
&lt;img src="https://bnlkqq-dm2306.files.1drv.com/y3mBC-i0nboWq0ibRyIcBhs1O2MnfOUx-OxTQZDUNHbh-eVRbfH0xP9-p9hLJqC82i1SBB5ZfpcfmTlvuZwMBD-Gt0BZZC-IvVCJOd1jzexDoiC-BV4LG7X2aOyOapkCKK6wM6WWffluiuD5MKKet6MfEIjTWNZMCpFx1FP13suiCY?width=660&amp;amp;height=249&amp;amp;cropmode=none" class="img-responsive" style="margin: auto; width:660px; margin-top: 6px; margin-bottom: 6px;" alt="Daily usage"&gt;
&lt;p&gt;So, the Xmas break is almost upon us and I will be returning my attention to ToddlerBox (yes, despite the career break, I try to spend most of my time working on 'serious' projects or study). I have a number of new features in mind for it (including the #1 requested feature of "sound!!") and will be looking to try out a couple of more features of the awesome Win2D library. It's going to be a lot of fun (for me and my little girl) and will hopefully allow ToddlerBox to reach the next major (although admittedly arbitrary) milestone of 25,000 acquisitions!&lt;/p&gt;
&lt;p&gt;Watch this space...&lt;/p&gt;



                                </content:encoded>
		</item>
		<item>
			<title>A sentiment(al) analysis of why Red Dwarf is no longer funny</title>
			<link>http://ian.bebbs.co.uk/posts/ASentimentalAnalysisOfRedDwarf</link>
			<description>&lt;p&gt;Yesterday I had a lot of fun playing with &lt;a href="http://jupyter.org/"&gt;Project Jupyter&lt;/a&gt;. For those that aren't aware of this project, it's an effort to provide a workspace for performing repeatable experimentation with data. In short it mixes markdown editing capabilities with a &lt;a href="https://en.wikipedia.org/wiki/Read%E2%80%93eval%E2%80%93print_loop"&gt;REPL&lt;/a&gt; environment for a large number of languages. From the website:&lt;/p&gt;</description>
			<guid>http://ian.bebbs.co.uk/posts/ASentimentalAnalysisOfRedDwarf</guid>
			<pubDate>Tue, 31 Jan 2017 00:00:00 GMT</pubDate>
			<content:encoded>
                                        


&lt;p&gt;Yesterday I had a lot of fun playing with &lt;a href="http://jupyter.org/"&gt;Project Jupyter&lt;/a&gt;. For those that aren't aware of this project, it's an effort to provide a workspace for performing repeatable experimentation with data. In short it mixes markdown editing capabilities with a &lt;a href="https://en.wikipedia.org/wiki/Read%E2%80%93eval%E2%80%93print_loop"&gt;REPL&lt;/a&gt; environment for a large number of languages. From the website:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;"a web application that allows you to create and share documents that contain live code, equations, visualizations and explanatory text. Uses include: data cleaning and transformation, numerical simulation, statistical modeling, machine learning and much more"&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;After having my interest tweaked by the &lt;a href="https://try.jupyter.org/"&gt;browser version&lt;/a&gt; I bit the bullet and spent ages downloading and installing the &lt;a href="http://jupyter.org/install.html"&gt;full version&lt;/a&gt; to a virtual machine. It's a Python based web-app so requires quite a bit of setup and unfortunately I found the documentation to be a bit sparse.&lt;/p&gt;
&lt;p&gt;And so it was that while trying to work out how to install the &lt;a href="https://github.com/fsprojects/IfSharp"&gt;FSharp module&lt;/a&gt; I came across &lt;a href="https://notebooks.azure.com/"&gt;Azure Notebooks&lt;/a&gt;. This is a free, Azure hosted version of Jupyter that has almost all the features of a local installation but with none of the faff. After quickly spinning up a new notebook here I didn't even look back at the local installation.&lt;/p&gt;
&lt;h2&gt;A Jupyter [Data] Mining Core Project&lt;/h2&gt;
&lt;p&gt;As per the title and lead of this post, I decided to use Jupyter to have a little fun.&lt;/p&gt;
&lt;p&gt;Back in September, while grinding my way through &lt;a href="http://www.imdb.com/title/tt0094535/episodes?season=11&amp;amp;ref_=tt_eps_sn_11"&gt;season 11 of Red Dwarf&lt;/a&gt;, I began to wonder why it wasn't as funny as it used to be. Had the writing deteriorated? Were the actors past it? Or were these elements still as great as they used to be and it was me who had changed?&lt;/p&gt;
&lt;p&gt;I started thinking about ways this could be investigated such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Using IMDB rating as a measure of humour in each episode of Red Dwarf&lt;/li&gt;
&lt;li&gt;Performing semantic analysis of episode's transcript to see if the sentiment had changed&lt;/li&gt;
&lt;li&gt;Using word-count to determine whether there was a correlation between character participation and overall humour&lt;/li&gt;
&lt;li&gt;etc&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Well, it was a funny notion and provided a pleasant distraction from the &lt;a href="http://www.imdb.com/title/tt5218266"&gt;pretty awful episode&lt;/a&gt; of Red Dwarf I was watching at the time. I added it to my "ideas" list in &lt;a href="https://trello.com/b/KoTWuFUi/public-board"&gt;Trello&lt;/a&gt;, finished the episode and went to bed.&lt;/p&gt;
&lt;p&gt;Yesterday, when I came across Project Jupyter, I knew it'd be a great medium for performing this investigation so shifted the analysis from "Ideas" to "In progress" and got cracking.&lt;/p&gt;
&lt;h2&gt;Data Science using F#&lt;/h2&gt;
&lt;p&gt;Now, while in relation to this investigation I use the term "data science" to basically mean "munging a few numbers and drawing a few graphs", I do think F# makes a fantastic language for the discipline in general. It has some incredible mechanisms for &lt;a href="https://docs.microsoft.com/en-us/dotnet/articles/fsharp/tutorials/type-providers/"&gt;acquiring&lt;/a&gt; and &lt;a href="http://fsharpforfunandprofit.com/posts/the-option-type/"&gt;cleaning&lt;/a&gt; data as well as for &lt;a href="http://www.quanttec.com/fparsec/"&gt;parsing natural language&lt;/a&gt;. Couple this with it's concise, functional, elegant language and the ability to leverage components from the entire breadth of .NET ecosystem and you have quite a significant offering.&lt;/p&gt;
&lt;h2&gt;Azure Notebooks&lt;/h2&gt;
&lt;p&gt;The Azure implementation of Project Jupyter is first class and, for now at least, totally free. Getting started is as simple as logging in with Microsoft credentials and then clicking 'Add notebook'. Being an MS implementation, I used Edge to edit the notebook and found the experience extremely robust, especially given it's a "Preview" program.&lt;/p&gt;
&lt;p&gt;In fact I experience just two issues while authoring my notebook:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Data Store - It's not currently possible to upload or store data within the Azure notebook library (despite having functionality to do this in the web-interface). Instead you need to host your data on one of a small number of whitelisted sites. Fortunately Github is one of these sites so this doesn't prove to be much of an issue.&lt;/li&gt;
&lt;li&gt;Packages - While Azure Notebooks provides access to a large number of packages "out-of-the-box" (i.e. FSharp.Data, XPlot.Plotly, etc) it can be tricky to add/use other packages. For example, I wanted to use the XPlot.GoogleCharts package (as it provided trendline capabilities) and ended up having to write a custom display printer for it to work (due to an &lt;a href="https://github.com/fsprojects/IfSharp/issues/118"&gt;open issue&lt;/a&gt; on Github).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Apart from these issues, authoring and scripting F# in an Azure Notebook was almost as fast as using &lt;a href="https://docs.microsoft.com/en-us/dotnet/articles/fsharp/tutorials/fsharp-interactive/"&gt;"F# Interactive"&lt;/a&gt;. It even provides Intellisense capabilities but, in practice, these are usually too slow to be of actual use.&lt;/p&gt;
&lt;h2&gt;Sharing Notebooks&lt;/h2&gt;
&lt;p&gt;From Azure Notebooks you're able to download your notebook as a native ".ipynb" file (in fact this is encouraged as MS reserves the right to remove unused notebooks after 60 days). You can then share this file to other people who have Jupyter installed or, preferably, commit it to a repository in Github which has excellent support for Jupyter Notebooks.&lt;/p&gt;
&lt;p&gt;You can find my notebook "A sentiment(al) analysis of why Red Dwarf is no longer funny (to me)" &lt;a href="https://github.com/ibebbs/RedDwarfAnalysis/blob/master/Investigation.ipynb"&gt;here&lt;/a&gt;. As you will see when you click the link, Github not only shows you the static parts of the notebook but actually tries to spin up a kernel and execute the code parts too. This is a "limited rendering only" so Github also provides a link to open the notebook in 'nbviewer' &lt;a href="http://nbviewer.jupyter.org/github/ibebbs/RedDwarfAnalysis/blob/2712285e1f9c69fc347bdfe6404792101eaea5f1/Investigation.ipynb"&gt;web-app&lt;/a&gt;. This link is shown below:&lt;/p&gt;
&lt;img src="https://mvpfyw-dm2306.files.1drv.com/y3mppldGfaYEhvWkV7mdUw26-lP3SOzlMTGFbf8slchIfjBL57IH-GrJev6ai_rISiHBKrom7Abg9YFjfhZ1ArOFT7a7mh4gJuGq-CErv1dun48GQC_BdhMV08fh6hbw400d9nHSEXJ0jA2nPBIrpOPNrOz0I3lVY1tu_L656ylQKg?width=660&amp;amp;height=283&amp;amp;cropmode=none" class="img-responsive" style="margin: auto; width:660; margin-top: 6px; margin-bottom: 6px;" alt="Open external view with nbviewer"&gt;
&lt;h2&gt;The analysis&lt;/h2&gt;
&lt;p&gt;I had timeboxed my investigation into Project Jupyter and therefore didn't get round to performing an actual sentiment anaylsis of the content of each episode. However I did manage to do the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Programmatically download episode information from several sources in JSON format and use &lt;a href="http://fsharp.github.io/FSharp.Data/library/JsonValue.html"&gt;JsonValue&lt;/a&gt; to dynamically query these sources&lt;/li&gt;
&lt;li&gt;Scrape demographically categorized rating information from IMDB and use &lt;a href="http://fsharp.github.io/FSharp.Data/reference/fsharp-data-htmldocument.html"&gt;HtmlDocument&lt;/a&gt; to parse the data into strong types&lt;/li&gt;
&lt;li&gt;Resolve issue with rendering XPlot.GoogleCharts charts within the notebook and use these charts to provide an interactive visualisation of the decline in rating of Red Dwarf across time and demographic categories.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This provided a fair stab at correlating episode rating with the overall decline in Red Dwarf's humourousness but is a long way short of any form of "data science". It was both enlightening and a lot of fun doing this small project and I will certainly consider Azure Notebooks as a valuable tool in my toolbox.&lt;/p&gt;
&lt;p&gt;Should I find the time, I would certainly like to return to this project and use &lt;a href="http://www.quanttec.com/fparsec/"&gt;FParsec&lt;/a&gt; and &lt;a href="https://azure.microsoft.com/en-gb/services/cognitive-services/text-analytics/"&gt;Azure Text Analytics&lt;/a&gt; to perform an actual sentiment analysis. Hopefully it'll overturn, or at least help justify, my somewhat disturbing conclusion!&lt;/p&gt;



                                </content:encoded>
		</item>
		<item>
			<title>UWP Community Toolkit v1.1 Released</title>
			<link>http://ian.bebbs.co.uk/posts/UWPCommunityToolkitv1_1</link>
			<description>&lt;p&gt;Shortly after my blog post about &lt;a href="http://ian.bebbs.co.uk/posts/UsingHyperlinkInMVVM"&gt;Using a Hyperlink in MVVM&lt;/a&gt; a group of developers at Microsoft collated and released the &lt;a href="https://blogs.windows.com/buildingapps/2016/08/17/introducing-the-uwp-community-toolkit/#pRVgJbZbTBMHPyGG.97"&gt;UWP Community Toolkit&lt;/a&gt;. They were actively asking for contributions and, given the self contained nature of the Hyperlink extension, it seemed like a natural fit for the toolkit so I decided to try contributing it via pull request.&lt;/p&gt;</description>
			<guid>http://ian.bebbs.co.uk/posts/UWPCommunityToolkitv1_1</guid>
			<pubDate>Mon, 10 Oct 2016 00:00:00 GMT</pubDate>
			<content:encoded>
                                        


&lt;p&gt;Shortly after my blog post about &lt;a href="http://ian.bebbs.co.uk/posts/UsingHyperlinkInMVVM"&gt;Using a Hyperlink in MVVM&lt;/a&gt; a group of developers at Microsoft collated and released the &lt;a href="https://blogs.windows.com/buildingapps/2016/08/17/introducing-the-uwp-community-toolkit/#pRVgJbZbTBMHPyGG.97"&gt;UWP Community Toolkit&lt;/a&gt;. They were actively asking for contributions and, given the self contained nature of the Hyperlink extension, it seemed like a natural fit for the toolkit so I decided to try contributing it via pull request.&lt;/p&gt;
&lt;p&gt;The toolkit's contribution guidelines and conventions closely matched my own coding style so I had to change very little of the code in my article but did have to add XML summary blocks to each public class and members. With that done I could add the class to the &lt;a href="https://github.com/ibebbs/UWPCommunityToolkit/tree/dev/Microsoft.Toolkit.Uwp/Helpers"&gt;Helpers folder on the dev branch of my fork of the toolkit&lt;/a&gt; and &lt;a href="https://github.com/Microsoft/UWPCommunityToolkit/pull/226"&gt;issue a pull request&lt;/a&gt;. I was also asked to produce some documentation and a sample for the class which was then pulled into the document repo.&lt;/p&gt;
&lt;p&gt;The rest, as they say, is history. &lt;a href="https://blogs.windows.com/buildingapps/2016/10/05/announcing-uwp-community-toolkit-1-1/#1QBL3lQjtLbY537i.97"&gt;Version 1.1 of the toolkit released&lt;/a&gt; with yours truly in the &lt;a href="https://github.com/Microsoft/UWPCommunityToolkit/releases/"&gt;list of contributors&lt;/a&gt;. It's a small contribution but I really think the toolkit is a promising project and absolutely intend to contribute further in the near future.&lt;/p&gt;
&lt;p&gt;In the mean time, well done to all the maintainers of, and contributors to, the toolkit on this important milestone.&lt;/p&gt;



                                </content:encoded>
		</item>
		<item>
			<title>A Rip Storing Time</title>
			<link>http://ian.bebbs.co.uk/posts/ARipStoringTime</link>
			<description>&lt;p&gt;From a youth of misspent money, I have a moderately large DVD collection. Some 600 movies in half a dozen DVD racks take a disproportionately large space in my dining room. This year, my partner and I  decided to host Xmas dinner for the family, meaning a dining table that could host a dozen was in order and... well, you can see where this is heading. The DVDs had to go.&lt;/p&gt;</description>
			<guid>http://ian.bebbs.co.uk/posts/ARipStoringTime</guid>
			<pubDate>Wed, 07 Dec 2016 00:00:00 GMT</pubDate>
			<content:encoded>
                                        


&lt;p&gt;From a youth of misspent money, I have a moderately large DVD collection. Some 600 movies in half a dozen DVD racks take a disproportionately large space in my dining room. This year, my partner and I  decided to host Xmas dinner for the family, meaning a dining table that could host a dozen was in order and... well, you can see where this is heading. The DVDs had to go.&lt;/p&gt;
&lt;p&gt;Now, even though I barely watch them anymore, there are still a couple of amazing films in my collection that aren't available from the (far too changeable) online streaming services and, as such, I was reluctant to simply banish them all to the attic. Instead, I decided to undertake the monumental task of ripping them all to HDD before boxing them all up.&lt;/p&gt;
&lt;h2&gt;Software&lt;/h2&gt;
&lt;p&gt;There are innumerable software solutions for ripping and transcoding DVD's... if you want to do one at a time. As you can probably appreciate, this wasn't a possibility for me or I'd still be ripping discs well into the New Year (not to mention through the Xmas dinner I'm actually doing this for). So a more, 'roll-your-own' solution was required.&lt;/p&gt;
&lt;p&gt;A bit of googling revealed &lt;a href="http://lifehacker.com/autorip-rips-dvds-and-blu-rays-as-soon-as-you-insert-th-477274988"&gt;this post&lt;/a&gt; which described a mechanism for ripping discs as soon as you put them into the drive. While it was still targetting people who wanted to rip discs one at a time, it did point me towards the two pieces of software I did ultimately use:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="http://www.makemkv.com/"&gt;MakeMKV&lt;/a&gt; - for ripping the entire contents of the DVD and,&lt;/li&gt;
&lt;li&gt;&lt;a href="https://handbrake.fr/"&gt;Handbrake&lt;/a&gt; - for transcoding source files into compressed H.264 format.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Both these utilities come with CLI interfaces (details &lt;a href="http://www.makemkv.com/developers/"&gt;here&lt;/a&gt; and &lt;a href="https://handbrake.fr/docs/en/latest/cli/cli-options.html"&gt;here&lt;/a&gt; respectively) allowing them to be automated. Furthermore, MakeMKV can run multiple instances allowing you to do your drive ripping (the long slow process) in parallel from multiple drives.&lt;/p&gt;
&lt;h2&gt;Hardware&lt;/h2&gt;
&lt;p&gt;I'd been intending to buy a new server for my home to be used as a Docker host because my current server is so old it simply doesn't support the required virtualization instructions. I'd had my eye on the &lt;a href="http://www.ebuyer.com/714837-dell-poweredge-t20-3708-xeon-e3-1225v3-3-2-ghz-4gb-ram-1tb-hdd-tower-t20-3708"&gt;Dell T20 Xeon E3&lt;/a&gt; but was waiting for it to drop back below the £200 (after cashback) mark. However, realising I could (temporarily) use it as a DVD ripping machine, I decided to bite the bullet and bought the T20 with an additional 4Gb of RAM for £324 (less 2% Quidco and £80 cashback).&lt;/p&gt;
&lt;p&gt;It arrived a couple of days later and I must say I'm impressed. It's a lot of machine for the money, well built, has a copious number of USB sockets and is surprisingly fast. I installed Windows Server 2016 on this as it's the OS I intend to use for the Docker host and I thought it'd be a good practice run.&lt;/p&gt;
&lt;p&gt;To this I added the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;2 x 1Tb internal HDDs - configured as RAID-0 providing a fast destination for the drive rips.&lt;/li&gt;
&lt;li&gt;4 x USB DVD-ROM drives - drives for ripping from - a couple I already had plus a &lt;a href="https://www.amazon.co.uk/dp/B01B8U7JW2/ref=pe_385721_51767431_TE_dp_1"&gt;couple from Amazon&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;1 x 4Tb external USB HDD - destination for transcoded rips&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;All in, it's quite a monster.&lt;/p&gt;
&lt;h2&gt;Putting it together&lt;/h2&gt;
&lt;p&gt;Unlike most of the out of the box software I'd found, I needed software that would do the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Wait for a disc to be inserted in one of the drives&lt;/li&gt;
&lt;li&gt;Determine that the disc is actually a DVD video&lt;/li&gt;
&lt;li&gt;Rip the disc to a destination folder based on the volume label of the DVD&lt;/li&gt;
&lt;li&gt;Allow up to four concurrent rip operations&lt;/li&gt;
&lt;li&gt;Queue ripped folders for transcoding allowing only a single transcoding operation to run at a time.&lt;/li&gt;
&lt;li&gt;Save the transcoded movie to the external drive.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;While there may be software out there that does this, a couple of evening's googling didn't reveal it so, I decided a DIY job was in order. Besides, I thought the multiple producer/single consumer nature of the multiple rips/single transcoding would be a great fit for playing with &lt;a href="https://msdn.microsoft.com/en-us/library/hh228603%28v=vs.110%29.aspx?f=255&amp;amp;MSPPError=-2147217396"&gt;Dataflow&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In relatively short order, I created &lt;a href="https://github.com/ibebbs/DriveRipper"&gt;DriveRipper&lt;/a&gt;... and it's worked pretty well. I've been happily ripping four DVD's at a time, averaging around 12-15 DVD's an hour, with only a few small problems:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Because Dataflow enforces order of tasks throughout the pipeline (one if it's biggest strengths!) a slow rip or drive can hold up transcoding such that a queue builds up. Not a massive problem as the encoding process on the Xeon is actually pretty rapid (averaging 450fps) so it catches up with the relatively slow ripping process (about 20 minutes per disc) with ease.&lt;/li&gt;
&lt;li&gt;Some DVD's - particularly the old ones - are single sided or low quality or lack additional material or all of the above. This means that the contents of the disc is less than the 4.2Gb cut off I used for determining that a particular disc is likely to be a DVD. This can be sorted with a simple code change but I dediced to put these discs to one side for now and come back to them.&lt;/li&gt;
&lt;li&gt;Some DVD's - again, particularly the olds ones - don't have a rational volume name; using terms like 'e19245' or, even worse, 'DVDVideo'. This has meant that, after the transcoding is complete, I need to rename the folders so that the contents don't get overwritten by a subsequent rip.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;So far I'm 150 DVD's through and just been told that the new dining room furniture is being delivered Monday.&lt;/p&gt;
&lt;p&gt;Rip little machine, rip like wind!! Oh, wait...&lt;/p&gt;



                                </content:encoded>
		</item>
		<item>
			<title>Unicorn Pi Tweet Bot</title>
			<link>http://ian.bebbs.co.uk/posts/RaspberryPiUnicornTwitterBot</link>
			<description>&lt;p&gt;So, my partner and I - being &lt;a href="https://www.stem.org.uk/stem-ambassadors/ambassadors"&gt;STEM Ambassadors&lt;/a&gt; - were asked to prepare and present a talk about Raspberry Pis for a local university on &lt;a href="http://www.inwed.org.uk/"&gt;International Women In Engineering Day&lt;/a&gt;. Unfortunately, this invitation came rather late as a previous presenter had dropped out and this left very little time to prepare.&lt;/p&gt;</description>
			<enclosure url="http://ian.bebbs.co.uk/vi/g3sxXgLr1uQ/maxresdefault" length="0" type="image" />
			<guid>http://ian.bebbs.co.uk/posts/RaspberryPiUnicornTwitterBot</guid>
			<pubDate>Wed, 21 Jun 2017 00:00:00 GMT</pubDate>
			<content:encoded>
                                        


&lt;p&gt;So, my partner and I - being &lt;a href="https://www.stem.org.uk/stem-ambassadors/ambassadors"&gt;STEM Ambassadors&lt;/a&gt; - were asked to prepare and present a talk about Raspberry Pis for a local university on &lt;a href="http://www.inwed.org.uk/"&gt;International Women In Engineering Day&lt;/a&gt;. Unfortunately, this invitation came rather late as a previous presenter had dropped out and this left very little time to prepare.&lt;/p&gt;
&lt;p&gt;While I had a couple of Pis around the house doing IoT related chores, I really wanted something a little more flashy to present so I quickly rushed out to the local Maplins and picked up a new Raspberry Pi 3 and a &lt;a href="https://shop.pimoroni.com/products/unicorn-hat"&gt;Pimoroni Unicorn HAT&lt;/a&gt; - the very definition of flashy.&lt;/p&gt;
&lt;img src="https://cdn.shopify.com/s/files/1/0174/1800/products/Unicorn_Still_4_1024x1024.jpg?v=1418813740" class="img-responsive" style="margin: auto; width:600px; margin-top: 6px; margin-bottom: 6px;" alt="Pimoroni Unicorn HAT"&gt;
&lt;p&gt;I then needed to think of something to &lt;strong&gt;do&lt;/strong&gt; with it.&lt;/p&gt;
&lt;p&gt;Wanting something interactive and, ideally, internet related (seeing a physical manifestation of an event in cyberspace really has 'wow' factor) I decided to write a Twitter bot that listens for a specific hashtag (#INWED17 in this instance) and scrolls the text of the tweet across the Unicorn HAT leds. As with most things of this nature, a short google session resulted in several articles / blog posts I could mash up to produce the desired effect. These were:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://www.raspberrypi.org/learning/getting-started-with-the-twitter-api/requirements/"&gt;Getting started with Twitter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://learn.pimoroni.com/tutorial/unicorn-hat/getting-started-with-unicorn-hat"&gt;Settings up Unicorn HAT&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/topshed/UnicornHatScroll"&gt;Unicorn HAT scroll&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;All in it took about an hour to get the various dependencies installed (including dealling with a rather annoying &lt;a href="https://stackoverflow.com/questions/27341064/how-do-i-fix-importerror-cannot-import-name-incompleteread"&gt;'pip' error&lt;/a&gt;), accounts set up and code mashed together in order to get a functional app up and running. Not bad considering this was my first stab at writing Python on a Pi.&lt;/p&gt;
&lt;p&gt;I pushed the code to a &lt;a href="https://github.com/ibebbs/UnicornPiBot"&gt;Github repository&lt;/a&gt; but have included it below simply because I am impressed with how concise the it is:&lt;/p&gt;
&lt;pre class="prettyprint"&gt;&lt;code&gt;import sys
import random

from UHScroll import *
from twython import TwythonStreamer
from auth import (
    consumer_key,
    consumer_secret,
    access_token,
    access_token_secret
)

colours = ['red','white','pink','blue','green','cyan']

non_bmp_map = dict.fromkeys(range(0x10000, sys.maxunicode + 1), 0xfffd)

class UnicornPiBotStreamer(TwythonStreamer):
    def on_success(self, data):
        if 'text' in data:
            text = data['text'].translate(non_bmp_map)
            colour = random.choice(colours)
            print(text)
            unicorn_scroll(text, colour, 200, 0.05)

stream = UnicornPiBotStreamer(
    consumer_key,
    consumer_secret,
    access_token,
    access_token_secret
)

stream.statuses.filter(track='#INWED17')
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I'm now beginning to see why Python has gained such traction in non-enterprise markets and will definitely consider using it for future projects. Skipping the edit-compile-run loop and simply interpretting the code via a command line call certainly makes for fast iterations but I don't think I'd appreciate it so much if I was trying to develop anything of any size/complexity.&lt;/p&gt;
&lt;p&gt;Anyway, having an exhibit visitors can interact with (albeit indirectly) will make a good addition to the &lt;a href="http://ian.bebbs.co.uk/posts/MonsterPi"&gt;other&lt;/a&gt; &lt;a href="https://www.raspberrypi.org/magpi/issues/40/"&gt;Pi&lt;/a&gt; &lt;a href="https://aiyprojects.withgoogle.com/voice"&gt;devices&lt;/a&gt; we'll be taking along and discussing.&lt;/p&gt;
&lt;p&gt;Really quite looking forward to it now...&lt;/p&gt;



                                </content:encoded>
		</item>
		<item>
			<title>Home Network Monitoring - Part II</title>
			<link>http://ian.bebbs.co.uk/posts/HomeNetworkMonitoring-PartII</link>
			<description>&lt;p&gt;In part one of this series I set up my home router to send a variety of Syslog messages to LogStash which then forwarded these messages to ElasticSearch for indexing and querying by Kibana in a simplistic dashboard. In this post, I'm going to start parsing out some of the details of the Syslog messages to start giving us a clearer idea about which devices on my local network are opening sessions with remote servers.&lt;/p&gt;</description>
			<guid>http://ian.bebbs.co.uk/posts/HomeNetworkMonitoring-PartII</guid>
			<pubDate>Sun, 10 Apr 2016 00:00:00 GMT</pubDate>
			<content:encoded>
                                        


&lt;p&gt;In part one of this series I set up my home router to send a variety of Syslog messages to LogStash which then forwarded these messages to ElasticSearch for indexing and querying by Kibana in a simplistic dashboard. In this post, I'm going to start parsing out some of the details of the Syslog messages to start giving us a clearer idea about which devices on my local network are opening sessions with remote servers.&lt;/p&gt;
&lt;h1&gt;message types&lt;/h1&gt;
&lt;p&gt;My router sends a number of different messages types; everything from router/dsl bandwidth information to the firewall log as can be seen below:&lt;/p&gt;
&lt;img src="/Content/HomeNetworkMonitoring/DrayTek-Syslog.png" class="img-responsive" style="margin: auto; width:600px; margin-top: 6px; margin-bottom: 6px;" alt="DrayTek Syslog Administration"&gt;
&lt;p&gt;For now, I'm really only interested in the 'User Access Log' which tells us when a local device resolves an IP address for a domain name or accesses a remote server. This log constitutes a number of message types such as:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;TCP / UDP access
This message is sent by the router when a device on the local network accesses a remote server via TCP or UDP. It looks like this: &lt;code&gt;&amp;lt;150&amp;gt;Apr 10 16:55:38 Vigor: Local User (MAC=00-00-00-00-00-00): 192.168.1.205:64281 -&amp;gt; 5.10.110.36:80 (TCP)Web&lt;/code&gt;. From this type of message I am interested in parsing out:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Syslog timestamp&lt;/li&gt;
&lt;li&gt;Syslog username&lt;/li&gt;
&lt;li&gt;Source Mac Address&lt;/li&gt;
&lt;li&gt;Source IP Address&lt;/li&gt;
&lt;li&gt;Destination IP Address&lt;/li&gt;
&lt;li&gt;Protocol&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;DNS lookup
This message is sent by the router when a device on the local network resolves an IP address for a remote server DNS nane. It looks like this: &lt;code&gt;&amp;lt;150&amp;gt;Apr 10 18:52:49 Vigor: Local User (MAC=00-00-00-00-00-00): 192.168.1.62 DNS -&amp;gt; 192.168.1.1 inquire a.root-servers.net&lt;/code&gt;. From this type of message I am interested in parsing out:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Syslog timestamp&lt;/li&gt;
&lt;li&gt;Syslog username&lt;/li&gt;
&lt;li&gt;Source Mac Address&lt;/li&gt;
&lt;li&gt;Source IP Address&lt;/li&gt;
&lt;li&gt;Domain being resolved&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In this post I'll focus on enriching the incoming message with information from the access message but will move on to parsing multiple message types in a future post.&lt;/p&gt;
&lt;h1&gt;parsing access messages&lt;/h1&gt;
&lt;p&gt;Logstash provides a great tool for parsing messages: the 'grok' filter. As explained in &lt;a href="https://www.elastic.co/guide/en/logstash/current/plugins-filters-grok.html"&gt;the documentation&lt;/a&gt; 'grok' allows you to 'Parse arbitrary text and structure it. This tool is perfect for syslog logs, apache and other webserver logs, mysql logs, and in general, any log format that is generally written for humans and not computer consumption.'&lt;/p&gt;
&lt;p&gt;This filter applies a regular expression like patterns to a field in the incoming message and, if matched successfully, adds each matched expression to the message being processed as a new field. The fields to capture are defined in the format &lt;code&gt;%{SYNTAX:SEMANTIC}&lt;/code&gt; where the &lt;code&gt;SYNTAX&lt;/code&gt; element is a known pattern (see below) or a regular expression and the &lt;code&gt;SEMANTIC&lt;/code&gt; is a name of the field to capture.&lt;/p&gt;
&lt;p&gt;Logstash ships with over 120 known &lt;code&gt;SYNTAX&lt;/code&gt; patterns and these patterns can be supplemented by adding new pattern files to a specific directory. However, the default patterns supplied cover nearly all common scenarios and allow almost all the pertinent information in the TCP access message above to be parsed.&lt;/p&gt;
&lt;p&gt;So, by just using the default patterns, I can write the expression&lt;/p&gt;
&lt;pre class="prettyprint"&gt;&lt;code&gt;&amp;lt;%{POSINT:syslog_pri}&amp;gt;%{SYSLOGTIMESTAMP:syslog_timestamp} Vigor\: Local User \(MAC=%{MAC:source_mac}\): %{IP:source_address}(?::%{POSINT:source_port})? -&amp;gt; %{IP:destination_address}(?::%{POSINT:destination_port})?
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Which gives us the following additional fields in the message:&lt;/p&gt;
&lt;p&gt;|field|value|
|-----|-----|
|source_address|192.168.1.205|
|source_port|64281|
|source_mac|00-00-00-00-00-00|
|syslog_pri|150|
|syslog_timestamp|Apr·10·16:55:38|
|destination_address|5.10.110.36|
|destination_port|80|&lt;/p&gt;
&lt;p&gt;Neat huh! I'd also like to know whether the access message is TCP or UDP so I'm going to add a custom regular expression to the end of the pattern to parse the protocol string out of the message. In full the pattern is now:&lt;/p&gt;
&lt;pre class="prettyprint"&gt;&lt;code&gt;&amp;lt;%{POSINT:syslog_pri}&amp;gt;%{SYSLOGTIMESTAMP:syslog_timestamp} Vigor\: Local User \(MAC=%{MAC:source_mac}\): %{IP:source_address}(?::%{POSINT:source_port})? -&amp;gt; %{IP:destination_address}(?::%{POSINT:destination_port})? \((?&amp;lt;protocol&amp;gt;TCP|UDP)\)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This would result in the following fields available in the message:&lt;/p&gt;
&lt;p&gt;|field|value|
|-----|-----|
|source_address|192.168.1.205|
|source_port|64281|
|source_mac|00-00-00-00-00-00|
|syslog_pri|150|
|syslog_timestamp|Apr·10·16:55:38|
|destination_address|5.10.110.36|
|destination_port|80|
|&lt;strong&gt;protocol&lt;/strong&gt;|&lt;strong&gt;TCP&lt;/strong&gt;|&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;NOTE: You can use &lt;a href="http://grokconstructor.appspot.com/"&gt;Grok Constructor&lt;/a&gt; to help you perfect your grok expressions prior to trying them in Logstash&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1&gt;adding the grok filter to logstash&lt;/h1&gt;
&lt;p&gt;So, now I know how to extract all the information I'm interested in from the access message, I need to get Logstash to actually do it. This is done by adding a &lt;code&gt;grok&lt;/code&gt; filter to the &lt;code&gt;filter { }&lt;/code&gt; section of the &lt;code&gt;syslog.config&lt;/code&gt; file authored in the previous post as shown here:&lt;/p&gt;
&lt;pre class="prettyprint"&gt;&lt;code&gt;input {
  tcp {
    port =&amp;gt; 5000
    type =&amp;gt; syslog
  }
  udp {
    port =&amp;gt; 5000
    type =&amp;gt; syslog
  }
}

filter {
    grok {
      match =&amp;gt; [ "message", "&amp;lt;%{POSINT:syslog_pri}&amp;gt;%{SYSLOGTIMESTAMP:syslog_timestamp} Vigor\: Local User \(MAC=%{MAC:source_mac}\): %{IP:source_address}(?::%{POSINT:source_port})? -&amp;gt; %{IP:destination_address}(?::%{POSINT:destination_port})? \((?&amp;lt;protocol&amp;gt;TCP|UDP)\)" ]
    }
}

output {
  elasticsearch {
    hosts =&amp;gt; ["192.168.1.30:9200"]
    index =&amp;gt; "syslog-%{+YYYY.MM.dd}"
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice the grok section in which we specify a 'match' field. This field takes an array of two strings; the first string is the field containing the text to match and the second is the pattern to match the text to.&lt;/p&gt;
&lt;p&gt;Once this is added to the &lt;code&gt;syslog.config&lt;/code&gt; file and Logstash restarted, these new fields should be being written to ElasticSearch. The easiest way to see if this is working is to return to Kibana and, from the 'Settings/Indices' area, click the 'syslog-*' pattern on the left and then click the orange 'Refresh field list' button and the field list appears like this:&lt;/p&gt;
&lt;img src="/Content/HomeNetworkMonitoring/Kibana-RefreshSyslogIndex.png" class="img-responsive" style="margin: auto; width:600px; margin-top: 6px; margin-bottom: 6px;" alt="Kibana Refresh Syslog Index"&gt;
&lt;h1&gt;exploring network access information&lt;/h1&gt;
&lt;p&gt;Now the additional fields are available to Kibana, I can start exploring the data to learn about which devices on my network are access which remote servers.&lt;/p&gt;
&lt;p&gt;To do this I first need to add the new fields to the 'Syslog Messages' saved search. This can be done by navigating to the 'Discover' page, loading the 'Syslog Messages' search (using the 'Load Saved Search' button in the top right of the screen), adding the new fields and re-saving it. This is shown below:&lt;/p&gt;
&lt;img src="/Content/HomeNetworkMonitoring/Kibana-AddingNewFieldsToSyslogMessagesSavedSearch.png" class="img-responsive" style="margin: auto; width:600px; margin-top: 6px; margin-bottom: 6px;" alt="Kibana Adding New Fields To Syslog Messages Saved Search"&gt;
&lt;p&gt;I'm now able to add some very interesting new visualizations to our dashboard such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Number of access by each local device&lt;/li&gt;
&lt;li&gt;Number of access by port&lt;/li&gt;
&lt;li&gt;Number of access to each remote server&lt;/li&gt;
&lt;li&gt;Number of access by protocol&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I'll only run through adding the first visualization is they're all very similar.&lt;/p&gt;
&lt;p&gt;First, I navigate to the 'Visualization' area and click the 'New Visualization' button. I want to see the number of accesses being made by each device as a fraction of the whole so will use a donut chart. This is done by selecting 'Pie chart' from the "Create new Visualization" menu and selecting our 'Syslog Messages' saved search in the "Select a search source" menu. Once this is done, I get a pie chart with a single section as shown here:&lt;/p&gt;
&lt;img src="/Content/HomeNetworkMonitoring/Kibana-NewPieChartVisualization.png" class="img-responsive" style="margin: auto; width:600px; margin-top: 6px; margin-bottom: 6px;" alt="Kibana New Pie Chart Visualization"&gt;
&lt;p&gt;I then need to select the 'Split Slices' bucket type, choose 'Terms' as the aggregation type and finally select 'source_address' as the field on which to aggregate (i.e. count) unique terms. Applying this to the pie-chart (using the 'Apply' button) results in the following:&lt;/p&gt;
&lt;img src="/Content/HomeNetworkMonitoring/Kibana-SplitSlicePieChartUsingSourceAddress.png" class="img-responsive" style="margin: auto; width:600px; margin-top: 6px; margin-bottom: 6px;" alt="Kibana Split Slice Pie Chart Using Source  ddres s"&gt;
&lt;p&gt;I'll  then  turn  this into a donut chart by ticking the 'Donut' check box from the 'Options' area as shown below:&lt;/p&gt;
&lt;img src="/Content/HomeNetworkMonitoring/Kibana-SplitSliceDonutChartUsingSourceAddress.png" class="img-responsive" style="margin: auto; width:600px; margin-top: 6px; margin-bottom: 6px;" alt="Kibana Split Slice Donut Chart Using Source Address"&gt;
&lt;p&gt;Now I'll use the same process for adding donut charts for each of the other metrics outlined above and add all the charts to my dashboard giving me the following:&lt;/p&gt;
&lt;img src="/Content/HomeNetworkMonitoring/Kibana-DashboardWithAccessCharts.png" class="img-responsive" style="margin: auto; width:600px; margin-top: 6px; margin-bottom: 6px;" alt="Kibana Dashboard With Access Charts"&gt;
&lt;h1&gt;summary&lt;/h1&gt;
&lt;p&gt;In this post, I showed how to extract structured information from unstructured text data in our source message. I then showed how this can be used within Kibana to highlight which devices on the local network are accessing which remote servers, on which ports and using which protocols.&lt;/p&gt;
&lt;p&gt;In the next post, I'll examine how to further enhance the information we've extracted from the access messages to make it easier to relate ip address to actual devices on the network and out in the world.&lt;/p&gt;



                                </content:encoded>
		</item>
	</channel>
</rss>ur dashboard such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Number of access by each local device&lt;/li&gt;
&lt;li&gt;Number of access by port&lt;/li&gt;
&lt;li&gt;Number of access to each remote server&lt;/li&gt;
&lt;li&gt;Number of access by protocol&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I'll only run through adding the first visualization is they're all very similar.&lt;/p&gt;
&lt;p&gt;First, I navigate to the 'Visualization' area and click the 'New Visualization' button. I want to see the number of accesses being made by each device as a fraction of the whole so will use a donut chart. This is done by selecting 'Pie chart' from the "Create new Visualization" menu and selecting our 'Syslog Messages' saved search in the "Select a search source" menu. Once this is done, I get a pie chart with a single section as shown here:&lt;/p&gt;
&lt;img src="/Content/HomeNetworkMonitoring/Kibana-NewPieChartVisualization.png" class="img-responsive" style="margin: auto; width:600px; margin-top: 6px; margin-bottom: 6px;" alt="Kibana New Pie Chart Visualization"&gt;
&lt;p&gt;I then need to select the 'Split Slices' bucket type, choose 'Terms' as the aggregation type and finally select 'source_address' as the field on which to aggregate (i.e. count) unique terms. Applying this to the pie-chart (using the 'Apply' button) results in the following:&lt;/p&gt;
&lt;img src="/Content/HomeNetworkMonitoring/Kibana-SplitSlicePieChartUsingSourceAddress.png" class="img-responsive" style="margin: auto; width:600px; margin-top: 6px; margin-bottom: 6px;" alt="Kibana Split Slice Pie Chart Using Source  ddres s"&gt;
&lt;p&gt;I'll  then  turn  this into a donut chart by ticking the 'Donut' check box from the 'Options' area as shown below:&lt;/p&gt;
&lt;img src="/Content/HomeNetworkMonitoring/Kibana-SplitSliceDonutChartUsingSourceAddress.png" class="img-responsive" style="margin: auto; width:600px; margin-top: 6px; margin-bottom: 6px;" alt="Kibana Split Slice Donut Chart Using Source Address"&gt;
&lt;p&gt;Now I'll use the same process for adding donut charts for each of the other metrics outlined above and add all the charts to my dashboard giving me the following:&lt;/p&gt;
&lt;img src="/Content/HomeNetworkMonitoring/Kibana-DashboardWithAccessCharts.png" class="img-responsive" style="margin: auto; width:600px; margin-top: 6px; margin-bottom: 6px;" alt="Kibana Dashboard With Access Charts"&gt;
&lt;h1&gt;summary&lt;/h1&gt;
&lt;p&gt;In this post, I showed how to extract structured information from unstructured text data in our source message. I then showed how this can be used within Kibana to highlight which devices on the local network are accessing which remote servers, on which ports and using which protocols.&lt;/p&gt;
&lt;p&gt;In the next post, I'll examine how to further enhance the information we've extracted from the access messages to make it easier to relate ip address to actual devices on the network and out in the world.&lt;/p&gt;



                                </content:encoded>
		</item>
		<item>
			<title>Home Network Monitoring - Part I</title>
			<link>http://ian.bebbs.co.uk/posts/HomeNetworkMonitoring-PartI</link>
			<description>&lt;p&gt;Home networks are becoming increasingly complex. It is no longer just geeks and techies who have pervasive WiFi through-out their home to which a myriad of devices connect and communicate. When things go wrong or, worse still, the network is compromised by rouge hardware or software it's extremely difficult to work out what has happened and where to start troubleshooting the issue.&lt;/p&gt;</description>
			<guid>http://ian.bebbs.co.uk/posts/HomeNetworkMonitoring-PartI</guid>
			<pubDate>Fri, 08 Apr 2016 00:00:00 GMT</pubDate>
			<content:encoded>
                                        


&lt;p&gt;Home networks are becoming increasingly complex. It is no longer just geeks and techies who have pervasive WiFi through-out their home to which a myriad of devices connect and communicate. When things go wrong or, worse still, the network is compromised by rouge hardware or software it's extremely difficult to work out what has happened and where to start troubleshooting the issue.&lt;/p&gt;
&lt;p&gt;In the next few posts, I'm going to be showing how I used free and open-source software to build a home network monitoring solution that allows me to:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Monitor the devices on my networks&lt;/li&gt;
&lt;li&gt;Monitor my network connectivity and utilisation&lt;/li&gt;
&lt;li&gt;Monitor which devices are connecting to which remote sites.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These posts will use the following components:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;An internet router capable of sending Syslog messages&lt;/li&gt;
&lt;li&gt;A mid-spec (i.e. dual-core 2Ghz) server with plenty of storage&lt;/li&gt;
&lt;li&gt;Java Runime Environment installed on the server&lt;/li&gt;
&lt;li&gt;The ElasticSearch stack consisting of:
&lt;ul&gt;
&lt;li&gt;Logstash (I will be using v2.4.1)&lt;/li&gt;
&lt;li&gt;ElasticSearch (I will be using v2.4.1)&lt;/li&gt;
&lt;li&gt;Kibana (I will be using v4.5.0)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h1&gt;the router&lt;/h1&gt;
&lt;p&gt;At the core of my solution is a router that supports writing &lt;a href="https://en.wikipedia.org/wiki/Syslog"&gt;Syslog&lt;/a&gt; messages to a Syslog server. While once a rarity, this functionality is becoming increasingly prevalent in home / SOHO routers. Personally I use a &lt;a href="http://www.draytek.co.uk/products/business/vigor-2830"&gt;DrayTek Vigor 2830&lt;/a&gt;, a versatile and - most importantly - extremely reliable router that can be purchased for just over £100 in the UK.&lt;/p&gt;
&lt;p&gt;From the router administration web interface, you can set the router to write a variety of Syslog messages to a Syslog server by specifying the servers IP address. This can be seen below:&lt;/p&gt;
&lt;img src="/Content/HomeNetworkMonitoring/DrayTek-Syslog.png" class="img-responsive" style="margin: auto; width:600px; margin-top: 6px; margin-bottom: 6px;" alt="DrayTek Vigor Syslog Settings"&gt;
&lt;p&gt;I simply tick all the boxes and add the IP address of the PC hosting the Syslog server.&lt;/p&gt;
&lt;h1&gt;the syslog server&lt;/h1&gt;
&lt;p&gt;Now we have a router that is sending connectivity information via Syslog, we need a service running on a server that is capable of receiving these messages. For this I am using Elastic's &lt;a href="https://www.elastic.co/products/logstash"&gt;Logstash&lt;/a&gt; to receive the syslog messages and enrich them before forwarding them to an indexed store.&lt;/p&gt;
&lt;p&gt;Logstash is an extremely versatile tool capable of consuming data from a variety of sources. A configuration file is used to set up a pipeline of inputs, operations (known as filters) and outputs which can do some truly fantastic things. If you're new to Logstash it might be worthwhile giving the ("Getting Started")[https://www.elastic.co/guide/en/logstash/current/getting-started-with-logstash.html] guide a quick read.&lt;/p&gt;
&lt;p&gt;To start with, we'll get Logstash to simply accept Syslog input from a given port and write it to the console. To do this, simply &lt;a href="https://www.elastic.co/downloads/logstash"&gt;download Logstash&lt;/a&gt; and extract it to a directory on the PC. Next, open notepad and copy paste the following:&lt;/p&gt;
&lt;pre class="prettyprint"&gt;&lt;code&gt;input {
  tcp {
    port =&amp;gt; 5000
    type =&amp;gt; syslog
  }
  udp {
    port =&amp;gt; 5000
    type =&amp;gt; syslog
  }
}

filter {
}

output {
  stdout {
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Save this file named 'Syslog.config' and, from a command prompt, start Logstash with the following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;[LogStashDirectory]\bin\logstash.bat agent -f [ConfigFilePath]\Syslog.config&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;If all goes well, you should see Logstash start receiving Syslog messages from the router which should appear something like the following:&lt;/p&gt;
&lt;pre class="prettyprint"&gt;&lt;code&gt;io/console not supported; tty will not be manipulated
Settings: Default pipeline workers: 2
Pipeline main started
2016-04-08T13:52:21.903Z 192.168.1.1 &amp;lt;150&amp;gt;Apr  8 14:51:59 Vigor: Local User (MAC=ZZ-ZZ-ZZ-ZZ-ZZ-ZZ): 192.168.1.62 DNS -&amp;gt; 192.168.1.1 inquire a.root-servers.net
2016-04-08T13:52:22.528Z 192.168.1.1 &amp;lt;150&amp;gt;Apr  8 14:51:59 Vigor: Local User (MAC=ZZ-ZZ-ZZ-ZZ-ZZ-ZZ): 192.168.1.51 DNS -&amp;gt; 192.168.1.1 inquire api-global.netflix.com
2016-04-08T13:52:22.528Z 192.168.1.1 &amp;lt;150&amp;gt;Apr  8 14:51:59 Vigor: Local User (MAC=ZZ-ZZ-ZZ-ZZ-ZZ-ZZ): 192.168.1.51 DNS -&amp;gt; 213.120.234.54 inquire api-global.netflix.com
2016-04-08T13:52:22.903Z 192.168.1.1 &amp;lt;150&amp;gt;Apr  8 14:52:00 Vigor: Local User (MAC=ZZ-ZZ-ZZ-ZZ-ZZ-ZZ): 192.168.1.62 DNS -&amp;gt; 192.168.1.1 inquire a.root-servers.net
2016-04-08T13:52:24.356Z 192.168.1.1 &amp;lt;150&amp;gt;Apr  8 14:52:01 Vigor: Local User (MAC=ZZ-ZZ-ZZ-ZZ-ZZ-ZZ): 192.168.1.100 DNS -&amp;gt; 192.168.1.1 inquire sls.update.microsoft.com
2016-04-08T13:52:24.356Z 192.168.1.1 &amp;lt;150&amp;gt;Apr  8 14:52:01 Vigor: Local User (MAC=ZZ-ZZ-ZZ-ZZ-ZZ-ZZ): 192.168.1.100 DNS -&amp;gt; 213.120.234.54 inquire sls.update.microsoft.com
2016-04-08T13:52:24.356Z 192.168.1.1 &amp;lt;158&amp;gt;Apr  8 14:52:01 Vigor: Load_balance 192.168.1.100 --(DNS)--&amp;gt; 213.120.234.54 go WAN1
2016-04-08T13:52:24.731Z 192.168.1.1 &amp;lt;150&amp;gt;Apr  8 14:52:01 Vigor: Local User (MAC=ZZ-ZZ-ZZ-ZZ-ZZ-ZZ): 192.168.1.100:53432 -&amp;gt; 157.56.77.138:443 (TCP)
2016-04-08T13:52:24.731Z 192.168.1.1 &amp;lt;158&amp;gt;Apr  8 14:52:01 Vigor: Load_balance 192.168.1.100 --(BAL)--&amp;gt; 157.56.77.138 go WAN1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you're not receiving Syslog messages try the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Ensure to check the firewall settings on the server you're using. You need to allow incomming TCP and UDP connections on port 5000.&lt;/li&gt;
&lt;li&gt;Attempt to telnet to port 5000 on the Syslog server. If Logstash is running correctly, you should be able to connect and see anything you sent from Telnet mirrored in the Logstash console window.&lt;/li&gt;
&lt;li&gt;If you're still unable to see any output, try using a network analysis too like &lt;a href="https://www.wireshark.org/"&gt;Wireshark&lt;/a&gt; to see if your router is actually sending any messages.&lt;/li&gt;
&lt;/ol&gt;
&lt;h1&gt;the syslog store&lt;/h1&gt;
&lt;p&gt;Now we're able to receive Syslog messages, we need to store and index them. For this, we will use ElasticSearch. Simply &lt;a href="https://www.elastic.co/downloads/elasticsearch"&gt;download ElasticSearch&lt;/a&gt;, extract it to a directory and start it. If necessary you can change the directory used to store  ElasticSearch data or the interface/port on which ElasticSearch listens for incomming connections by modifying the &lt;code&gt;config\ElasticSearch.yml&lt;/code&gt; file. Modifying this file is pretty straight forward but for help the &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html"&gt;ElasticSearch documentation&lt;/a&gt; is available online and very thorough.&lt;/p&gt;
&lt;p&gt;Additionally, rather than having to manually start ElasticSearch everytime you want to use it, you can easily install it as a Windows service simply - as I have done - by issuing the following commands from a command prompt:&lt;/p&gt;
&lt;pre class="prettyprint"&gt;&lt;code&gt;[PathToElasticSearch]\bin\service.bat install
[PathToElasticSearch]\bin\service.bat start
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ElasticSearch should no be running. You can check this calling REST methods on ElasticSearch's web interface; by default on port 9200. In a browser, simple enter &lt;code&gt;http://[server-ip]:9200&lt;/code&gt; and you should see something like the following:&lt;/p&gt;
&lt;pre class="prettyprint"&gt;&lt;code&gt;"name" : "Shellshock",
"cluster_name" : "cluster",
"version" : {
  "number" : "2.3.1",
  "build_hash" : "bd980929010aef404e7cb0843e61d0665269fc39",
  "build_timestamp" : "2016-04-04T12:25:05Z",
  "build_snapshot" : false,
  "lucene_version" : "5.5.0"
},
"tagline" : "You Know, for Search"
}
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;NOTE: You will be using the REST API extensively in future steps so I suggest finding a toolset that make querying and posting to REST endpoints easier. I use the excellent &lt;a href="https://chrome.google.com/webstore/detail/postman/fhbjgbiflinjbdggehcddcbncdddomop?hl=en"&gt;&lt;code&gt;Postman&lt;/code&gt;&lt;/a&gt; Chrome application.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;With ElasticSearch running, we now need to modify Logstash to forward Syslog messages to ElasticSearch for indexing. As both tools are part of the Elastic Stack, this is every bit as easy as you might expect it to be. Simply open the &lt;code&gt;syslog.config&lt;/code&gt; file we created earler and change it to the following:&lt;/p&gt;
&lt;pre class="prettyprint"&gt;&lt;code&gt;input {
  tcp {
    port =&amp;gt; 5000
    type =&amp;gt; syslog
  }
  udp {
    port =&amp;gt; 5000
    type =&amp;gt; syslog
  }
}

filter {
}

output {
  elasticsearch {
    hosts =&amp;gt; ["[ElasticSearchServer-NameOrIPAddress]:9200"]
    index =&amp;gt; "syslog-%{+YYYY.MM.dd}"
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With this in place, restart the Logstart service and, instead of writing received messages to the console, they will be stored in ElasticSearch.&lt;/p&gt;
&lt;h1&gt;the dashboard&lt;/h1&gt;
&lt;p&gt;Now we have syslog messages in a central store, we will look how to set up a simple (for now) dashboard that lets us see a minimally useful feature: the number of Syslog messages being received over time.&lt;/p&gt;
&lt;p&gt;In order to do this, we will be using ElasticSearch's Kibana tool to query messages from ElasticSearch and display a histogram of messages on a dashboard. To get started, simply download and extract Kibana to a directory on the server. Kibana comes with a default configuration that allows it to run correctly when co-located on the same server as ElasticSearch. If you are not running Kibana on the same server as ElasticSearch, you will need to modify the Kibana configuration file as described in the &lt;a href="https://www.elastic.co/guide/en/kibana/current/index.html"&gt;documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To start Kibana, simply open a command prompt and execute the following command:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;[PathToKibana]\bin\Kibana.bat&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;You should see something like the following:&lt;/p&gt;
&lt;pre class="prettyprint"&gt;&lt;code&gt;log   [15:51:32.912] [info][status][plugin:kibana] Status changed from uniniti
alized to green - Ready
log   [15:51:32.975] [info][status][plugin:elasticsearch] Status changed from
uninitialized to yellow - Waiting for Elasticsearch
log   [15:51:33.006] [info][status][plugin:kbn_vislib_vis_types] Status change
d from uninitialized to green - Ready
log   [15:51:33.022] [info][status][plugin:markdown_vis] Status changed from u
ninitialized to green - Ready
log   [15:51:33.037] [info][status][plugin:metric_vis] Status changed from uni
nitialized to green - Ready
log   [15:51:33.053] [info][status][plugin:spyModes] Status changed from unini
tialized to green - Ready
log   [15:51:33.068] [info][status][plugin:statusPage] Status changed from uni
nitialized to green - Ready
log   [15:51:33.068] [info][status][plugin:table_vis] Status changed from unin
itialized to green - Ready
log   [15:51:33.100] [info][listening] Server running at http://0.0.0.0:5601
log   [15:51:38.131] [info][status][plugin:elasticsearch] Status changed from
yellow to yellow - No existing Kibana index found
log   [15:51:41.053] [info][status][plugin:elasticsearch] Status changed from
yellow to green - Kibana index ready
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With Kibana started, use a browser to navigate to the Kibana web interface, typically on port 5601.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;NOTE: If you are connected to Kibana from another PC, you will need to open firewall port 5602 on the server to allow connections to Kibana.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;After a short pause while Kibana is initialised, you should see the following screen:&lt;/p&gt;
&lt;img src="/Content/HomeNetworkMonitoring/Kibana-InitialIndex.png" class="img-responsive" style="margin: auto; width:600px; margin-top: 6px; margin-bottom: 6px;" alt="Kibana Initial Index Settings"&gt;
&lt;p&gt;This screen allows you to add an index to Kibana that it can query messages in order to discover, visualise and ultimately produce a dashboard from information stored in the index. It is currently set to examine an index called &lt;code&gt;logstash-*&lt;/code&gt; and shows a disable button at the bottom of the screen containing the text 'Unable to fetch mapping. Do you have indices matching the pattern' as we do not have a logstash index stored in ElasticSearch. We want Kibana to query our Syslog index so we change the 'Index name or pattern' to &lt;code&gt;Syslog-*&lt;/code&gt; and, shortly after changing this value, we should see the button at the bottom change to 'Create' as shown here:&lt;/p&gt;
&lt;img src="/Content/HomeNetworkMonitoring/Kibana-SyslogIndex.png" class="img-responsive" style="margin: auto; width:600px; margin-top: 6px; margin-bottom: 6px;" alt="Kibana Syslog Index Settings"&gt;
&lt;p&gt;As Kibana has defaulted to the correct timestamp field (more on this later) you can simply click the 'Create' button to add the index to Kibana.&lt;/p&gt;
&lt;p&gt;After creating the index, you are taken to a screen that allows you modify how Kibana displays the fields within the index as shown below:&lt;/p&gt;
 &lt;img src="/Content/HomeNetworkMonitoring/Kibana-SyslogIndexMapping.png" class="img-responsive" style="margin: auto; width:600px; margin-top: 6px; margin-bottom: 6px;" alt="Kibana Syslog Index Mapping"&gt;
&lt;p&gt;We do not need to change anything here so can immediately start 'discovering' information in our Syslog index by clicking the 'Discover' tab as shown below:&lt;/p&gt;
 &lt;img src="/Content/HomeNetworkMonitoring/Kibana-DiscoverSyslog.png" class="img-responsive" style="margin: auto; width:600px; margin-top: 6px; margin-bottom: 6px;" alt="Kibana Discover Syslog"&gt;
&lt;p&gt;Here we're able to see the detail of the Syslog messages stored within the Syslog index on ElasticSearch. However, for now the messages are just strings so there's not a great deal we can do with them other than count them to produce a histogram of messages over time. To do this, we
first want to add the fields of the index we're interested in (just timestamp, host, message and type for now) to the selected fields area as shown below:&lt;/p&gt;
&lt;img src="/Content/HomeNetworkMonitoring/Kibana-SelectSyslogFields.png" class="img-responsive" style="margin: auto; width:600px; margin-top: 6px; margin-bottom: 6px;" alt="Kibana Select Syslog Fields"&gt;
&lt;p&gt;Once we have the fields we're interested in selected, we save them as a new search by clicking the 'Save Search' button in the top right of the window as shown below:&lt;/p&gt;
&lt;img src="/Content/HomeNetworkMonitoring/Kibana-SaveSyslogSearch.png" class="img-responsive" style="margin: auto; width:600px; margin-top: 6px; margin-bottom: 6px;" alt="Kibana Save Syslog Search"&gt;
&lt;p&gt;Once the search is saved, we can proceed straight to the 'Visualise' tab to create the histogram as shown below:&lt;/p&gt;
&lt;img src="/Content/HomeNetworkMonitoring/Kibana-VisualiseSyslogAsVerticalBarChart.png" class="img-responsive" style="margin: auto; width:600px; margin-top: 6px; margin-bottom: 6px;" alt="Kibana Visualise Syslog As Vertical Bar Chart"&gt;
&lt;p&gt;To create a histogram, we use a "Vertical bar chart". Note it's description:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The goto chart for oh-so-many needs. Great for time and non-time data. Stacked or grouped, exact numbers or percentages. If you are not sure which chart you need, you could do worse than to start here.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So lets start here by clicking this option and selecting 'From a saved search' as shown below:&lt;/p&gt;
&lt;img src="/Content/HomeNetworkMonitoring/Kibana-VisualiseFromSavedSearch.png" class="img-responsive" style="margin: auto; width:600px; margin-top: 6px; margin-bottom: 6px;" alt="Kibana Visualise From Saved Search"&gt;
&lt;p&gt;Once you select the 'Syslog Messages' save search you will immediately be taken to the Visualization screen with a vertical bar chart showing a single bar with the total count of all messages. Here we want to customise the visualisation to display the count of messages over time so we first need to define the X-axis as a 'Date Histogram' as shown below.&lt;/p&gt;
&lt;img src="/Content/HomeNetworkMonitoring/Kibana-VisualiseDateHistogram.png" class="img-responsive" style="margin: auto; width:600px; margin-top: 6px; margin-bottom: 6px;" alt="Kibana Visualise Date Histogram"&gt;
&lt;p&gt;With these settings in place, click the 'Apply Changes' button and you should immediately see a chart of the count of syslog messages over time. Note that Kibana has automatically selected an appropriate resolution of column grouping (messages 'per 30 seconds' in the example above) but that this can be changed later if required.&lt;/p&gt;
&lt;p&gt;For now, we want to add this chart to a new dashboard so we can have it available to us at a moments notice. To do this, we first save our visualization by clicking the 'Save Visuaization' button as shown below:&lt;/p&gt;
&lt;img src="/Content/HomeNetworkMonitoring/Kibana-VisualiseSaveSyslogMessagesOverTime.png" class="img-responsive" style="margin: auto; width:600px; margin-top: 6px; margin-bottom: 6px;" alt="Kibana Visualise Save Syslog Messages Over Time"&gt;
&lt;p&gt;Next we navigate to the 'Dashboard' tab and click the '+' button as prompted. Again, we selected the visualization we just saved as shown below:&lt;/p&gt;
&lt;img src="/Content/HomeNetworkMonitoring/Kibana-DashboardAddVisualization.png" class="img-responsive" style="margin: auto; width:600px; margin-top: 6px; margin-bottom: 6px;" alt="Kibana Dashboard Add Visualization"&gt;
&lt;p&gt;With the "Syslog Messages Over Time" visualization added, we make it a usable size by dragging the resize control (button right corner of the visualization) to extend the visualization across the width of the window as shown below:&lt;/p&gt;
&lt;img src="/Content/HomeNetworkMonitoring/Kibana-DashboardSizeVisualization.png" class="img-responsive" style="margin: auto; width:600px; margin-top: 6px; margin-bottom: 6px;" alt="Kibana Dashboard Size Visualization"&gt;
&lt;p&gt;Finally, to make it easier to see the messages the histogram refers to, we'll add a table of related Syslog messages onto our dashboard below the histogram. To do this, click the '+' button, selected 'Searches' and then our 'Syslog Messages' search. When the table is added to the dashboard, make it a similar size as the histogram as shown below:&lt;/p&gt;
&lt;img src="/Content/HomeNetworkMonitoring/Kibana-DashboardWithSearch.png" class="img-responsive" style="margin: auto; width:600px; margin-top: 6px; margin-bottom: 6px;" alt="Kibana Dashboard With Search"&gt;
&lt;p&gt;Once done, we need to save the dashboard so we can reload it any time we need it. Simply click the 'Save Dashboard' button as shown below:&lt;/p&gt;
&lt;img src="/Content/HomeNetworkMonitoring/Kibana-DashboardSaveSyslogMessages.png" class="img-responsive" style="margin: auto; width:600px; margin-top: 6px; margin-bottom: 6px;" alt="Kibana Dashboard Save Syslog Messages"&gt;
&lt;p&gt;Once saved you can bookmark the page and get back to your dashboard any time you like. Furthermore, once saved, you can monkey with the dashboard (explore the data by highlighting various areas of the chart, change time frames, set autorefresh etc) as much as you like knowing you can return to saved version any time.&lt;/p&gt;
&lt;h1&gt;summary&lt;/h1&gt;
&lt;p&gt;In this post, I have outlined how to use a Syslog capable router to send Syslog messages to Logstash and have Logstash store these messages in ElasticSearch for querying. Furthermore we then created a dashboard in which we can explore the number of Syslog messages we received over various timeframes.&lt;/p&gt;
&lt;p&gt;In the next post we'll increase the granularity of the messages we store so that we can start creating more interesting dashboard.&lt;/p&gt;



                                </content:encoded>
		</item>
		<item>
			<title>Combining the UWP SpeechSynthesizer and AudioGraph APIs</title>
			<link>http://ian.bebbs.co.uk/posts/CombiningUwpSpeechSynthesizerWithAudioGraph</link>
			<description>&lt;p&gt;Synchronicity is a wonderful thing.&lt;/p&gt;</description>
			<guid>http://ian.bebbs.co.uk/posts/CombiningUwpSpeechSynthesizerWithAudioGraph</guid>
			<pubDate>Wed, 25 Jan 2017 00:00:00 GMT</pubDate>
			<content:encoded>
                                        


&lt;p&gt;Synchronicity is a wonderful thing.&lt;/p&gt;
&lt;p&gt;Just this morning I was considering using the new &lt;a href="https://msdn.microsoft.com/en-us/library/windows/apps/windows.media.speechsynthesis.speechsynthesizer.aspx"&gt;SpeechSynthesizer&lt;/a&gt; capabilities of the UWP platform to add spoken language to my &lt;a href="https://www.microsoft.com/en-gb/store/p/toddlerbox/9nblggh3zr4l"&gt;ToddlerBox app for Xbox&lt;/a&gt;. I had already started using the &lt;a href="https://msdn.microsoft.com/en-us/library/windows/apps/windows.media.audio.audiograph.aspx"&gt;AudioGraph&lt;/a&gt; classes to play sounds in the app so ideally wanted to continue using this API to emit speech.&lt;/p&gt;
&lt;p&gt;Then, during my morning... ahem... ablutions, I came across &lt;a href="https://mtaulty.com/2017/01/15/windows-10-uwp-iot-core-speechsynthesizer-raspberry-pi-and-audio-popping/"&gt;this post&lt;/a&gt; by Mike Taulty who was looking to do the same thing but for different reasons. It seems that the RaspberryPi has a firmware issue that causes a &lt;a href="https://social.msdn.microsoft.com/Forums/en-US/7c312972-6a09-4acd-8a3f-c59485a81d74/clicking-sound-during-start-and-stop-of-audio-playback?forum=WindowsIoT"&gt;popping noise&lt;/a&gt;  every time speech is emitted using the MediaPlayer and AudioGraph seems to be a way of resolving it.&lt;/p&gt;
&lt;h2&gt;The problem&lt;/h2&gt;
&lt;p&gt;Mike had implemented a means of emitting speech via AudioGraph by saving the &lt;a href="https://msdn.microsoft.com/en-us/library/windows/apps/windows.media.speechsynthesis.speechsynthesisstream.aspx?f=255&amp;amp;MSPPError=-2147217396"&gt;SpeechSynthesisStream&lt;/a&gt; to a  temporary file and then using multiple &lt;a href="https://msdn.microsoft.com/en-us/library/windows/apps/windows.media.audio.audiofileinputnode.aspx"&gt;AudioFileInputNode&lt;/a&gt; instances to render the speech to the AudioGraph.&lt;/p&gt;
&lt;p&gt;"Well", I thought, "there's got to be a better way. How hard can this be..."&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Turns out the answer is: "Not all that hard, but...".&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;My approach&lt;/h2&gt;
&lt;p&gt;I wanted to find a way to eliminate the need for the temporary files and render the speech stream directly to the graph.&lt;/p&gt;
&lt;p&gt;To do this, I first saved the SpeechSynthesisStream to a file so that I could examine the content. As expected, the file turned out to be a simple 32-bit, mono, ADPCM waveform in &lt;a href="http://soundfile.sapp.org/doc/WaveFormat/"&gt;WAV/RIFF format&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Having previously messed about with AudioGraph I knew there was a way of creating an in-memory waveform and that the &lt;a href="https://github.com/Microsoft/Windows-universal-samples"&gt;Windows-Universal-Samples github repository&lt;/a&gt; had an &lt;a href="https://github.com/Microsoft/Windows-universal-samples/tree/master/Samples/AudioCreation"&gt;AudioCreation sample&lt;/a&gt; that &lt;a href="https://github.com/Microsoft/Windows-universal-samples/blob/master/Samples/AudioCreation/cs/AudioCreation/Scenario3_FrameInputNode.xaml.cs"&gt;showed how to do this&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Fundamentally, this sample shows how to use the &lt;a href="https://msdn.microsoft.com/en-gb/library/windows/apps/windows.media.audio.audioframeinputnode.quantumstarted"&gt;QuantumStarted event&lt;/a&gt; of the &lt;a href="https://msdn.microsoft.com/library/windows/apps/dn914147"&gt;AudioFrameInputNode&lt;/a&gt; to dynamically add &lt;a href="https://msdn.microsoft.com/en-us/library/windows/apps/windows.media.audioframe.aspx"&gt;AudioFrame&lt;/a&gt; to the AudioFrameInputNode which are then rendered to the &lt;a href="https://msdn.microsoft.com/en-gb/library/windows/apps/dn914151"&gt;output node&lt;/a&gt;. An extract from the sample is shown here:&lt;/p&gt;
&lt;pre class="prettyprint"&gt;&lt;code&gt;  unsafe private AudioFrame GenerateAudioData(uint samples)
  {
      // Buffer size is (number of samples) * (size of each sample)
      // We choose to generate single channel (mono) audio. For multi-channel, multiply by number of channels
      uint bufferSize = samples * sizeof(float);
      AudioFrame frame = new Windows.Media.AudioFrame(bufferSize);

      using (AudioBuffer buffer = frame.LockBuffer(AudioBufferAccessMode.Write))
      using (IMemoryBufferReference reference = buffer.CreateReference())
      {
          byte* dataInBytes;
          uint capacityInBytes;
          float* dataInFloat;

          // Get the buffer from the AudioFrame
          ((IMemoryBufferByteAccess)reference).GetBuffer(out dataInBytes, out capacityInBytes);

          // Cast to float since the data we are generating is float
          dataInFloat = (float*)dataInBytes;

          float freq = 1000; // choosing to generate frequency of 1kHz
          float amplitude = 0.3f;
          int sampleRate = (int)graph.EncodingProperties.SampleRate;
          double sampleIncrement = (freq * (Math.PI * 2)) / sampleRate;

          // Generate a 1kHz sine wave and populate the values in the memory buffer
          for (int i = 0; i &amp;lt; samples; i++)
          {
              double sinValue = amplitude * Math.Sin(theta);
              dataInFloat[i] = (float)sinValue;
              theta += sampleIncrement;
          }
      }

      return frame;
  }
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Imitation being the sincerest form of flattery, I then refactored this code to read the binary data from the SpeechSynthesisStream rather than generate a sine wave as shown above. This was greatly facilited by the &lt;a href="https://msdn.microsoft.com/en-us/library/hh582142.aspx"&gt;WindowsRuntimeStreamExtensions.AsStreamForRead&lt;/a&gt; method which allowed me to use basic &lt;a href="https://msdn.microsoft.com/en-us/library/system.io.stream.aspx"&gt;Stream&lt;/a&gt; methods  (specifically &lt;a href="https://msdn.microsoft.com/en-us/library/system.io.stream.readbyte.aspx"&gt;Stream.ReadByte()&lt;/a&gt;) instead of having to mess about with &lt;a href="https://msdn.microsoft.com/en-us/library/windows.media.speechsynthesis.speechsynthesisstream.readasync.aspx"&gt;IBuffer&lt;/a&gt; instances.&lt;/p&gt;
&lt;p&gt;In short order, I ended up with the code below (where &lt;code&gt;_stream&lt;/code&gt; is a member of the containing class pointing to the underlying SpeechSynthesisStream):&lt;/p&gt;
&lt;pre class="prettyprint"&gt;&lt;code&gt;    private unsafe void QuantumStarted(AudioFrameInputNode sender, FrameInputNodeQuantumStartedEventArgs args)
    {
        uint numSamplesNeeded = (uint)args.RequiredSamples;

        if (numSamplesNeeded != 0 &amp;amp;&amp;amp; _stream.Position &amp;lt; _stream.Length)
        {
            uint bufferSize = numSamplesNeeded * sizeof(float);
            AudioFrame frame = new AudioFrame(bufferSize);

            using (AudioBuffer buffer = frame.LockBuffer(AudioBufferAccessMode.Write))
            {
                using (IMemoryBufferReference reference = buffer.CreateReference())
                {
                    byte* dataInBytes;
                    uint capacityInBytes;

                    // Get the buffer from the AudioFrame
                    ((IMemoryBufferByteAccess)reference).GetBuffer(out dataInBytes, out capacityInBytes);

                    for (int i = 0; i &amp;lt; bufferSize; i++)
                    {
                        if (_stream.Position &amp;lt; _stream.Length)
                        {
                            dataInBytes[i] = (byte)_stream.ReadByte();
                        }
                        else
                        {
                            dataInBytes[i] = 0;
                        }
                    }
                }
            }

            _frameInputNode.AddFrame(frame);
        }
    }
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And to my surprised, it worked!&lt;/p&gt;
&lt;p&gt;I encapsulated this code into a class named &lt;a href="https://github.com/ibebbs/BlogProjects/blob/master/UwpSpeechAudio/GraphExtensions.cs"&gt;AudioSpeechInputNode&lt;/a&gt; and made this class implement &lt;a href="https://msdn.microsoft.com/en-us/library/windows/apps/windows.media.audio.iaudioinputnode.aspx"&gt;IAudioInputNode&lt;/a&gt; so it could be treated like any other node in the AudioGraph. Finally I added an extension method to AudioGraph that created instance of this node in the same way that other nodes are created. This is shown below:&lt;/p&gt;
&lt;pre class="prettyprint"&gt;&lt;code&gt;    AudioSpeechInputNode speechInputNode = await _graph.CreateSpeechInputNodeAsync(new SpeechSynthesizer(), "As input node");
    speechInputNode.AddOutgoingConnection(_outputNode); // device output node
    speechInputNode.Stop();
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With this node in hand you're then at liberty to call the &lt;a href="https://msdn.microsoft.com/en-us/library/windows/apps/windows.media.audio.iaudionode.start.aspx"&gt;Start&lt;/a&gt;, &lt;a href="https://msdn.microsoft.com/en-us/library/windows/apps/windows.media.audio.iaudionode.stop.aspx"&gt;Stop&lt;/a&gt; and &lt;a href="https://msdn.microsoft.com/en-us/library/windows/apps/windows.media.audio.iaudionode.reset.aspx"&gt;Reset&lt;/a&gt; methods as you see fit.&lt;/p&gt;
&lt;p&gt;Et voila, a SpeechSynthesisStream rendered in an AudioGraph without the need for an intermediary file.&lt;/p&gt;
&lt;h2&gt;You said there was a 'but' ...&lt;/h2&gt;
&lt;p&gt;Well, yes. Three of them actually.&lt;/p&gt;
&lt;h3&gt;The big 'but'&lt;/h3&gt;
&lt;p&gt;While this approach certainly solves the issue with needing temporary files and a 'popping' sound each time speech is emitted, I'm afraid to say it does not resolve the 'popping' noise encountered when the application starts on a RaspberryPi.&lt;/p&gt;
&lt;p&gt;Being a good nerd, I had a spare RaspberryPi 3 hanging around with a recent version of Windows 10 IoT Core installed. It took just a few minutes to recompile my &lt;a href="https://github.com/ibebbs/BlogProjects/tree/master/UwpSpeechAudio"&gt;sample app&lt;/a&gt; to ARM and deploy it to the Pi whereupon I could confirm that there is no popping when emitting speech but there is when the application starts. In fact, I receive three distinct 'pops' during application start-up which, by studiously placing breakpoints, I isolated to &lt;a href="https://msdn.microsoft.com/en-us/library/windows/apps/windows.media.audio.audiograph.createasync.aspx"&gt;AudioGraph.CreateAsync&lt;/a&gt; (two pops) and &lt;a href="https://msdn.microsoft.com/en-us/library/windows/apps/windows.media.audio.audiograph.start.aspx"&gt;AudioGraph.Start&lt;/a&gt; (one pop).&lt;/p&gt;
&lt;p&gt;Microsoft would have us believe that this is an issue with the RaspberryPi firmware but, as it also occurs on &lt;a href="https://developer.qualcomm.com/hardware/dragonboard-410c"&gt;DragonBoard 410c&lt;/a&gt; I'm more inclined to believe it's an issue with the Windows drivers. On a hunch, I've just ordered a &lt;a href="https://www.amazon.co.uk/dp/B016CU2PEU"&gt;USB Sound Adapter from Amazon&lt;/a&gt;. This device is &lt;em&gt;meant&lt;/em&gt; to be Windows and RaspberryPi compatible (which doesn't necessarily mean it'll work with IoT Core on RPi) and, if it works, I'll be very interested to see if I still get the popping noises when the application starts. I'll update this post once I have an answer...&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Update: I'm pleased to say that, not only does &lt;a href="https://www.amazon.co.uk/dp/B016CU2PEU"&gt;this device&lt;/a&gt; work with Windows 10 IoT Core running on the RaspberryPi 3, but it also resolves the issue with popping noises when the application starts. Of course, this would probably also solve the issue with popping noises when rendering speech through MediaPlayer too making my solution above less necessary.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;The intermediate 'but'&lt;/h3&gt;
&lt;p&gt;My code makes a number of assumptions about the format of the SpeechSynthesisStream and encapsulates these as constants. It would be much better to read the format from the WAVE 'fmt ' chunk of the underlying RIFF structures in the stream but, being a pragmatic, &lt;a href="https://martinfowler.com/bliki/Yagni.html"&gt;YAGNI principled&lt;/a&gt; developer... I skipped this for now.&lt;/p&gt;
&lt;h3&gt;The small 'but'&lt;/h3&gt;
&lt;p&gt;As is probably very obvious, the code above is in no way optimised. I'm sure there are &lt;em&gt;much&lt;/em&gt; better and faster ways of storing and copying the binary data from the SpeechSynthesisStream into the AudioBuffer (perhaps just using an intermediate byte array would help) but, for now, this code works fine.&lt;/p&gt;
&lt;h2&gt;Show me the code&lt;/h2&gt;
&lt;p&gt;All the code for the above can be found in a &lt;a href="https://github.com/ibebbs/BlogProjects/tree/master/UwpSpeechAudio"&gt;UWP sample app&lt;/a&gt; within the &lt;a href="https://github.com/ibebbs/BlogProjects"&gt;BlogProjects&lt;/a&gt; repository of my &lt;a href="https://github.com/ibebbs"&gt;Github&lt;/a&gt; account.&lt;/p&gt;
&lt;p&gt;Do &lt;a href="https://twitter.com/ibebbs"&gt;get in touch&lt;/a&gt; if you find this code helpful or have suggestions for improving it.&lt;/p&gt;



                                </content:encoded>
		</item>
	</channel>
</rss>